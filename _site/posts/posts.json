[
  {
    "path": "posts/2020-12-03-simulate-dependent-variables/",
    "title": "Simulate dependent variables",
    "description": "When you simulate a dataset it is often not enough to have independent variables, but you  want to have some dependency between the variables. In this post we explore ways of creating this dependency.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-12-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPackages\r\nSimulating dependent variablesRule based\r\nCorrelation based\r\n\r\nClosing comments\r\n\r\nPackages\r\nMost of the functions that we are using here are actually part of base R.\r\nWe will need some functions from the {dplyr} and {ggplot2} packages for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nSimulating dependent variables\r\nIf we want to have a dataset that “makes sense” from a real world perspective, wouldn’t it be great if managers in general had higher balances than junior analysts? Or if 18 years old clients are less likely to be married than 30 years olds? This does not mean that no 18 years old can be married in our dataset, but if we look at the average, we would expect a difference between both groups. In this section we are going to have a look at techniques to create dependence between variables.\r\nRule based\r\nWe can use ifelse() and case_when() from the {dplyr} package to create new variables that depend on others. Let’s make a small example with two columns which depend on each other: married, which indicates if the person is married, and age.\r\nWe will simulate 1000 clients, around 50% of which are married.\r\n\r\n\r\nk <- 1000\r\nmarried <- sample(c(\"Y\",\"N\"),k,replace=T)\r\n\r\n\r\n\r\nNext, we want that our married clients are slightly older than our non-married clients. For this example we just assume that the average age of the married clients is 40, and the average age of the non-married clients is 30.\r\n\r\n\r\ndata <- data.frame(id=1:k,married) %>% \r\n  mutate(\r\n    age=ifelse(married==\"Y\", rnorm(k, 40, sd = 10), rnorm(k, 30, sd= 12)) %>% \r\n      pmax(18) %>% #every client should be at least 18\r\n      round()\r\n    )\r\n\r\nhead(data)\r\n\r\n\r\n  id married age\r\n1  1       Y  31\r\n2  2       Y  54\r\n3  3       N  18\r\n4  4       N  30\r\n5  5       Y  37\r\n6  6       Y  61\r\n\r\nWe can take a quick look if the difference is visible in a boxplot.\r\n\r\n\r\n\r\nIf you have more than two options, case_when() can help. We want to see the balance of clients which are either managers, analysts or senior analysts.\r\n\r\n\r\nk <- 1000\r\n\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\n\r\ndata <- data.frame(id=1:k,ocupation)\r\n\r\ndata <- data %>% mutate(balance=case_when(\r\n  ocupation==\"analyst\" ~ 100+rexp(k,0.01),\r\n  ocupation==\"sr analyst\" ~ 200+rexp(k,0.005),\r\n  TRUE ~ 200+rexp(k,0.001) #this is the else case\r\n))\r\n\r\n#Check the average balance per group\r\ndata %>% \r\n  ggplot(aes(x=ocupation,y=balance))+geom_violin()\r\n\r\n\r\n\r\n\r\nCorrelation based\r\nIf we just deal with numeric variables and want to have a slightly more complex connection between the variables, we can also try another approach, for which we specify a correlation matrix beforehand and reorder our variables afterwards so that they match the desired correlation.\r\nOf course, we need to find reasonable correlation values, for example between age and number of kids (slightly positively correlated) or between savings and number of kids (slightly negatively correlated). This requires some research.\r\nFirst, we simulate the data independently. Ideas about how to do this can be found in this blogpost.\r\n\r\n\r\nset.seed(64)\r\n\r\nk <- 2000\r\n\r\nage <- rnorm(k,mean=35,sd=10) %>% pmax(18) %>% round()\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\ntenure <- rnorm(k,mean=15,sd=5) %>% pmax(1) %>% round()\r\nkids_cnt <- sample(0:5,k,replace=T,prob=c(100,120,80,30,5,1))\r\n\r\n\r\ndata <- data.frame(age,balance,kids_cnt,tenure)\r\ndata %>% head(7)\r\n\r\n\r\n  age balance kids_cnt tenure\r\n1  18 3665.34        2     10\r\n2  18  268.55        2      8\r\n3  22 1628.59        0     22\r\n4  50 1995.58        1     12\r\n5  35 1510.58        0     20\r\n6  32   58.58        0      5\r\n7  45  945.11        0     21\r\n\r\nWe directly see that there are things that don’t make too much sense, like the 22-years-old with a tenure of 22 years. Further, there is no dependence between the variables.\r\nTo improve this, we want to reshuffle the rows and get a correlation close to a desired one. First we simulate a helping dataset of same size, where every entry is random normal distributed.\r\n\r\n\r\nnvars <- ncol(data)\r\nnumobs <- nrow(data)\r\n\r\nset.seed(3)\r\nrnorm_helper <- matrix(rnorm(nvars*numobs,0,1),nrow=nvars)\r\n\r\n\r\n\r\nThe correlation of this matrix should be close to the identity matrix.\r\n\r\n\r\ncor(t(rnorm_helper))\r\n\r\n\r\n             [,1]        [,2]        [,3]         [,4]\r\n[1,]  1.000000000 -0.00574905 0.009783835 -0.023569599\r\n[2,] -0.005749050  1.00000000 0.049500977  0.010347672\r\n[3,]  0.009783835  0.04950098 1.000000000  0.005859748\r\n[4,] -0.023569599  0.01034767 0.005859748  1.000000000\r\n\r\nNext, we specify our desired correlation matrix. Just to put this in words, we want to correlate the four variables age, balance, kids_cnt and tenure. Each variable with itself has a correlation of 1. We want age and balance to have a positive correlation of 0.3, age and kids_cnt of 0.4 and age and tenure of 0.2. Likewise, we specify all desired correlations between pairs of variables.\r\n\r\n\r\nQ <- matrix(c(1,0.3,0.4,0.2,  0.3,1,0,0.3,  0.4,0,1,-0.3,  0.2,0.3,-0.3,1),ncol=nvars)\r\n\r\nQ\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0  0.0  0.3\r\n[3,]  0.4  0.0  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nWe can now multiply the rnorm_helper matrix with the Cholesky decomposition of our desired correlation matrix Q. Why this works, is explained in the following comment. If you are not interested in mathematical details, you can skip this part.\r\n\r\n(Explanation found here)\r\n\r\n\r\nL <- t(chol(Q))\r\nZ <- L %*% rnorm_helper\r\n\r\n\r\n\r\nGood, now we convert this new data to a data frame and name it like our original data.\r\n\r\n\r\nraw <- as.data.frame(t(Z),row.names=NULL,optional=FALSE)\r\nnames(raw) <- names(data)\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n         age     balance   kids_cnt     tenure\r\n1 -0.9619334 -0.56763178 -0.1130367 -1.3627041\r\n2  0.1957828  0.08747126  0.1520695  0.9806283\r\n3 -1.2188574  0.84333548 -1.3231141 -0.6184486\r\n4 -0.7163585  0.02610745 -0.1802922 -0.4043929\r\n5 -0.9530173 -0.90428943  0.8118207 -0.6505000\r\n6 -0.5784837 -1.07244273 -0.2978104 -1.7172915\r\n7 -0.4844551 -0.85227479  0.9530955  0.1474831\r\n\r\nThe correlation of this dataset is close to our desired outcome.\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance    kids_cnt     tenure\r\nage      1.0000000 0.29376273  0.39741190  0.1708577\r\nbalance  0.2937627 1.00000000  0.04302845  0.2777607\r\nkids_cnt 0.3974119 0.04302845  1.00000000 -0.3046079\r\ntenure   0.1708577 0.27776070 -0.30460794  1.0000000\r\n\r\nHowever, this dataset raw does not have anything to do with our original data. It is still only transformed random normal data. But as we know that this dataset has the correct correlation, we can use this to reorder the rows of our other dataset.\r\nAnd then we just replace the largest value of the random normal dataset with the largest value in our dataset, the second largest with the second largest etc. We go column by column and repeat this procedure.\r\n\r\n\r\nfor(name in names(raw)) {\r\n  raw <- raw[order(raw[,name]),]\r\n  data <- data[order(data[,name]),]\r\n  raw[,name] <- data[,name]\r\n}\r\n\r\n\r\n\r\nLet’s check the correlation of this new dataset. It is close to our desired correlation matrix. The main reason for the small difference is that our variables take less values than a random normal distributed variable (e.g. kids count just takes values between 0 and 5).\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance    kids_cnt     tenure\r\nage      1.0000000 0.24231169  0.36996781  0.1734406\r\nbalance  0.2423117 1.00000000  0.01880674  0.2358795\r\nkids_cnt 0.3699678 0.01880674  1.00000000 -0.3038222\r\ntenure   0.1734406 0.23587953 -0.30382217  1.0000000\r\n\r\nQ\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0  0.0  0.3\r\n[3,]  0.4  0.0  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nOur final reshuffled and correctly correlated dataset is now stored in raw.\r\n\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n     age balance kids_cnt tenure\r\n1934  34   36.36        3      1\r\n733   37   52.44        1      1\r\n123   22  290.91        2      1\r\n1032  26  130.01        2      1\r\n1463  32   88.87        2      1\r\n448   43   26.54        4      1\r\n1804  35  911.63        2      2\r\n\r\nClosing comments\r\nIf you like the correlation method please take a look at the GenOrd package which is a little more professional, when working with ordinal categorical variables.\r\nThe Cholesky decomposition is only possible for positive definite matrices. If this is not the case and you accept a slightly stronger deviation from your desired correlation matrix, the easiest way is to add 0.1, 0.2 etc. to the diagonals until you obtain a positive definite matrix. Note that this lowers the correlation between all variables.\r\n\r\n\r\ndiag(nvars) * 0.1 + Q\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.1  0.3  0.4  0.2\r\n[2,]  0.3  1.1  0.0  0.3\r\n[3,]  0.4  0.0  1.1 -0.3\r\n[4,]  0.2  0.3 -0.3  1.1\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-03-simulate-dependent-variables/simulate-dependent-variables_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-12-30T21:17:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-03-useful-packages-for-data-composition/",
    "title": "Useful packages for data simulation",
    "description": "We will explore the packages wakefield, rcorpora, charlatan, fabricatr, and GenOrd which can be helpful for data simulation.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-12-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAdditional packages\r\nwakefieldSeries\r\n\r\nrcorpora\r\ncharlatan\r\nfabricatrOrdered data\r\nTime series\r\n\r\nGenOrd\r\nMore packages\r\n\r\nWhen we simulate data we can rely on the distribution functions like rnorm, rexp and sample from base R. However, we can also leverage the great work from authors of packages which were written to make the simulation process easier. In this blogpost I will explore some of them.\r\nAdditional packages\r\nBefore starting with the simulation packages, we can load these two packages which will help with data transformation and visualization.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nwakefield\r\nLooking for interesting packages around data simulation I stumbled across the {wakefield} package by Tyler Rinker.\r\n\r\n\r\nlibrary(wakefield)\r\n\r\n\r\n\r\nIntroduction can be found here.\r\n\r\n\r\nr_data_frame(\r\n    n = 500,\r\n    id,\r\n    age,\r\n    iq,\r\n    height,\r\n    died,\r\n    animal,\r\n    internet_browser,\r\n    political\r\n)\r\n\r\n\r\n# A tibble: 500 x 8\r\n   ID      Age    IQ Height Died  Animal            Browser Political \r\n   <chr> <int> <dbl>  <dbl> <lgl> <fct>             <fct>   <fct>     \r\n 1 001      61   109     63 FALSE Siamese           Firefox Democrat  \r\n 2 002      28    96     70 FALSE Cesky Fousek      Chrome  Democrat  \r\n 3 003      32   100     73 TRUE  Red-Handed Tamar~ Chrome  Democrat  \r\n 4 004      29   105     72 FALSE Red-Handed Tamar~ Safari  Democrat  \r\n 5 005      89    99     67 TRUE  Poison Dart Frog  Chrome  Republican\r\n 6 006      32    72     69 FALSE Poison Dart Frog  Chrome  Democrat  \r\n 7 007      88   101     69 TRUE  Striped Rocket F~ Safari  Democrat  \r\n 8 008      37   111     67 FALSE Giraffe           Chrome  Libertari~\r\n 9 009      59    96     66 TRUE  Liger             Chrome  Democrat  \r\n10 010      50    98     69 FALSE Fly               Safari  Republican\r\n# ... with 490 more rows\r\n\r\nThere are a lot of predefined variables that you can use. (Call variables(type=\"matrix\",ncols=5) to see them.)\r\n\r\n      [,1]         [,2]          [,3]               [,4]       \r\n [1,] \"age\"        \"dice\"        \"hair\"             \"military\" \r\n [2,] \"animal\"     \"dna\"         \"height\"           \"month\"    \r\n [3,] \"answer\"     \"dob\"         \"income\"           \"name\"     \r\n [4,] \"area\"       \"dummy\"       \"internet_browser\" \"normal\"   \r\n [5,] \"car\"        \"education\"   \"iq\"               \"political\"\r\n [6,] \"children\"   \"employment\"  \"language\"         \"race\"     \r\n [7,] \"coin\"       \"eye\"         \"level\"            \"religion\" \r\n [8,] \"color\"      \"grade\"       \"likert\"           \"sat\"      \r\n [9,] \"date_stamp\" \"grade_level\" \"lorem_ipsum\"      \"sentence\" \r\n[10,] \"death\"      \"group\"       \"marital\"          \"sex\"      \r\n      [,5]           \r\n [1,] \"sex_inclusive\"\r\n [2,] \"smokes\"       \r\n [3,] \"speed\"        \r\n [4,] \"state\"        \r\n [5,] \"string\"       \r\n [6,] \"upper\"        \r\n [7,] \"valid\"        \r\n [8,] \"year\"         \r\n [9,] \"zip_code\"     \r\n[10,]                \r\nattr(,\"class\")\r\n[1] \"matrix\" \"array\" \r\n\r\nAdditionally, you can access the distribution functions easily and tweak parameters of the predefined functions.\r\n\r\n\r\ntest <- r_data_frame(\r\n    n = 500,\r\n    id,\r\n    age(x=18:50),\r\n    `Reading(mins)` = rpois(lambda=20),\r\n    income(digits=0)\r\n)\r\n\r\n\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      48              17  17263\r\n 2 002      49               9  35240\r\n 3 003      22              21  19994\r\n 4 004      22              14  14416\r\n 5 005      39              21  57343\r\n 6 006      36              12   1651\r\n 7 007      36              24  15642\r\n 8 008      31              17  12652\r\n 9 009      43              20  26321\r\n10 010      36              22  45015\r\n# ... with 490 more rows\r\n\r\nLooks too perfect? Include random missing values in columns 2 and 4:\r\n\r\n\r\ntest <- test %>% r_na(cols=c(2,4),prob=0.3)\r\n\r\n\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      48              17  17263\r\n 2 002      NA               9     NA\r\n 3 003      22              21  19994\r\n 4 004      22              14  14416\r\n 5 005      39              21     NA\r\n 6 006      36              12   1651\r\n 7 007      NA              24  15642\r\n 8 008      NA              17     NA\r\n 9 009      NA              20     NA\r\n10 010      36              22  45015\r\n# ... with 490 more rows\r\n\r\nSeries\r\n{wakefield} allows us to create several variables which can be seen as a sequence, for example survey results.\r\n\r\n\r\nr_series(likert,j = 5,n=10,name=\"Question\")\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Question_1    Question_2    Question_3    Question_4   Question_5  \r\n * <ord>         <ord>         <ord>         <ord>        <ord>       \r\n 1 Neutral       Disagree      Neutral       Strongly Ag~ Disagree    \r\n 2 Neutral       Strongly Dis~ Neutral       Strongly Ag~ Strongly Ag~\r\n 3 Neutral       Strongly Dis~ Strongly Agr~ Strongly Di~ Strongly Ag~\r\n 4 Neutral       Disagree      Strongly Agr~ Strongly Ag~ Disagree    \r\n 5 Agree         Strongly Agr~ Neutral       Disagree     Neutral     \r\n 6 Strongly Agr~ Neutral       Strongly Dis~ Agree        Strongly Ag~\r\n 7 Strongly Agr~ Disagree      Strongly Dis~ Agree        Neutral     \r\n 8 Strongly Dis~ Agree         Disagree      Neutral      Strongly Di~\r\n 9 Strongly Dis~ Strongly Dis~ Strongly Agr~ Agree        Strongly Ag~\r\n10 Neutral       Agree         Agree         Neutral      Strongly Di~\r\n\r\nThese can also be packaged inside a data frame, for example when simulating test results for students.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1 Test_2 Test_3\r\n   <chr>   <int>  <int>  <int>  <int>\r\n 1 01         16     92     88     81\r\n 2 02         17     92     88     91\r\n 3 03         12     95     89     86\r\n 4 04         11     91     87     77\r\n 5 05         16     91     86     89\r\n 6 06         19     87     89     84\r\n 7 07         24     90     94     94\r\n 8 08         13     88     84     89\r\n 9 09         15     87     87     88\r\n10 10         17     91     85     83\r\n\r\nThat is great but not very real, because the test results are completely independent from each other. The relate parameter inside the r_series function helps to connect the results, and the format is fM_sd.\r\nf is one of (+,-,*,/)\r\nM is the mean value\r\nsd is the standard deviation of the mean value\r\nExamples: * +3_1: The test results get better on average 3 points with a standard deviation of 1. * *1.05_0.2: The results get better on average 5% with a standard deviation of 0.2.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\",relate=\"+3_1\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1     Test_2     Test_3    \r\n   <chr>   <int> <variable> <variable> <variable>\r\n 1 01         15 96.2       98.2       103.2     \r\n 2 02         17 83.6       86.0        88.9     \r\n 3 03         11 93.2       96.4        99.1     \r\n 4 04         12 86.3       87.9        90.4     \r\n 5 05          7 91.2       93.7        97.2     \r\n 6 06          8 89.5       91.8        94.7     \r\n 7 07         26 88.7       93.1        95.9     \r\n 8 08          7 89.8       91.9        94.5     \r\n 9 09         13 84.9       88.5        91.5     \r\n10 10         23 87.9       90.2        94.1     \r\n\r\nWith this in mind, you can create customer balances over time very easily.\r\n\r\n\r\nbalances <- r_data_frame(\r\n  n=10,\r\n  Client=name,\r\n  age,\r\n  r_series(income,j=12,name=\"Month\",relate=\"*1.03_0.1\")\r\n)\r\n\r\n\r\n\r\nThis result is worth to be visualized.\r\n\r\n\r\nbalances %>%\r\n  tidyr::pivot_longer(-c(1,2),names_to=\"Month\") %>%\r\n  mutate(Month=readr::parse_number(Month)) %>%\r\n  ggplot(aes(x=Month,y=value))+geom_line()+facet_wrap(~Client,scales=\"free_y\")\r\n\r\n\r\n\r\n\r\nWe can see that there are customers who had very positive balance development and others whose balances were fluctuating more or declining. However, when we simulate a sufficiently large number of customers, we will observe that on average the increase each month will be the desired 3% with a standard deviation of 0.1.\r\nrcorpora\r\nCheck the github repository here.\r\nThe rcorpora library has 293 collections of words that can be very helpful for data simulation.\r\n\r\n\r\nlibrary(rcorpora)\r\n\r\nlength(corpora())\r\n\r\n\r\n[1] 293\r\n\r\ncorpora()[sample(1:293,10)]\r\n\r\n\r\n [1] \"mythology/greek_titans\"          \r\n [2] \"words/us_president_quotes\"       \r\n [3] \"words/literature/lovecraft_words\"\r\n [4] \"colors/paints\"                   \r\n [5] \"humans/lastNames\"                \r\n [6] \"games/wrestling_moves\"           \r\n [7] \"geography/countries\"             \r\n [8] \"words/stopwords/gr\"              \r\n [9] \"film-tv/game-of-thrones-houses\"  \r\n[10] \"religion/parody_religions\"       \r\n\r\nTo view the words of one collection use the name in the corpora() function.\r\n\r\n\r\ncorpora(\"foods/pizzaToppings\")\r\n\r\n\r\n$description\r\n[1] \"A list of pizza toppings.\"\r\n\r\n$pizzaToppings\r\n [1] \"anchovies\"        \"artichoke\"        \"bacon\"           \r\n [4] \"breakfast bacon\"  \"Canadian bacon\"   \"cheese\"          \r\n [7] \"chicken\"          \"chili peppers\"    \"feta\"            \r\n[10] \"garlic\"           \"green peppers\"    \"grilled onions\"  \r\n[13] \"ground beef\"      \"ham\"              \"hot sauce\"       \r\n[16] \"meatballs\"        \"mushrooms\"        \"olives\"          \r\n[19] \"onions\"           \"pepperoni\"        \"pineapple\"       \r\n[22] \"sausage\"          \"spinach\"          \"sun-dried tomato\"\r\n[25] \"tomatoes\"        \r\n\r\nLet see how we can use this in a simulated dataframe.\r\n\r\n\r\ntibble(\r\n  first_name=corpora(\"humans/firstNames\")$firstNames %>% sample(100,replace=TRUE),\r\n  last_name=corpora(\"humans/lastNames\")$lastNames %>% sample(100,replace=TRUE),\r\n  self_description=corpora(\"humans/descriptions\")$descriptions %>% sample(100,replace=TRUE),\r\n  home_country=corpora(\"geography/countries\")$countries %>% sample(100,replace=TRUE),\r\n  favorite_pizza_topping=corpora(\"foods/pizzaToppings\")$pizzaToppings %>% sample(100,replace=TRUE)\r\n)\r\n\r\n\r\n# A tibble: 100 x 5\r\n   first_name last_name self_description home_country favorite_pizza_~\r\n   <chr>      <chr>     <chr>            <chr>        <chr>           \r\n 1 Ryan       Allen     sturdy           United Arab~ mushrooms       \r\n 2 Victor     Boyd      condescending    St. Vincent~ spinach         \r\n 3 Jason      Gibson    distraught       Sudan        mushrooms       \r\n 4 Courtney   Stevens   unpopular        Kyrgyzstan   chili peppers   \r\n 5 Adrianna   Simmons   stupid           Denmark      spinach         \r\n 6 Brittany   Freeman   blunt            Benin        green peppers   \r\n 7 Angelina   Jenkins   great            Panama       artichoke       \r\n 8 Natalia    Morris    incompetent      Kuwait       grilled onions  \r\n 9 Adrianna   Lee       disruptive       Kenya        chicken         \r\n10 Andres     Fox       unwilling        China        olives          \r\n# ... with 90 more rows\r\n\r\ncharlatan\r\nSimilar to wakefield, charlatan has some out-of-the-box variables that can be used in your simulated data.\r\n\r\n\r\nlibrary(charlatan)\r\n\r\nch_job(n=10)\r\n\r\n\r\n [1] \"Travel agency manager\"           \r\n [2] \"Higher education careers adviser\"\r\n [3] \"Psychotherapist, dance movement\" \r\n [4] \"Armed forces technical officer\"  \r\n [5] \"Solicitor\"                       \r\n [6] \"Sales professional, IT\"          \r\n [7] \"Journalist, newspaper\"           \r\n [8] \"Facilities manager\"              \r\n [9] \"Tourism officer\"                 \r\n[10] \"Civil engineer, consulting\"      \r\n\r\nYou can even use get typical names or jobs for a given country. To see the available languages and countries type charlatan::PersonProvider$new()$allowed_locales().\r\n\r\n\r\nch_name(n=10,locale=\"de_DE\")\r\n\r\n\r\n [1] \"Tony Walter\"              \"Prof. Annette Putz\"      \r\n [3] \"Damian Harloff\"           \"Rosl Röhrdanz B.Eng.\"    \r\n [5] \"Anastasia Bachmann\"       \"Käte Kusch-Etzold\"       \r\n [7] \"Erdmute Lübs-Mende\"       \"Dipl.-Ing. Wulf Hoffmann\"\r\n [9] \"Ing. Marlies Hein B.Eng.\" \"Maurizio Freudenberger\"  \r\n\r\n\r\n\r\nch_phone_number(locale=\"de_DE\",n=10)\r\n\r\n\r\n [1] \"08260 03885\"       \"+49(0)8493552007\"  \"+49(0)8452 661356\"\r\n [4] \"0999133293\"        \"+49(0)9336402927\"  \"07347743915\"      \r\n [7] \"(01351) 368050\"    \"+49(0)9727884448\"  \"+49(0)2327 278014\"\r\n[10] \"01620 161222\"     \r\n\r\nA nice small application with fake locations and random R colors.\r\n\r\n\r\nlocations <- data.frame(lon=ch_lon(n=10),lat=ch_lat(n=10),col=ch_color_name(n=10))\r\n\r\nggplot(locations)+\r\n  borders(\"world\")+\r\n  geom_point(aes(x=lon,y=lat,col=col),size=3)+\r\n  coord_quickmap()\r\n\r\n\r\n\r\n\r\nfabricatr\r\nEasy creation of hierarchical data is possible with {fabricatr}. In this example there are five families, each one has between 1 and 12 members. Each family member has between 1 and 5 accounts. With add_level() we can automatically produce a table that shows all accounts of all members in all families.\r\n\r\n\r\nlibrary(fabricatr)\r\n\r\nfabricate(\r\n  family  = add_level(N = 5,\r\n  n_members = sample(1:12, N, replace = TRUE,prob=12:1)),\r\n  \r\n  members  = add_level(N = n_members,\r\n  n_accounts = sample(1:5,N,replace=TRUE,prob=(5:1)^2)),\r\n  \r\n  account = add_level(N = n_accounts)\r\n  ) %>%\r\nhead(10)\r\n\r\n\r\n   family n_members members n_accounts account\r\n1       1         2      01          1      01\r\n2       1         2      02          1      02\r\n3       2         4      03          1      03\r\n4       2         4      04          1      04\r\n5       2         4      05          2      05\r\n6       2         4      05          2      06\r\n7       2         4      06          2      07\r\n8       2         4      06          2      08\r\n9       3         1      07          1      09\r\n10      4         4      08          1      10\r\n\r\nLink levels. We can create 15 clients with their birth year and join year and some correlation between both variables.\r\n\r\n\r\ndf <- fabricate(\r\n  age = add_level(N=51, birth_year=1950:2000),\r\n  tenure = add_level(N = 20, join_year=1991:2010, nest = FALSE),\r\n  client = link_levels(N = 15, by = join(age, tenure, rho = 0.7))\r\n)\r\n\r\ndf %>% select(client,birth_year,join_year)\r\n\r\n\r\n   client birth_year join_year\r\n1      01       1979      2005\r\n2      02       1974      2007\r\n3      03       1958      1996\r\n4      04       1993      2004\r\n5      05       1960      1995\r\n6      06       1996      1997\r\n7      07       1980      1996\r\n8      08       1992      2004\r\n9      09       1964      1996\r\n10     10       1976      1999\r\n11     11       1983      1998\r\n12     12       1950      1991\r\n13     13       1976      1994\r\n14     14       1991      2001\r\n15     15       1992      1994\r\n\r\nOrdered data\r\nfabricatr has an amazing function to create ordered categorical data.\r\nThe function we need is draw_ordered. It internally simulates a numeric variable (x) and breaks them into predefined categories.\r\n\r\n\r\ndraw_ordered(\r\n  x = rnorm(10),\r\n  breaks = c(-2,-1,0.8,2),\r\n  break_labels = c(\"Very boring\",\"Boring\",\"OK\",\"Interesting\",\"Very Interesting\")\r\n)\r\n\r\n\r\n [1] Interesting      OK               Interesting     \r\n [4] OK               OK               Interesting     \r\n [7] OK               OK               OK              \r\n[10] Very Interesting\r\nLevels: Very boring Boring OK Interesting Very Interesting\r\n\r\nLet’s take a look at another example where we have two types of clients, gold clients that receive a yearly gift from the bank and standard clients that do not. How could we simulate their responses to a satisfaction survey?\r\n\r\n\r\ndf <- fabricate(\r\n  N = 100,\r\n  gold_client_flag = draw_binary(prob = 0.3, N),\r\n  satisfaction = draw_ordered(\r\n    x = rnorm(N, mean = -0.4 + 1.2 * gold_client_flag),\r\n    breaks = c(-1.5, -0.5, 0.5, 1.5),\r\n    break_labels = c(\"Very Unsatisfied\", \"Unsatisfied\", \"Neutral\",\r\n                     \"Satisfied\", \"Very Satisfied\")\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n   ID gold_client_flag     satisfaction\r\n1 001                0          Neutral\r\n2 002                1        Satisfied\r\n3 003                0          Neutral\r\n4 004                0 Very Unsatisfied\r\n5 005                0          Neutral\r\n6 006                0          Neutral\r\n\r\nWe can summarize the results and see the differences between the two groups. Ideal data for teaching hypothesis testing.\r\n\r\n\r\ndf %>% count(gold_client_flag,satisfaction) %>%\r\n  tidyr::pivot_wider(id_cols=satisfaction,names_from=\"gold_client_flag\",values_from=\"n\")\r\n\r\n\r\n# A tibble: 5 x 3\r\n  satisfaction       `0`   `1`\r\n  <fct>            <int> <int>\r\n1 Very Unsatisfied     9     1\r\n2 Unsatisfied         21     2\r\n3 Neutral             25    10\r\n4 Satisfied           12    11\r\n5 Very Satisfied       2     7\r\n\r\nTime series\r\nExample from this article.\r\nThis example contains the GDP of five countries over the course of five years.\r\n\r\n\r\npanel_units <- fabricate(\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.8),\r\n    growth_error = runif(N, 0.1, 0.5)\r\n  ),\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    gdp_measure = base_gdp + (ts_year * growth_units) + rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\npanel_units\r\n\r\n\r\n   countries base_gdp growth_units growth_error years ts_year\r\n1          1 19.49269    0.6806240    0.1792934    01       0\r\n2          1 19.49269    0.6806240    0.1792934    02       1\r\n3          1 19.49269    0.6806240    0.1792934    03       2\r\n4          1 19.49269    0.6806240    0.1792934    04       3\r\n5          1 19.49269    0.6806240    0.1792934    05       4\r\n6          2 19.76148    0.3870006    0.3199218    06       0\r\n7          2 19.76148    0.3870006    0.3199218    07       1\r\n8          2 19.76148    0.3870006    0.3199218    08       2\r\n9          2 19.76148    0.3870006    0.3199218    09       3\r\n10         2 19.76148    0.3870006    0.3199218    10       4\r\n11         3 21.88243    0.2711517    0.1089617    11       0\r\n12         3 21.88243    0.2711517    0.1089617    12       1\r\n13         3 21.88243    0.2711517    0.1089617    13       2\r\n14         3 21.88243    0.2711517    0.1089617    14       3\r\n15         3 21.88243    0.2711517    0.1089617    15       4\r\n16         4 20.53214    0.5544189    0.2329021    16       0\r\n17         4 20.53214    0.5544189    0.2329021    17       1\r\n18         4 20.53214    0.5544189    0.2329021    18       2\r\n19         4 20.53214    0.5544189    0.2329021    19       3\r\n20         4 20.53214    0.5544189    0.2329021    20       4\r\n21         5 19.87462    0.3846960    0.2351165    21       0\r\n22         5 19.87462    0.3846960    0.2351165    22       1\r\n23         5 19.87462    0.3846960    0.2351165    23       2\r\n24         5 19.87462    0.3846960    0.2351165    24       3\r\n25         5 19.87462    0.3846960    0.2351165    25       4\r\n   gdp_measure\r\n1     19.21336\r\n2     20.56162\r\n3     21.01361\r\n4     21.56555\r\n5     22.32086\r\n6     20.02920\r\n7     19.99657\r\n8     21.21771\r\n9     20.50072\r\n10    20.92311\r\n11    21.97651\r\n12    22.25644\r\n13    22.20033\r\n14    22.93052\r\n15    23.04249\r\n16    20.63399\r\n17    21.38011\r\n18    21.48930\r\n19    22.31458\r\n20    22.81663\r\n21    19.76463\r\n22    20.62171\r\n23    20.72968\r\n24    21.25291\r\n25    21.67064\r\n\r\n\r\n\r\n\r\nWe can take this to the next level and introduce some year specific information and then cross this with the country specific information. We just have to add one layer.\r\n\r\n\r\npanel_global_data <- fabricate(\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    year_shock = rnorm(N, 0, 0.5) #each year has a global trend\r\n  ),\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.5), \r\n    growth_error = runif(N, 0.1, 0.5),\r\n    nest = FALSE\r\n  ),\r\n  country_years = cross_levels(\r\n    by = join(years, countries),\r\n    gdp_measure = base_gdp + year_shock + (ts_year * growth_units) +\r\n      rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nGenOrd\r\nThis package helps to create discrete random variables with prescribed correlation matrix and marginal distributions.\r\n\r\n\r\nlibrary(GenOrd)\r\n\r\n\r\nk <- 4 #number of random variables\r\nmarginal <- list(0.6, c(1/3,2/3), c(1/4,2/4,3/4), c(1/5,2/5,3/5,4/5))\r\n\r\n\r\n\r\nRead the list as follows: * We will create 4 random variables. * The first variable will have two values: 60% of the data will be 1, 40% will be 2. * The second variable will have three values, 1,2 and 3 with a probability of 33% each. * etc… * Each vector in this list refers to one variable, and we will see the cumulative probability for each value.\r\n\r\n\r\ncorrcheck(marginal)\r\n\r\n\r\n[[1]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n           [,1]       [,2]       [,3]       [,4]\r\n[1,]  1.0000000 -0.8333333 -0.8215838 -0.8660254\r\n[2,] -0.8333333  1.0000000 -0.9128709 -0.9237604\r\n[3,] -0.8215838 -0.9128709  1.0000000 -0.9486833\r\n[4,] -0.8660254 -0.9237604 -0.9486833  1.0000000\r\n\r\n[[2]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n          [,1]      [,2]      [,3]      [,4]\r\n[1,] 1.0000000 0.8333333 0.8215838 0.8660254\r\n[2,] 0.8333333 1.0000000 0.9128709 0.9237604\r\n[3,] 0.8215838 0.9128709 1.0000000 0.9486833\r\n[4,] 0.8660254 0.9237604 0.9486833 1.0000000\r\n\r\nThis function shows what are allowable ranges for the correlation matrix, given the input from the marginal distributions.\r\n\r\n\r\nSigma <- matrix(c(1,0.5,0.4,0.3,\r\n                  0.5,1,0.5,0.4,\r\n                  0.4,0.5,1,0.5,\r\n                  0.3,0.4,0.5,1),\r\n                k, k, byrow=TRUE)\r\n\r\n\r\n\r\nWe will create 1000 observations, with the given correlation matrix. Each variable will have the marginal distribution described above.\r\n\r\n\r\nn <- 1000 # sample size\r\nm <- ordsample(n, marginal, Sigma)\r\n\r\ndf <- data.frame(m)\r\nhead(df)\r\n\r\n\r\n  X1 X2 X3 X4\r\n1  2  3  1  5\r\n2  2  3  3  2\r\n3  1  1  2  3\r\n4  1  2  2  2\r\n5  2  3  4  3\r\n6  2  1  4  5\r\n\r\nLet’s verify that the data is actually what we expected. We check the correlation and the marginal distribution for two of the variables.\r\n\r\n\r\ncor(df)\r\n\r\n\r\n          X1        X2        X3        X4\r\nX1 1.0000000 0.5206230 0.3987477 0.3544100\r\nX2 0.5206230 1.0000000 0.4895694 0.4059342\r\nX3 0.3987477 0.4895694 1.0000000 0.5008865\r\nX4 0.3544100 0.4059342 0.5008865 1.0000000\r\n\r\ndf %>% count(X4)\r\n\r\n\r\n  X4   n\r\n1  1 187\r\n2  2 229\r\n3  3 208\r\n4  4 190\r\n5  5 186\r\n\r\ndf %>% count(X1)\r\n\r\n\r\n  X1   n\r\n1  1 591\r\n2  2 409\r\n\r\nLater we can rename the columns and values, but will have assured that they have the desired correlations.\r\nMore packages\r\nIn this blogpost by Joseph Rickert on R Views.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-03-useful-packages-for-data-composition/useful-packages-for-data-composition_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2020-12-30T18:16:35+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-13-data-composition-with-rmultinom/",
    "title": "Data simulation with rmultinom",
    "description": "When creating several datasets that depend on each other, the rmultinom function from the stats package can be a useful helper. In this example we will see how to create customer transactions from a customer table.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-11-13",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAutomate this for all customers\r\nOther possible applications\r\n\r\nIn this article I want to show how the rmultinom() function can help to simulate data. We will simulate client data, and for each client we will create transactions.\r\nThe rmultinom() function simulates the multinomial distribution (Link).\r\nIn my head I always picture the multinomial distribution as a game setup. You have N balls and K bins. Instead of the number of bins, we send a vector of probabilities (of length K), how likely it is for the balls to land in each bin (you can imagine that some bins are closer and others are further away, or that some are larger than others). This vector will be normalized automatically, so you do not have to worry about this.\r\nLet’s try an example, with N=1000 and K=5. We want one of the bins to be twice as large as the others.\r\n\r\n\r\ntest1 <- rmultinom(n=1,size=1000,c(2,1,1,1,1))\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote: The rmultinom() function with n=1 is similar to the sample() function. You would obtain an equivalent result with table(sample(1:5,1000,replace=TRUE,prob=c(2,1,1,1,1))) but I believe that rmultinom() is more elegant.\r\nHow can we use this function to create transactions for a given number of customers? The key is to simulate all important values on client level and use rmultinom to decompose the values into smaller portions. First, let’s get some clients.\r\n\r\n\r\nset.seed(61)\r\n\r\nage <- rnorm(10,mean=50,sd=15) %>% pmax(18) %>% round()\r\ntenure <- (age - 18 - runif(10,1,30)) %>% pmax(0) %>% round()\r\nincome <- rexp(10,0.0001) %>% round(2)\r\n\r\nclient <- data.frame(id=1:10,age,tenure,income)\r\n\r\nclient\r\n\r\n\r\n   id age tenure   income\r\n1   1  44     22 32181.33\r\n2   2  44      7  5454.56\r\n3   3  24      3  4559.78\r\n4   4  55     34 26790.03\r\n5   5  29      0 18592.27\r\n6   6  47     19 40229.93\r\n7   7  61     14  1065.49\r\n8   8  58     17  2750.75\r\n9   9  71     49 12292.53\r\n10 10  45     15   282.05\r\n\r\nFor this exercise, we do not distinguish between different types of transactions. In practice, it would make sense to separate rent, supermarket, transport and other categories.\r\nWe create a second dataframe for clients, which contains “invisible” information needed for the transactions. Let’s begin with the total spending. This can depend on anything we know about the client. In this case, we will assume that each client has more or less the same behavior and spends around 70% of their income. The standard deviation of 0.1 assures that this value varies from client to client.\r\n\r\n\r\ncl_secret_info <- client\r\n\r\ncl_secret_info$total_spend <- (cl_secret_info$income * rnorm(10,0.7,sd=0.1)) %>% round(2)\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend\r\n1   1  44     22 32181.33    20755.16\r\n2   2  44      7  5454.56     4342.34\r\n3   3  24      3  4559.78     3280.65\r\n4   4  55     34 26790.03    21012.47\r\n5   5  29      0 18592.27    13990.48\r\n6   6  47     19 40229.93    25986.32\r\n7   7  61     14  1065.49      699.14\r\n8   8  58     17  2750.75     2158.39\r\n9   9  71     49 12292.53    10135.33\r\n10 10  45     15   282.05      160.08\r\n\r\nThe next ingredient is the number of transactions. For this example, we create a formula depending on age: Clients which are younger than 50 have (on average) a higher number of transactions per month.\r\n\r\n\r\ncl_secret_info$n_trans <- ifelse(cl_secret_info$age < 50, rbinom(10,60,0.5),rbinom(10,60,0.3))\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans\r\n1   1  44     22 32181.33    20755.16      27\r\n2   2  44      7  5454.56     4342.34      37\r\n3   3  24      3  4559.78     3280.65      28\r\n4   4  55     34 26790.03    21012.47      25\r\n5   5  29      0 18592.27    13990.48      32\r\n6   6  47     19 40229.93    25986.32      27\r\n7   7  61     14  1065.49      699.14      11\r\n8   8  58     17  2750.75     2158.39      14\r\n9   9  71     49 12292.53    10135.33      16\r\n10 10  45     15   282.05      160.08      27\r\n\r\nNow we already know that our transaction table will have 244 rows, the sum of all our 10 clients’ transactions.\r\nWe will create a last parameter which is an indicator of how similar the transactions are. You could split $100 into one large transaction of $80 and four small transactions of $5 each or you could have five transactions of around $20 each. The higher the value of diff_trans the higher the variability within the transactions of a client.\r\n\r\n\r\ncl_secret_info$diff_trans <- rexp(10,100/cl_secret_info$total_spend) %>% ceiling()\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans diff_trans\r\n1   1  44     22 32181.33    20755.16      27        328\r\n2   2  44      7  5454.56     4342.34      37         55\r\n3   3  24      3  4559.78     3280.65      28         38\r\n4   4  55     34 26790.03    21012.47      25         65\r\n5   5  29      0 18592.27    13990.48      32          1\r\n6   6  47     19 40229.93    25986.32      27       1408\r\n7   7  61     14  1065.49      699.14      11          1\r\n8   8  58     17  2750.75     2158.39      14         47\r\n9   9  71     49 12292.53    10135.33      16        183\r\n10 10  45     15   282.05      160.08      27          2\r\n\r\nNow we have all the necessary ingredients to split total_spend into n_trans transactions for each client. And this is the moment where the rmultinom function is extremely helpful. Let’s take a look at the first client, who spends $20755.16 in 27 transactions. The high diff_trans value indicates that there will likely be some very high transaction values and some very low.\r\nBefore doing the rmultinom magic, we will create the vector with the bins first. Remember that this vector determines how “large” each bin is or how likely it is to\r\n\r\n\r\nbins <- runif(cl_secret_info$n_trans[1],min=1,max=cl_secret_info$diff_trans[1])\r\n\r\ntransactions1 <- rmultinom(1,cl_secret_info$total_spend[1],bins)\r\n\r\ndf <- data.frame(client_id=1, trans_id=1:cl_secret_info$n_trans[1],value=transactions1)\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[880,1240,1364,1262,158,1291,1182,86,1455,1354,288,831,738,8,1209,164,594,822,280,1093,321,297,211,1276,9,1375,967]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>client_id<\\/th>\\n      <th>trans_id<\\/th>\\n      <th>value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nAutomate this for all customers\r\nIn order to efficiently do this for all customers we will put what we just did in a function.\r\n\r\n\r\ncreate_transactions <- function(i) {\r\n  bins <- runif(cl_secret_info$n_trans[i],min=1,max=cl_secret_info$diff_trans[i])\r\n\r\n  transactions <- rmultinom(1,cl_secret_info$total_spend[i],bins)\r\n\r\n  df <- data.frame(client_id=i, trans_id=1:cl_secret_info$n_trans[i],value=transactions)\r\n  \r\n  return(df)\r\n}\r\n\r\n\r\n\r\nWe call this function repeatedly with lapply.\r\n\r\n\r\ntrans_list <- lapply(seq_along(client$id),create_transactions)\r\n\r\n\r\n\r\nFinally, we bind all the transactions from all clients together in our final dataframe.\r\n\r\n\r\ntrans_df <- do.call(rbind,trans_list)\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,6,7,8,9,10,11,12,13,14,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[573,1367,688,250,228,521,873,1136,533,374,1118,399,482,473,1265,536,1237,701,585,1041,1311,1408,617,1036,1298,381,324,115,51,206,40,184,69,73,150,89,111,34,165,188,128,216,157,119,61,139,204,6,46,188,141,139,131,225,73,140,60,145,182,135,9,91,94,38,148,52,262,91,26,116,164,24,10,36,157,247,225,24,186,167,50,58,14,40,116,186,180,185,106,138,211,61,1513,360,286,160,467,1165,1553,1520,218,916,443,783,1549,153,224,455,327,1545,813,1282,647,1480,679,1144,1330,427,418,473,438,420,464,460,442,431,424,450,432,408,416,445,427,419,444,437,432,434,433,457,421,438,439,427,475,416,459,462,422,1027,1425,355,63,1355,1845,329,530,1729,998,1150,356,53,1370,1290,248,1692,854,1802,19,1562,1244,663,1533,770,797,927,63,60,56,76,65,65,73,59,69,55,58,178,262,110,49,154,233,29,258,251,32,228,231,109,34,366,835,517,464,1270,368,100,1232,210,701,617,211,583,1190,179,1292,7,7,2,2,4,5,7,6,9,1,5,3,6,5,8,6,8,4,12,5,8,8,11,4,8,5,4]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>client_id<\\/th>\\n      <th>trans_id<\\/th>\\n      <th>value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nOther possible applications\r\nStudents and grades (it is easier if the grades are points and you have a total number of points to reach). You might want to check the package {wakefield} to create sequences of grades / tests etc.\r\nProducts and sales numbers in supermarkets.\r\nAnimals and tracked kilometers.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-13-data-composition-with-rmultinom/data-composition-with-rmultinom_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-12-30T17:32:20+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-data-simulation/",
    "title": "Simulate variables and data",
    "description": "The purpose of this post is to enable readers to create data from scratch which they can use for their analyses or visualizations.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPackages\r\nManual values\r\nCategorical variables with sample()\r\nNumerical variables\r\nDistributions\r\nCombining variables in a dataframe\r\nExporting\r\n\r\nPackages\r\nMost of the functions that we are using here are part of base R.\r\nWe will need some functions from the {dplyr} and {ggplot2} packages for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nImportant to make your data creation reproducible (i.e. if you run it again, it gives the same result) is the set.seed() function. As we are creating instances of random variables we assure with this function that every time the same sequence of random variables is generated. You can use any number you like inside this function.\r\n\r\n\r\nset.seed(64)\r\n\r\n\r\n\r\nManual values\r\nLet’s start with the most simple but most time-consuming way. Type everything manually and save it in a vector:\r\n\r\n\r\nclient_gen <- c(\"Millenial\",\"Gen X\",\"Millenial\",\r\n                \"Baby Boomer\",\"Gen X\",\"Millenial\",\"Gen X\")\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1   Millenial\r\n2  2       Gen X\r\n3  3   Millenial\r\n4  4 Baby Boomer\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nCategorical variables with sample()\r\nFor categorical variables, we can save some time using the sample function. You specify first the possible values and then how many of these values you would like to pick. If you want to allow values to be picked more than once, make sure to set replace=TRUE.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"),7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1 Baby Boomer\r\n2  2   Millenial\r\n3  3   Millenial\r\n4  4   Millenial\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nThe sample function is quite flexible and we can tweak the prob parameter, for example to say that we want (approximately) half of the population to be Baby Boomers. The effect will be visible if we produce larger amounts of data.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"), 1000, replace=TRUE, prob=c(0.25,0.25,0.5))\r\n\r\nqplot(client_gen)\r\n\r\n\r\n\r\n\r\nNumerical variables\r\nThe same sample() function works with numbers.\r\n\r\n\r\nclient_age <- sample(1:100,size=7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1          4\r\n2  2         42\r\n3  3         33\r\n4  4         62\r\n5  5         76\r\n6  6         65\r\n7  7         81\r\n\r\nIn both cases above, each number had the same probability of being selected. If we would like some numbers to be more likely to be selected, we can specify this with prob.\r\nThe probability values will be automatically scaled to 1. If I would like to have 50% of the population to have the age of 27, I can specify the weight. (Note: rep(1,5) is equivalent to c(1,1,1,1,1), replicating the number 1 five times.)\r\n\r\n\r\nclient_age <- sample(1:100,size=1000,replace=TRUE,prob=c(rep(1,26),99,rep(1,73)))\r\n\r\nqplot(client_age==27)\r\n\r\n\r\n\r\n\r\nDistributions\r\nIf you would like to work with probability distributions to create numerical variable, this is also very easy with the base functions of type r+(starting letters of the distribution).\r\nLet’s try the uniform distribution:\r\n\r\n\r\nclient_age <- runif(7,min=1,max=100)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1   55.10342\r\n2  2   85.19588\r\n3  3   86.47791\r\n4  4   73.91516\r\n5  5   48.15197\r\n6  6   32.90848\r\n7  7   58.76874\r\n\r\nWe can use the round() function to round each value to their next integer.\r\n\r\n\r\nrunif(10000,1,100) %>% round() %>% qplot(binwidth=10)\r\n\r\n\r\n\r\n\r\nBut uniformly distributed variables are not always what we want. In the example above we simulated 10,000 clients and distributes their ages uniformly. Then there are as many 99 year old clients as there are 50 year old clients.\r\nBut we can easily access a whole list of other distribution functions, like the famous Normal distribution (with mean and standard deviation as parameters).\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf we want to limit the values to not be smaller than 0 or larger than 100, we can use pmin and pmax.\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% pmax(0) %>% pmin(100) %>% qplot()\r\n\r\n\r\n\r\n\r\nFor many applications (like balance distribution or any data that contains outliers) I like to use the Exponential distribution (with parameter rate and expectation 1/rate).\r\n\r\n\r\nrexp(10000,rate=0.01) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf you want to explore further probability distributions check out this link.\r\nCombining variables in a dataframe\r\nTo create our first simulated dataframe, we can start by simulating the variables separately and then putting them together.\r\n\r\n\r\nset.seed(61)\r\n\r\nk <- 7\r\n\r\nid <- 1:k\r\nname <- c(\"Frank\",\"Dorian\",\"Eva\",\"Elena\",\"Andy\",\"Barbara\",\"Yvonne\")\r\nage <- rnorm(k,mean=30,sd=10) %>% pmax(18) %>% round()\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\nmarried <- sample(c(\"Yes\",\"No\"),k,replace=T,prob=c(0.6,0.4))\r\n\r\ndata <- data.frame(client_id=id,name,age,ocupation,balance,married_flg=married)\r\ndata\r\n\r\n\r\n  client_id    name age  ocupation balance married_flg\r\n1         1   Frank  26 sr analyst  245.96          No\r\n2         2  Dorian  26    analyst 2273.39          No\r\n3         3     Eva  18    manager 2270.47          No\r\n4         4   Elena  34    analyst  373.45          No\r\n5         5    Andy  18    analyst  961.21         Yes\r\n6         6 Barbara  28    analyst   69.32         Yes\r\n7         7  Yvonne  37    analyst 3218.13         Yes\r\n\r\nGreat! We just simulated a dataset which we can use now for visualization or modeling purposes.\r\nExporting\r\nWhen we are happy with our created dataset and want to use it somewhere else we can export it using the base R function.\r\n\r\n\r\nwrite.csv(raw,\"data.csv\")\r\n\r\n\r\n\r\nAlternatively, if this is not fast enough, we can also use the fwrite function from the data.table package which is much faster.\r\n\r\n\r\nlibrary(data.table)\r\nfwrite(raw,\"data.csv\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-09-data-simulation/data-simulation_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-29T11:37:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-fuzzy-merging/",
    "title": "Fuzzy matching",
    "description": "Whenever you have text data that was input manually by a human, there is a chance that it contains errors: Typos, abbreviations or different ways of writing can be challenges for your analysis. Fuzzy matching is a way to find inexact matches that mean the same thing like mcdonalds, McDonalds and McDonald's Company.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPackages\r\nThe data\r\nMain process\r\nResults\r\nNext Steps and other resources\r\n\r\nPackages\r\nThe only packages you need are dplyr and stringdist.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(stringdist)\r\n\r\n\r\n\r\nThe data\r\nThis method requires as input two lists. To distinguish them, we will call the one that contains the handtyped input as the “dirty list”. The reference list will be called the “clean list”. In this blogpost I will create the dirty list by hand with a few made-up examples of alternative company names.\r\n\r\n\r\nnames <- c(\"Haliburton\", \"ExxonMobile\",\"ABBOTT LABORATORIES\",\"Marrriott\",\"Self\",\"Activision Blizzard\",\r\n           \"Quest dianotstics\",\"Unemployed\",\"other company\",\"burger king\",\r\n           \"MARRIOT\",\"wall mart\", \"Illumin\", \"3M\",\"NORTHROP TRUMMON\",\"MCCormicks\",\"MARSH MCLEANNON\",\r\n           \"FLO SERVE\", \"Kansas City Southern Fed.\",\"MCDONALD'S\",\"F5 Networks\",\r\n           \"McDonalds\",\"MacKindsey\",\"Oracle\",\"Self-employed\",\"None\",\"Retired\",\r\n           \"f5 networks\",\"Harley Davidson\",\"Harly Davidson\",\"HARLEY DAVIDSEN\",\"DRHorton\",\"D.R. Horten\",\r\n           \"cincinati fin\",\"cincinnatti financials\",\"cincinnati financial\",\"CINCINATTI FINANCE\",\r\n           \"Mohaws Industry\",\"Mowahk Industries\",\"Mohawk Ind\")\r\n\r\nset.seed(64)\r\ndirty_list <- sample(names,50000,replace=T)\r\n\r\n\r\n\r\n\r\ndirty_list\r\nHaliburton\r\nExxonMobile\r\nABBOTT LABORATORIES\r\nMarrriott\r\nSelf\r\nActivision Blizzard\r\nQuest dianotstics\r\nUnemployed\r\nother company\r\nburger king\r\n\r\nAs a clean list we will use the list of S&P500 companies. This can be downloaded or scraped from the internet.\r\n\r\nclean_list\r\n3M Company\r\nAbbott Laboratories\r\nAbbVie Inc.\r\nABIOMED Inc\r\nAccenture plc\r\nActivision Blizzard\r\nAdobe Systems Inc\r\nAdvanced Micro Devices Inc\r\nAdvance Auto Parts\r\nAES Corp\r\n\r\nBefore we start, we will pre-process both lists, remove some common words and transform everything to lower case. If you prefer, you can also use the {stringr} package for this. One comment from my experience: Usually, the construction of the common words to remove is an iterative approach: You would check your final result and see which words are still causing problems. Then you add them to the cleaner function and run the process again until you are satisfied with the results.\r\n\r\n\r\ncleaner <- function(vec) {\r\n  wordremove <- c(\" and \",\" comp \",\" company\",\"companies\",\" corp \",\"corporation\",\" inc \",\"[.]com\")\r\n  output <- vec %>% tolower() %>% \r\n    {gsub(paste(wordremove,collapse='|'),\"\",.)} %>%\r\n    {gsub(\"[[:punct:]]\",\"\",.)} %>%\r\n    {gsub(\"[[:blank:]]\",\"\",.)}\r\n  return(output)\r\n}\r\n\r\ncontrol <- data.frame(original=dirty_list)\r\n\r\nclean_list_cl <- cleaner(clean_list)\r\ndirty_list_cl <- cleaner(dirty_list)\r\n\r\n\r\n\r\nMain process\r\nWe calculate a matrix of string distances. The {stringdist} package has a lot of different methods implemented which can be checked here. After comparing some of the methods I decided to go with the Jaro-Winkler distance as it yields higher similarity for words which start with the same letters.\r\nExample\r\n\r\n\r\nstringdistmatrix(c(\"other\",\"words\",\"otherexample\",\"exapmle\"),\r\n                 c(\"example\",\"other example\",\"word\"),\r\n                 method='jw',p=0.1,useNames=\"strings\")\r\n\r\n\r\n                example other example      word\r\nother        1.00000000    0.12307692 0.5166667\r\nwords        1.00000000    0.48205128 0.0400000\r\notherexample 0.28174603    0.01538462 0.4444444\r\nexapmle      0.03333333    0.55799756 1.0000000\r\n\r\nEach row of the matrix of string distances is one string from the dirty list. We find the minimum in each row, which is equivalent to the best fit from the clean list.\r\n\r\n\r\ndistmatrix <- stringdist::stringdistmatrix(dirty_list_cl,clean_list_cl,method='jw',p=0.1)\r\nbest_fit <- apply(distmatrix,1,which.min) %>% as.integer()\r\nsimilarity <- apply(distmatrix,1,min)\r\n\r\ncontrol$best_fit <- clean_list[best_fit]\r\ncontrol$distance <- round(similarity,3)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nRetired\r\nResMed\r\n0.203\r\nSelf\r\nSealed Air\r\n0.244\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nSelf-employed\r\nTeleflex\r\n0.306\r\n\r\nResults\r\nWhen we order the control dataframe by similarity we can find a suitable cutoff value (in this example 0.12) to separate real matches from false positives. This cutoff value depends on the application.\r\n\r\n\r\ncontrol$result <- ifelse(control$distance<=0.12,control$best_fit,NA)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nresult\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nMarriott Int’l.\r\nRetired\r\nResMed\r\n0.203\r\nNA\r\nSelf\r\nSealed Air\r\n0.244\r\nNA\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nHalliburton Co.\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nIllumina Inc\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMohawk Industries\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nMarsh & McLennan\r\nSelf-employed\r\nTeleflex\r\n0.306\r\nNA\r\nMohawk Ind\r\nMohawk Industries\r\n0.088\r\nMohawk Industries\r\nMowahk Industries\r\nMohawk Industries\r\n0.017\r\nMohawk Industries\r\nHarly Davidson\r\nHarley-Davidson\r\n0.014\r\nHarley-Davidson\r\nf5 networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\n3M\r\n3M Company\r\n0.000\r\n3M Company\r\nOracle\r\nOracle Corp.\r\n0.080\r\nOracle Corp.\r\n\r\nNext Steps and other resources\r\nImprove performance for large datasets. On Github, I have an implementation of this method with the parallel package which improves performance slightly. But there is definitely more room for improvement.\r\nThere is an interesting video about performance improvement by not calculating the full matrix by Seth Verrinder and Kyle Putnam here.\r\nAndrés Cruz created an Add-in which helps to fine-tune the final result, his slide from LatinR 2019 can be found here.\r\nCheck out David Robinson’s fuzzyjoin package here.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-29T12:45:38+01:00",
    "input_file": {}
  }
]
