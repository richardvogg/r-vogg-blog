[
  {
    "path": "posts/2021-05-24-ggraph/",
    "title": "First steps with ggraph",
    "description": "The {ggraph} packages allows to visualize networks and hierarchical data in beautiful ways. In this post I would like to show in which format your data has to be so that ggraph does what you want it to do.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-05-24",
    "categories": [
      "ggraph",
      "widyr",
      "network",
      "treemap"
    ],
    "contents": "\r\n\r\nContents\r\nggraph\r\nPackages\r\nMini example\r\nReal-world examplesMaking a treemap\r\nCreating a network\r\n\r\nFinal remark\r\n\r\nggraph\r\nI recently tried out {ggraph} by Thomas Lin Pedersen and think it is a great tool to add to one’s data visualization toolbox. This package allows to create networks and all kinds of cool plots with hierarchical data.\r\n\r\n\r\n\r\nWhile I am quite familiar with ggplot (still have to google a lot, but I know what I have to do to get from data to a desired output), it took some time to understand the logic behind ggraph. The good news is: It is similar to ggplot, so the plot is created with a layer-like grammar which converts the raw data in one of these beautiful visualizations.\r\nMore information at the package’s website.\r\nPackages\r\nWe will need the following packages.\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nlibrary(dplyr)\r\n\r\n\r\n\r\nMini example\r\nThe data for ggplot graphs is a dataframe or a tibble. For ggraph, we are working with networks and therefore need two components:\r\nVertices / Nodes\r\nEdges\r\nThe edges define the connections between the nodes. And if we do not pass along any information with the nodes, it is enough to define a dataframe with edges.\r\nLet’s take a look at a mini example:\r\n\r\n\r\nedges <- data.frame(\r\n  from = c(\"father\", \"father\", \"father\", \"mother\", \"mother\", \"mother\"),\r\n  to = c(\"me\", \"sister1\", \"sister2\", \"me\", \"sister1\", \"sister2\")\r\n)\r\n\r\n\r\n\r\nWe had to load the {igraph} package in the beginning as it contains the function which converts this to a graph.\r\n\r\n\r\ng <- graph_from_data_frame(edges)\r\n\r\n\r\n\r\nAnd this graph is used to visualize this small example:\r\n\r\n\r\nggraph(g) +\r\n  geom_edge_link() + \r\n  geom_node_text(aes(label = name))\r\n\r\n\r\n\r\n\r\nThis is a very small example. The next step would be to add information to the nodes. So far the nodes have been created from the edges by using the names appearing in the columns from and to (by the way: you can name them as you like and even add further columns - the first two columns will always indicate from which node to which node a line has to be drawn).\r\nWe can also do this manually:\r\n\r\n\r\nvertices <- data.frame(name = c(\"mother\", \"father\", \"me\", \"sister1\", \"sister2\"),\r\n                       letters = c(7, 4, 7, 4, 7))\r\n\r\n\r\n\r\n\r\n\r\ng <- graph_from_data_frame(edges, vertices = vertices)\r\n\r\n\r\n\r\nggplot2 users will be happy to hear that dealing with sizes, colors etc. is the exact same logic, you just have to add scale_edge_... when you refer to edges.\r\n\r\n\r\nggraph(g) +\r\n  geom_edge_link() + \r\n  geom_node_text(aes(label = name, size = letters)) +\r\n  scale_size_continuous(range = c(2,4))\r\n\r\n\r\n\r\n\r\nEnough with the basics, let’s look at real data.\r\nReal-world examples\r\nThe data stems from the Global Health Data Exchange website and you can customize the data download. It is really worth a visit, and contains country-level data around the Burden of Diseases, broken down by sex, age-group and year (1990 - 2019).\r\nFor this example I downloaded a subset containing the percentage of different death causes per country in 2019.\r\n\r\n\r\n\r\n\r\npreserve20e6fdadbc8faa02\r\n\r\nThe dataset contains 133 death causes and which percentage of total deaths they had in 2019 in each one of 213 countries.\r\nMaking a treemap\r\nFirst, we will try to make a treemap to show each country’s profile. For this, we will need some hierarchy. It took some manual work for me to get the hierarchical data from the website (which groups together certain death causes into higher level families).\r\nThe file will be on the second sheet of the excel file in this blogpost’s repository.\r\n\r\npreserve3213ed3c6b389ffb\r\n\r\nWe will join the two datasets and filter out a country of interest.\r\n\r\n\r\ncountry <- \"Chile\"\r\n\r\ngraph_data <- df %>%\r\n  filter(location == country) %>%\r\n  inner_join(causes, by = c(\"cause\" = \"Cause\"))\r\n\r\n\r\n\r\nIn the introduction, we were dealing with networks, here we are dealing with hierarchical data, but the idea is the same: We will create edges between higher level and lower level features. In our case we have three levels and thus will create connections between Level 3 and Level 2 and then between Level 2 and Level 1.\r\nExactly as in our mini example, the edges data.frame will have two columns (from and to).\r\n\r\n\r\nedges <- graph_data %>%\r\n    distinct(from = CauseL3, to = CauseL2) %>%\r\n    rbind(graph_data %>% \r\n            distinct(from = CauseL2,\r\n                     to = cause))\r\n\r\n\r\n\r\nSimilarly, we will do for the vertices. In theory, the vertices would just require the names of all causes from the three levels. We cannot have vertices with a value of 0 (unless we would remove them from the edges), so I am setting those to a very small value.\r\nIn this code I am adding a few extra columns which will help to create a better visual:\r\nI am adding the parent names (from the higher levels) as I will need those for coloring later.\r\nAnd I will add a new column called level, so that not all the labels are displayed, but just the level 1 labels. This is stored in new_label at the end of the code.\r\nI removed all but the top 10 causes of death in a country, to make the plot less cluttered, but this can be easily changed in the last mutate.\r\n\r\n\r\nvertices <- graph_data %>%\r\n    select(name = cause, val = val, parent = CauseL2, parent2 = CauseL3) %>%\r\n    mutate(val = pmax(val, 0.000001), level = 1) %>%\r\n    rbind(graph_data %>% \r\n            distinct(name = CauseL2, parent = CauseL3, parent2 = NA) %>% \r\n            mutate(val = 0, level = 2)) %>%\r\n    rbind(graph_data %>% \r\n            distinct(name = CauseL3, parent = NA, parent2 = NA) %>% \r\n            mutate(val = 0, level = 3)) %>%\r\n    mutate(rank = rank(-val, ties.method = \"first\"),\r\n      new_label = ifelse(level==1 & rank <= 10, name, NA)) %>%\r\n    distinct(name, val, level, new_label, parent, parent2)\r\n\r\n\r\n\r\nLet’s have a look at the data of the vertices:\r\n\r\npreserve69a9743099e9420a\r\n\r\nGood! We are ready to take a look at our graph. Some of the causes have very long names, so I use str_wrap from {stringr} to cut them into several lines. You can also replace that part by new_label and all label will appear as they are.\r\n\r\n\r\ngraph <- graph_from_data_frame(edges, vertices = vertices)\r\n  \r\nggraph(graph, 'treemap', weight = val) + \r\n    geom_node_tile(aes(fill = parent2)) +\r\n    geom_node_text(aes(label = stringr::str_wrap(new_label,15), size = val)) +\r\n    guides(size = FALSE) +\r\n    labs(title = paste(\"Most frequent death causes in\", country)) +\r\n    theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nLet’s put all of the above in a function and call it get_country_profile.\r\n\r\n\r\nget_country_profile <- function(country) {\r\n  \r\n  graph_data <- df %>%\r\n    inner_join(causes, by = c(\"cause\" = \"Cause\")) %>%\r\n    filter(location == country)\r\n  \r\n  edges <- graph_data %>%\r\n    distinct(from = CauseL3, to = CauseL2) %>%\r\n    rbind(graph_data %>% \r\n            distinct(from = CauseL2,\r\n                     to = cause))\r\n  \r\n  vertices <- graph_data %>%\r\n    select(name = cause, val = val, parent = CauseL2, parent2 = CauseL3) %>%\r\n    mutate(val = pmax(val, 0.000001), level = 4) %>%\r\n    rbind(graph_data %>% \r\n            distinct(name = CauseL2, parent = CauseL3, parent2 = NA) %>% \r\n            mutate(val = 0, level = 3)) %>%\r\n    rbind(graph_data %>% \r\n            distinct(name = CauseL3, parent = country, parent2 = NA) %>% \r\n            mutate(val = 0, level = 2)) %>%\r\n    mutate(rank = rank(-val, ties.method = \"first\"),\r\n      new_label = ifelse(level==4 & rank <= 3, name, NA)) %>%\r\n    distinct(name, val, level, new_label, parent, parent2)\r\n  \r\n  graph <- graph_from_data_frame(edges, vertices = vertices)\r\n  \r\n  ggraph(graph, 'treemap', weight = val) + \r\n    geom_node_tile(aes(fill = parent2)) +\r\n    geom_node_text(aes(label = stringr::str_wrap(new_label,15), size = val)) +\r\n    guides(size = FALSE) +\r\n    harrypotter::scale_fill_hp_d(option = \"HarryPotter\") +\r\n    labs(title = country)\r\n\r\n}\r\n\r\n\r\np1 <- get_country_profile(\"Afghanistan\")\r\np2 <- get_country_profile(\"Germany\")\r\np3 <- get_country_profile(\"Chile\")\r\np4 <- get_country_profile(\"Nigeria\")\r\np5 <- get_country_profile(\"Japan\")\r\np6 <- get_country_profile(\"Yemen\")\r\np7 <- get_country_profile(\"New Zealand\")\r\np8 <- get_country_profile(\"United States of America\")\r\n\r\n\r\nlibrary(patchwork)\r\n\r\n\r\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + plot_spacer() +\r\n  plot_layout(guides = \"collect\") &\r\n  theme(legend.title = element_blank(),\r\n        legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\nCreating a network\r\nIf you feel that doing the manual step of creating the dataframes for edges and vertices is too much, you might be happy to hear that you can create great networks without doing that step manually.\r\nFor this we will additionally need the package {widyr}.\r\n\r\n\r\nlibrary(widyr)\r\n\r\n\r\n\r\nThis package allows for pairwise comparisons between countries.\r\n\r\n\r\nall_sim <- df %>%\r\n  pairwise_similarity(location, cause, val, upper = FALSE) %>%\r\n  filter(similarity > 0.95)\r\n\r\n\r\n\r\n\r\npreservedb04feb9121af060\r\n\r\n\r\n\r\nnet <- all_sim %>%\r\n  graph_from_data_frame()\r\n\r\n\r\n\r\n\r\n\r\nnet %>%\r\n  ggraph(layout=\"fr\") +\r\n  geom_edge_link(aes(edge_alpha = similarity)) +\r\n  #geom_node_point() +\r\n  geom_node_text(aes(label=name), size = 2, col = \"red\",\r\n                 check_overlap = TRUE)\r\n\r\n\r\n\r\n\r\nThis was just to show how quickly you can generate a plot using {widyr} and {ggraph}. This probably has too much information in it, but we can already see some interesting trends and connections between states which share different health issues.\r\nFinal remark\r\nI hope this post has sparked some curiosity in you to use the ggraph package. Although the data structure with edges and vertices is somewhat new, it is all about getting used to this format and soon you will create better and better visuals. And remember: You do not have to learn everything on the first day or with the first visual. Repeat and add small pieces of knowledge to your toolbox every time you come across interesting data.\r\nAgain, check out the website of the package for many more examples.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-24-ggraph/ggraph_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-05-24T18:53:13+02:00",
    "input_file": "ggraph.utf8.md"
  },
  {
    "path": "posts/2021-04-30-30daychartchallenge/",
    "title": "What I learned: #30DayChartChallenge",
    "description": "During the whole month of April I participated in the #30DayChartChallenge, organized by Cédric Scherer and Dominic Royé. Looking for suitable data, pre-processing, choosing a type of visualization and finally trying to make it as beautiful as possible - for 30 days, every day. I learned so many new cool things and would like to share a few of them here.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-04-30",
    "categories": [
      "ggplot2",
      "ggstream",
      "corrmorant",
      "patchwork",
      "ggfx",
      "ggtext",
      "waffle",
      "harrypotter"
    ],
    "contents": "\r\n\r\nContents\r\nThe challenge\r\nData sourcesThe Big 5 personality test\r\nGlobal Health Data\r\nInternational soccer matches\r\nYour own data\r\nThe Fragile State Index\r\n\r\nPackagesggfx\r\nggstream\r\nggtext\r\ncorrmorant\r\npatchwork\r\n\r\nOther tipsPictogram\r\nFonts\r\nHarry Potter\r\nCommunity\r\n\r\n\r\nIf you don’t feel like reading today and would just like to see all my results in a gallery, please visit this GitHub repository.\r\nThe challenge\r\nThe idea of the challenge is to create one plot every day. There were five topics and a subtopic for every day. Ideally each chart should be touching the topic and the subtopic, but the rules are not too strict - it is mainly about learning and producing charts.\r\n\r\nI would like to structure my learning into three categories:\r\nWhich new data sources did I get to know?\r\nWhich packages did I find most useful?\r\nWhat else did I learn?\r\nData sources\r\nThe only few frustrating moments were almost all related to datasets. Sometimes I did not have a great inspiration or a dataset at hand, so I spent a lot of time browsing through Kaggle and looking on Google for interesting datasets, while I felt that I was using the time I should actually spend on the visualization.\r\nIt helped that I had already worked with data in the past (TidyTuesdays, Kaggle). And just as Alex Cookson said in his great rstudio::global(2021) talk The Power of Great Datasets, you will be much more motivated if you have data that is interesting to you. So I looked on Kaggle and Google for things that I find interesting. I experienced that looking for data before knowing the topic can be easier, because in this case all that matters is if the dataset seems interesting. When you see the topic for the day, you can then decide whether your new dataset fits or not.\r\nHere are some of my favorite datasets I used during the challenge:\r\nThe Big 5 personality test\r\nThis is a giant dataset from Kaggle (datasource: Open Psychometrics) of over 1 million answer sets of the famous Big 5 personality test. Participants have to rate how much they agree to the following 50 affirmations:\r\n\r\n\r\n\r\nYou also get country data and response times for each question, so there is a lot to discover and many stories to be told. You can download the dataset here. If you want to get started and skip the process of data cleaning a little bit, get the questions and code from this repository.\r\nGlobal Health Data\r\nI discovered this data relatively late in the challenge and will definitely explore it further in the nearer future. It is about death causes in different countries with a lot (really a lot!) of detail. You can also see historical data for many of the last years. And it is completely customizable: You can select which aggregation level you wish and will get a link to download the desired data. (Be aware: If you select All countries, you will get All countries aggregated, so to get the data for all countries, you actually have to select all of them individually. Same with all causes, all genders etc.).\r\nYou can get your own data for non-commercial use here.\r\n\r\nInternational soccer matches\r\nA dataset containing all official international soccer matches and results between 1872 and 2021. If you like soccer, this one is definitely worth taking a look at. It will bring back some memories.\r\nYou can find it here on Kaggle.\r\nYour own data\r\nFor me, working with my own data is the best. I used carbon dioxide measurements I did when I was at high school to show the strong influence of vegetation and photosynthesis on the CO2 concentration over the course of a day. This data feels so heartwarming because I remember how I got up at 3 am just to make a measurement, so each data point actually means something to me.\r\n\r\nSimilarly, I downloaded the recycling data from an app I was using in Chile which tracked how many plastic bottles and metal cans I recycled over the course of little more than a year.\r\nAnd also a Twitter analysis feels more interesting when it is your own data you are looking at.\r\nThe Fragile State Index\r\nThe Fragile State Index is one of my currently favorite datasets. I already explored it in this blogpost but there is so much more to discover that I used it in three visualizations.\r\n\r\nPackages\r\nggfx\r\nOn March 31, one day before the challenge started, Thomas Lin Pedersen announced a new package: ggfx. It allows to apply filters (like glow, blur or shadows) to any part of your ggplot or to the whole plot.\r\nI counted that I used this package in every second submission! Two nice effects:\r\nwith_inner_glow() makes round shapes look 3-dimensional. Apply this to a round flag, and it looks almost like a button.\r\n\r\n\r\n\r\nwith_outer_glow() makes lines look straight. In my opinion, it make a huge difference if you use the standard ggplot2 lines or apply a filter to it.\r\n\r\n\r\nlibrary(ggfx)\r\n\r\ndf1 <- tibble(x = 1:10, y = rnorm(10))\r\n\r\nggplot(df1, aes(x, y, group = 1)) + \r\n  geom_line()\r\n\r\nggplot(df1, aes(x, y, group = 1))  + \r\n  with_outer_glow(geom_line())\r\n\r\n\r\n\r\n\r\nggstream\r\nDavid Sjoberg is the author of many amazing ggplot extension packages and they were widely used by many participants during the chart challenge. I used ggstream and it makes beautiful data visualization extremely easy!\r\n\r\nAlso, check his latest package ggsankey for making sankey charts with ggplot.\r\nggtext\r\nClaus Wilke’s ggtext package also helped enormously with making plots better and easier to understand. It contains many cool text-related features, but the one I used most was to color the title or the subtitle of the plot. This helps to replace a legend or tell the story more efficiently.\r\n\r\n\r\nlibrary(ggtext)\r\n\r\nggplot(data = example, aes(x = x, y = y)) +\r\n  geom_point() +\r\n  labs(title = \"The last word will be colored <span style='color:red;'>red<\/span>\") +\r\n  theme(plot.title = element_markdown())\r\n\r\n\r\n\r\ncorrmorant\r\nWhen visualizing correlations between several variables it is good to have an overview. Roman Link created corrmorant which help with creating customized corrplots/pairplots in ggplot logic and style.\r\n\r\n\r\n\r\npatchwork\r\nThis package by Thomas Lin Pedersen is awesome when it comes to assembling plots. Whether you want to put them side by side or on top of each other, you can achieve they layout you want with very easy syntax.\r\nIf the layout is getting somewhat more complex, it also has this beautiful option of describing it with letters (of course, you have to create the plots first, but assembling them is as easy as this with patchwork):\r\n\r\n\r\nlayout <- \"\r\nACCCD\r\nBCCCD\r\nBCCCD\r\nBEEEF\r\n\"\r\n\r\nguide_area() + map + tiles + months + years + plot_spacer() + \r\n         plot_layout(design = layout, guides = \"collect\")\r\n\r\n\r\n\r\nAnd this is the result of above’s code (after adding title and other annotations):\r\n\r\nOther tips\r\nPictogram\r\nPictograms can be a great way to show proportions, given that there are so many emojis to almost any topic. Using the waffle package, this is quite easy. However, I faced some difficulties to get the fonts set up correctly and also saw that others tweeted about this. In case that you would like to play around with pictograms and want to get set up, step by step, follow this tutorial for using icons in waffle.\r\n\r\n\r\n\r\nFonts\r\nFor a very long time I have wondered, how people make great visualizations with non-standard ggplot fonts. While doing Tidy Tuesdays in the past I have already learned how to use a font from Google Fonts - for example with this blog, but during this challenge I used this knowledge a lot more.\r\nOnce you have the packages installed, it is basically just going to Google Fonts, selecting a font you like (for this example “Indie Flower” - the font I used in the Harry Potter magical spells example above) and then adding these two lines to your code.\r\n\r\n\r\nsysfonts::font_add_google(name = \"Indie Flower\", \"Indie\")\r\nshowtext::showtext_auto()\r\n\r\n\r\n\r\nAnd later you call specify in your ggplot theme that you want this font to be used, either one by one for plot.title, legend.text etc. or for all texts:\r\n\r\n\r\nggplot(data = example, aes(x = x, y = y)) +\r\n  geom_point() +\r\n  labs(title = \"Example title\") +\r\n  theme(text = element_text(family = \"Indie\"))\r\n\r\n\r\n\r\nNice and easy and opens so many new options to make visualizations look better.\r\nHarry Potter\r\nThere are two packages with the same name on GitHub, I used them both and found both very helpful.\r\nBeautiful Harry Potter Color Palettes\r\nAll texts from the Harry Potter books.\r\nCommunity\r\nSo far, I have only talked about learning by doing. But during the challenge I also learned many tricks by looking at other participants’ submissions and codes. The creativity and visualization capabilities of the others for each one of these topics was outstanding and looking for #30DayChartChallenge on Twitter will be worth a visit.\r\nA big thank you to Cédric Scherer and Dominic Royé for organizing this challenge and to congratulations to all participants!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-30-30daychartchallenge/img/ggstream.png",
    "last_modified": "2021-05-01T11:39:36+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-27-fuzzy-matching-packages/",
    "title": "Fuzzy matching packages",
    "description": "Which packages help us with fuzzy matching? We are going to explore stringdist, tidystringdist, fuzzyjoin, inexact, refinr, fuzzywuzzyR, and lingmatch.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-03-24",
    "categories": [
      "text data",
      "fuzzy matching",
      "stringdist"
    ],
    "contents": "\r\n\r\nContents\r\nIntro\r\nThe data\r\nstringdist\r\ntidystringdist\r\nfuzzyjoin\r\ninexact\r\nrefinr\r\nfuzzywuzzyR\r\nlingmatch\r\nConclusion\r\n\r\nIntro\r\nBefore starting to write a package which helps with fuzzy matching I wanted to do a small research and explore the packages which are already out there.\r\nI am loading {dplyr} as it is always helpful when working with data.\r\n\r\n\r\nlibrary(dplyr)\r\n\r\n\r\n\r\nThe data\r\nIt is very hard to come up with a realistic example which is perfect for all types of fuzzy matching algorithms. I will do a Harry Potter example and modify it along the way.\r\nAssume that we want to find all occurrences of Harry Potter and Voldemort. As a human it is easy to see them directly, but the computer would have some problems with these examples, due to different types of spelling and typos.\r\n\r\n\r\ninput <- c(\"harry j potter\", \"harrypotter\", \"Voldemort\", \r\n           \"Harry POTTER\", \"Harrry Potter\", \"Ron Weasley\")\r\n\r\ncompare <- c(\"Harry Potter\", \"Voldemort\")\r\n\r\n\r\n\r\nstringdist\r\nThe {stringdist} package by Mark van der Loo is super useful for comparing strings. And as comparison of strings is the core of the fuzzy string matching process {stringdist} is maybe the most important package to look at.\r\nThe package contains a function with the same name stringdist which calculates the distance between input and compare string. The most known method to calculate string distances is probably the Levenshtein distance which checks how many letters would have to be inserted, deleted or replaced to get from the input the the compare string.\r\nHere, we see the distance of all input strings to the first compare string “Harry Potter”. We get six values, because our input contained six strings.\r\n\r\n\r\nlibrary(stringdist)\r\n\r\nstringdist(input, compare[1], method = 'lv')\r\n\r\n\r\n[1]  4  3 10  5  1 11\r\n\r\nYou can also use the string comparison method of your choice, e.g. Jaro-Winkler. To see the whole list of available methods use the help function.\r\n\r\n\r\n#help(`stringdist-metrics`)\r\n\r\nstringdist(input, compare[1], \r\n           method = \"jw\", p = 0.1)\r\n\r\n\r\n[1] 0.15079365 0.14393939 0.58333333 0.16666667 0.01538462 0.65909091\r\n\r\nWe observe that the values are rather low if strings are similar, and higher if they are different.\r\nSo far, we have only compared the input with one of the strings (namely “Harry Potter” the first one of the compare strings).\r\nIf we would like to test all possible combinations, we can use stringdistmatrix.\r\n\r\n\r\nstringdistmatrix(input, compare, \r\n                 method = \"cosine\", useNames = TRUE)\r\n\r\n\r\n               Harry Potter Voldemort\r\nharry j potter   0.13277262 0.5076340\r\nharrypotter      0.12791840 0.4466284\r\nVoldemort        0.46064011 0.0000000\r\nHarry POTTER     0.38508131 0.8492443\r\nHarrry Potter    0.01023759 0.4777670\r\nRon Weasley      0.62789580 0.5818790\r\n\r\ntidystringdist\r\nIf you are a fan of tidy data, you might want to have a look at Colin Fay’s {tidystringdist} which was built on top of {stringdist}. The tidy_comb function creates a dataframe with all comparisons and then tidy_stringdist calculates the distance measures.\r\n\r\n\r\nlibrary(tidystringdist)\r\n\r\ntidy_comb(input, compare)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  V1           V2            \r\n* <chr>        <chr>         \r\n1 Harry Potter harry j potter\r\n2 Voldemort    harrypotter   \r\n3 Harry Potter Voldemort     \r\n4 Voldemort    Harry POTTER  \r\n5 Harry Potter Harrry Potter \r\n6 Voldemort    Ron Weasley   \r\n\r\n\r\n\r\ntidy_comb(input, compare) %>%\r\n  tidy_stringdist(method = c(\"jw\", \"lv\", \"cosine\"))\r\n\r\n\r\n# A tibble: 6 x 5\r\n  V1           V2                 jw    lv cosine\r\n* <chr>        <chr>           <dbl> <dbl>  <dbl>\r\n1 Harry Potter harry j potter 0.151      4 0.133 \r\n2 Voldemort    harrypotter    0.576      9 0.447 \r\n3 Harry Potter Voldemort      0.583     10 0.461 \r\n4 Voldemort    Harry POTTER   0.602     12 0.849 \r\n5 Harry Potter Harrry Potter  0.0256     1 0.0102\r\n6 Voldemort    Ron Weasley    0.532      9 0.582 \r\n\r\nfuzzyjoin\r\nDavid Robinson’s {fuzzyjoin} package is useful for so many applications. As the name already says, we are looking at joins / merges of tables here. It is the fuzzy version of left join / inner join / full outer join etc.\r\nWe will look at a small variation of our example to show how fuzzy join works. Assume that we have some extra information coming along with the compare vector, e.g. how often they use bad spells. We are working with the input, and want to add this information to our analysis. As a direct join is not possible (due to the different spellings), we can use the stringdist_join function.\r\n\r\n\r\nlibrary(fuzzyjoin)\r\n\r\ndf1 <- data.frame(name = c(\"harry j potter\", \"harrypotter\", \"Voldemort\", \r\n           \"Harry POTTER\", \"Harrry Potter\", \"Ron Weasley\"))\r\n\r\ndf2 <- data.frame(name = c(\"Harry Potter\", \"Voldemort\"), \r\n                  bad_spells_index = c(0.02,0.87))\r\n\r\nstringdist_join(df1, df2, \r\n                mode = \"inner\",\r\n                by = \"name\",\r\n                max_dist = 6)\r\n\r\n\r\n          name.x       name.y bad_spells_index\r\n1 harry j potter Harry Potter             0.02\r\n2    harrypotter Harry Potter             0.02\r\n3      Voldemort    Voldemort             0.87\r\n4   Harry POTTER Harry Potter             0.02\r\n5  Harrry Potter Harry Potter             0.02\r\n\r\nNote that with larger datasets you will have to be careful with max_dist. It is the threshold of the maximal allowable string distance. If you select it too high, some names from the input will have several matches.\r\n\r\n\r\nstringdist_join(df1, df2,\r\n                mode = \"inner\",\r\n                by = \"name\",\r\n                max_dist = 10)\r\n\r\n\r\n          name.x       name.y bad_spells_index\r\n1 harry j potter Harry Potter             0.02\r\n2    harrypotter Harry Potter             0.02\r\n3    harrypotter    Voldemort             0.87\r\n4      Voldemort Harry Potter             0.02\r\n5      Voldemort    Voldemort             0.87\r\n6   Harry POTTER Harry Potter             0.02\r\n7  Harrry Potter Harry Potter             0.02\r\n8    Ron Weasley    Voldemort             0.87\r\n\r\nAnd you can actually use any method from the {stringdist} package. This small paragraph is not enough to showcase the flexibility of {fuzzyjoin}. You can also do joins based on regular expressions, spatial locations and many more.\r\ninexact\r\nThe RStudio addin {inexact} by Andrés Cruz helps to manually improve your fuzzy string matching process.\r\nIt is still under development but you can install it from Github.\r\n\r\n\r\nremotes::install_github(\"arcruz0/inexact\")\r\n\r\n\r\n\r\nIt helps if there are some matches which are hard to be found automatically by the program, but can be performed manually by us.\r\n\r\n\r\ninput_strings <- data.frame(name = c(\"Harry J. Potter\",\"Voldemort\",\"Lord Voldemort\",\"Tom Riddle\"), \r\n                    appearances = c(50,30,20,2), stringsAsFactors = F)\r\n\r\n\r\ncompare_strings <- data.frame(name = c(\"Harry Potter\",\"Voldemort\",\"Ted Tonks\"), \r\n                      bad_spells = c(0.05,0.87,0.03), stringsAsFactors = F)\r\n\r\n\r\n\r\nYou can now either open in from the Addins panel on top of your Rstudio window, or run:\r\n\r\n\r\ninexact::inexact_addin()\r\n\r\n\r\n\r\nThis opens a window which helps to fix matches manually.\r\n\r\n\r\nknitr::include_graphics(\"img/inexact.gif\")\r\n\r\n\r\n\r\n\r\nYou can copy the code in the third stage of the addin to perform the join.\r\n\r\n\r\n# You added custom matches:\r\ninexact::inexact_join(\r\n  x  = input_strings,\r\n  y  = compare_strings,\r\n  by = 'name',\r\n  method = 'osa',\r\n  mode = 'left',\r\n  custom_match = c(\r\n   'Tom Riddle' = 'Voldemort'\r\n  )\r\n)\r\n\r\n\r\n              name appearances bad_spells\r\n1: Harry J. Potter          50       0.05\r\n2:       Voldemort          30       0.87\r\n3:  Lord Voldemort          20       0.87\r\n4:      Tom Riddle           2       0.87\r\n\r\nrefinr\r\nThe {refinr} package works with string fingerprints (very good explanation of the topic here). There are different types of fingerprints, for example the 2-gram fingerprint of Harry is arharrry and the 1-gram fingerprint is ahrry.\r\nRemember that we had one example “harry j potter”, to get the same fingerprint, we will ignore the j. I like that the compare strings are called dict, as they serve as some sort of dictionary - the ultimate truth of spelling.\r\n\r\n\r\nlibrary(refinr)\r\n\r\nkey_collision_merge(input, dict=compare, ignore_strings = \"j\")\r\n\r\n\r\n[1] \"Harry Potter\"  \"harrypotter\"   \"Voldemort\"     \"Harry Potter\" \r\n[5] \"Harrry Potter\" \"Ron Weasley\"  \r\n\r\nAfter this cleaning, we can use the n_gram_merge to replace the occurrences of Harry Potter. This method is also based on the {stringdist} package and allows for usage of other string matching methods.\r\n\r\n\r\ninput %>%\r\n  key_collision_merge(dict=compare,ignore_strings = \"j\") %>%\r\n  n_gram_merge()\r\n\r\n\r\n[1] \"Harry Potter\" \"Harry Potter\" \"Voldemort\"    \"Harry Potter\"\r\n[5] \"Harry Potter\" \"Ron Weasley\" \r\n\r\nfuzzywuzzyR\r\nWhen you google fuzzy string matching, you will see tons of Python articles. Most of them use the fuzzywuzzy library. The {fuzzywuzzyR} package ports this functionality to R. As far as I have seen, it only works with the Levenshtein distance.\r\nYou need to have the {reticulate} package installed which helps with the Python connection. I had some problems getting the example to work. Issues on github advice to install fuzzywuzzy and Levenshtein in Python (with pip install fuzzywuzzy) and then call reticulate::py_discover_config(required_module = 'fuzzywuzzy').\r\n\r\n\r\nlibrary(fuzzywuzzyR)\r\n\r\n# Processor\r\ninit_proc <- FuzzUtils$new()\r\nPROC <- init_proc$Full_process\r\n\r\n# Scorer\r\ninit_scor <- FuzzMatcher$new()\r\nSCOR <- init_scor$WRATIO #scorer function\r\n\r\ninit <- FuzzExtract$new()\r\ninit$Extract(string = \"harry Potter\", sequence_strings = compare, \r\n             processor = PROC, scorer = SCOR)\r\n\r\n\r\n[[1]]\r\n[[1]][[1]]\r\n[1] \"Harry Potter\"\r\n\r\n[[1]][[2]]\r\n[1] 100\r\n\r\n\r\n[[2]]\r\n[[2]][[1]]\r\n[1] \"Voldemort\"\r\n\r\n[[2]][[2]]\r\n[1] 29\r\n\r\nWe can also try to find the best fit (or best fits - depending on the limit input) for every word.\r\n\r\n\r\ninit$ExtractBests(string = \"harry j potter\", sequence_strings = compare,\r\n                  processor = PROC,\r\n                  scorer = SCOR, limit = 1L)\r\n\r\n\r\n[[1]]\r\n[[1]][[1]]\r\n[1] \"Harry Potter\"\r\n\r\n[[1]][[2]]\r\n[1] 95\r\n\r\n{fuzzywuzzyR} also has a deduplication functionality. Let’s try this with our initial input.\r\n\r\n\r\ninit$Dedupe(contains_dupes = input, threshold = 70L, scorer = SCOR)\r\n\r\n\r\ndict_keys(['harry j potter', 'Voldemort', 'Ron Weasley'])\r\n\r\nWe see no duplicates. However, the algorithm chose the first appearance “harry j potter” and adapted all the others. It would be great to have “Harry Potter” instead.\r\nCheck the vignette for more functionality and examples.\r\nlingmatch\r\nThe {lingmatch} package does not directly work with characters but with words.\r\n\r\n\r\nlibrary(lingmatch) \r\n\r\nmatches <- lingmatch(\"Harry Potter and Voldemort are enemies\", \"Harry Potter does not like Voldemort\")\r\n\r\nmatches$dtm\r\n\r\n\r\n2 x 9 sparse Matrix of class \"dgCMatrix\"\r\n     and are does enemies harry like not potter voldemort\r\n[1,]   .   .    1       .     1    1   1      1         1\r\n[2,]   1   1    .       1     1    .   .      1         1\r\n\r\nmatches$sim\r\n\r\n\r\ncosine \r\n   0.5 \r\nattr(,\"time\")\r\nsimets \r\n     0 \r\n\r\nThere could be a potential value in fuzzy string matching if we decompose our input strings into ngrams (for example with the {quanteda} package) and use {lingmatch} with ngrams instead of words.\r\n\r\n\r\nlibrary(quanteda) # for the n gram decomposition\r\n\r\ninput1 <- \"harry potter\"\r\n\r\ninput2 <- \"harry james potter\"\r\n\r\ninput1_ngram <- input1 %>%\r\n  tokens(\"character\") %>%\r\n  unlist() %>%\r\n  char_ngrams(concatenator = \"\") %>%\r\n  paste(collapse = \" \")\r\n\r\ninput2_ngram <- input2 %>%\r\n  tokens(\"character\") %>%\r\n  unlist() %>%\r\n  char_ngrams(concatenator = \"\") %>%\r\n  paste(collapse = \" \")\r\n\r\nlingmatch(input1_ngram,input2_ngram)\r\n\r\n\r\n$dtm\r\n2 x 16 sparse Matrix of class \"dgCMatrix\"\r\n                                    \r\n[1,] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 .\r\n[2,] . 1 1 . 1 . . 1 1 1 1 . 1 1 . 1\r\n\r\n$processed\r\n1 x 16 sparse Matrix of class \"dgCMatrix\"\r\n                                    \r\n[1,] . 1 1 . 1 . . 1 1 1 1 . 1 1 . 1\r\n\r\n$comp.type\r\n[1] \"text\"\r\n\r\n$comp\r\nam ar er es ha ja me ot po rr ry sp te tt yj yp \r\n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0 \r\n\r\n$group\r\nNULL\r\n\r\n$sim\r\n   cosine \r\n0.7348469 \r\nattr(,\"time\")\r\nsimets \r\n     0 \r\n\r\nWe see that the cosine similarity of the words is 0.735.\r\nConclusion\r\nThere are already some great packages available to make the task of fuzzy string matching easier. It depends on the application which one or which combination of functionalities suits your needs best. As always, I am very interested in feedback and discussion: Did you already work with any of these packages? What has your experience been? What do you feel is missing to make fuzzy matching easier and better?\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-27-fuzzy-matching-packages/img/harry-potter.png",
    "last_modified": "2021-03-28T22:17:50+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-18-fuzzy-string-matching/",
    "title": "The big picture: Fuzzy string matching",
    "description": "An overview over fuzzy string matching problems and solutions. After reading this article you will know in which situations fuzzy string matching can be helpful, and know variations like fuzzy deduplication and record linkage.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-03-18",
    "categories": [
      "text data",
      "fuzzy matching",
      "stringdist"
    ],
    "contents": "\r\n\r\nContents\r\nStructuring thoughts\r\nWhen to use fuzzy string matching\r\nImperfect strings - how to find the “perfect” version\r\nCleaning\r\nThe string comparison - the core of fuzzy string matching\r\nCheck results - false positives and false negatives\r\nThe ignore list\r\nOne column matching vs several column matching\r\nQuestions to ask before starting\r\n\r\nStructuring thoughts\r\nI started drawing a map with draw.io to show the structure of the ideas I want to write about. I want to keep this post on a conceptual level with examples. I plan to write about useful packages and how to use R to work with fuzzy string matching in future posts.\r\n\r\n\r\n\r\nThis map will serve as an outline for the post.\r\nWhen to use fuzzy string matching\r\nData Analysts and Data Scientists know how to work with different types of variables. Knowing if a variable is numeric, a (categorical) factor or a boolean is important for preprocessing, visualization and modeling. Fuzzy string matching is helpful when working with text input, specifically imperfect text input.\r\nIn general, I would distiguish two different types of imperfection in the text variable.\r\nThe data was typed in manually by a human (or many humans)\r\nresponses from forms or surveys\r\ndata that was typed by hand into an Excel/text file\r\n\r\nThe data comes from different sources or has several possible alternative names\r\ncountry names: United States vs United States of America\r\naddresses: Main Street 123 vs 123 Main Street\r\nproduct descriptions: iPhone vs Apple iPhone\r\n\r\nThe problem with imperfection in text data is that analysis is not easily possible:\r\nIf we are dealing with handtyped company names, we do not even receive a reliable answer when asking: “How many records do we have from company xyz?” (check this blogpost)\r\nWe will see unfair rankings, because some records are not counted, like in this TidyTuesday example.\r\n\r\n\r\n\r\nWe will have problems joining to other data sources, as described in this blogpost.\r\nImperfect strings - how to find the “perfect” version\r\nWhen we have a list of imperfect strings, there are two possibilities how to match them to their “perfect” counterparts.\r\nEither we have a list of correct names. This could be a dictionary, a list of official company names (like the S&P 500), or a file which contains further information for every country.\r\nOr we want the data to find the correct names by itself. In this case we would need some criterion which names are more likely to be “correct” (e.g. word frequency). This is often called fuzzy deduplication.\r\nCleaning\r\nMany differences can already be removed by cleaning the data. This could be:\r\nconverting all strings to lower case letters.\r\nremoving leading and trailing white spaces - or maybe even removing all whitespaces.\r\nremoving punctuation and control characters (%,&,$,-)\r\nremoving some words (more about this in the section “The ignore list”)\r\nturning extended characters to their basic form (e.g. ö to o, ñ to n).\r\nusing string fingerprints (described for example here).\r\nThe string comparison - the core of fuzzy string matching\r\nAfter all the initial steps, it comes down to the core of the matching algorithm. We take two words (either one from the imperfect list and one from the perfect list, or we compare words from the imperfect list in a pairwise manner) and compare them.\r\nWe want to put a numerical value to the similarity of two strings (or alternatively to the distance, which is the opposite).\r\nIntuitively, we “know” or “feel” that the strings baseball and basketball are rather similar and that breakfast and hello are not very similar.\r\nWhat is the reasoning behind this feeling?\r\nHow many letters do the words share?\r\nAre the letters they share in a similar position in the string?\r\nAre the strings approximately of the same length?\r\nAre parts of the words the same but in different order? (homework vs work-from-home)\r\nThere are many algorithms which are used to measure similarity between strings. As always, too many choices can be intimidating, so I will name only two methods which I find useful for many applications.\r\nJaro-Winkler: takes into account the length of the strings, the number of shared letters, the number of transpositions of shared letters (e.g. example and exampel would be 1 transposition), and the number of matching letters among the first four letters. Jaro-Winkler is strong in cases when the imperfections happen mainly at the end of the string, for example through additional words (Company ABC International vs Company ABC Int).\r\nLevenshtein distance: also known as Edit distance. This is the most famous and most used string distance. It counts, how many insertions, deletions and replacements of letters are needed to get from one string to the other.\r\nIf you are interested in more details or more string distance measures, I recommend this blogpost.\r\nWhich string distance measure works best depends highly on the type of imperfection in the data. It would even be possible to calculate two distance measures and use them in combination to find a suitable threshold.\r\nCheck results - false positives and false negatives\r\nAfter making a lot of string-by-string comparisons, the fuzzy string matching process is almost over. For each imperfect string we will have a closest match or several closest matches and can review the process.\r\nTo review the results I usually create a small data frame, containing the original string, the best fit, and the distance between both. In this data frame, I order the results by the distance.\r\n\r\n\r\n\r\nBy scrolling through the complete ordered list, we will see many cases with small distance, i.e. almost perfect matches. In some applications, we will see that there are matches which should not be matches (false positives) - i.e. McDonnell and McDonald’s. Those strings are very similar but should not be matches.\r\nOne chance to avoid false positives is to set a threshold, i.e. we would only consider a best-fit to be a match if it is similar enough. And this similarity threshold could be determined by eye (“at which level of similarity does the first false positive occur?”).\r\nDoing this, we will create a second problem: false negatives. By setting a very low threshold, we would potentially miss a lot of actual matches.\r\nThis is a though challenge. We have to find a good compromise, between not being too strict and losing a lot of opportunities for matches and being too relaxed and replacing many strings by matches which are actually wrong. The decision depends a lot on the cost of false positives and false negatives.\r\nThe ignore list\r\nIn many applications you will not find a satisfying threshold from the beginning. You will see that there are some early false positives, but also many actual matches with lower similarity.\r\nOne possibility to improve the result is to ignore some words which do not actually help to identify matches, and rather disturb the process of string similarity calculation.\r\nThere is no golden rule about which words to ignore, but some guidelines:\r\nIgnore words that are applicable for almost all strings in the domain. If you are dealing with company names ignore “Company”, “Corporation”, “International”, “Ltd”.\r\nIgnore abbreviations of these words as well: “Comp”, “Corp”, “Int”.\r\nIf you work with a list of “perfect” strings, you have to ignore the words in both lists. I usually include the words to ignore in the cleaning process.\r\nCreating the ignore list is often an iterative process. You run the matching process, and review the result. Then you notice how you can prevent many false negatives from happening because they contain certain types of words. You add those words to the ignore list and start again with the matching process.\r\nOne column matching vs several column matching\r\nSo far, we have been looking at matching one variable. There are many applications where it is necessary to compare several columns.\r\nRecord Linkage is a subtype of fuzzy string matching where you want to check the identity of a person by information coming from different systems. Banks have to make sure that their customers are who they claim to be, and invest a lot of money and efforts into KYC (Know your customer) systems.\r\nHowever, just checking the name will give tons of false positives (there are so many people who share the same name - i.e. that would be perfect matches). When you compare name, address and phone number, you can expect better results. Additional to the points we mentioned about cleaning and fuzzy matching, you would also have to put weights to different situations - what if name and address coincide and the phone number is missing in one source? What if the name is slightly similar and the address is the same?\r\nAs before, there is some manual checking necessary to create a good fuzzy string matching system.\r\nQuestions to ask before starting\r\nDo I really need fuzzy matching? If your dataset only contains 10 values, it is much faster to manually find the matches. As with all automation tasks, you have to compare the time investment for developing the automation with the time savings.\r\nWhat is the source of imperfection in the strings? This influences:\r\nHow much cleaning is necessary before starting?\r\nWhich stringdist algorithm is the best to find the correct matches?\r\n\r\nWhat is the “cost” of false positives and false negatives? This influences:\r\nHow much time do I have to invest in creating the ignore list.\r\n\r\nDo I want to match one column or several columns?\r\nDo I have a list of “correct” strings or do I want the data to find its “correct” candidates itself?\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-18-fuzzy-string-matching/img/fuzzy.png",
    "last_modified": "2021-03-20T13:03:17+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-03-the-fragile-state-index/",
    "title": "The Fragile State Index",
    "description": "One of the most interesting datasets I looked at recently is The Fragile State Index. It is a project dedicated to measure risk and vulnerability in countries worldwide, assigning values to 12 indicators like economic inequality, demographic pressures and brain drain (the tendendy of intellectuals and politicians to leave the country). The project is powered by The Fund For Peace and the data is publicly available.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-03-03",
    "categories": [
      "data",
      "spatial",
      "widyr",
      "ggraph",
      "network"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nThe Fragile State Index\r\nThe world map\r\nThe long-term trend\r\nFirst comparison between countries\r\nNetwork graph\r\nFinal notes\r\n\r\nIntroduction\r\nLast November, I participated in the 30DayMapChallenge. I learned a lot by making different maps with R and discovering some of the cool packages that are around for spatial data.\r\nBut I also learned a ton by looking at other’s results. One example is @sporella’s contribution for Day 25. She worked with a very interesting dataset called The Fragile State Index and provided the link to their website. So I added it to my list of “datasets I want to explore sometime in the future”.\r\n\r\n\r\n\r\nThe time has come and I had time to expore the Fragile State Index. For some visualizations I wanted to allow user input, so I wrote a very similar article on shinyapps. In this blogpost I am just showing the static visuals.\r\nThe Fragile State Index\r\nThe Fragile State Index is a project dedicated to measure risk and vulnerability in countries worldwide. It is powered by The Fund For Peace and the data is publicly available.\r\nIt assigns a value between 1 and 10 to each country in each one of 12 categories:\r\nCohesion indicators\r\nSecurity Apparatus\r\nFactionalized Elites\r\nGroup Grievance\r\n\r\nEconomic indicators\r\nEconomy\r\nEconomic Inequality\r\nHuman Flight and Brain Drain\r\n\r\nPolitical indicators\r\nState Legitimacy\r\nPublic Services\r\nHuman Rights and Rule of Law\r\n\r\nSocial indicators\r\nDemographic Pressures\r\nRefugees and IDPs\r\nExternal Intervention\r\n\r\nBehind each indicator many questions are asked for each country. A detailed description of the indicators can be found here. It is recommended to read through the texts before analyzing the data.\r\nThe world map\r\nPlotting a world map does not show a lot of detail but it helps us to understand general trends. The vulnerability of countries worldwide is provoked by different factors in different countries. We will go into more detail in the next sections, but this helps to get a general overview.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe long-term trend\r\nIt is interesting to look at long-term trends and see in which indicators countries improved / worsened between 2006 and 2020. Note, that a lower value means higher stability.\r\n\r\n\r\n\r\nLooking at the worldwide average, we can see that most categories have improved slightly. Large improvements have been made in Economic Inequality and Demographic Pressures, while Factionalized Elites have made countries on average more unstable.\r\n\r\n\r\n\r\nFirst comparison between countries\r\nAfter looking at the world map we got a first intuition of the indicators. However, if we want to compare selected countries, it is easier to compare the exact values for 2020 in these countries with the average value for all countries.\r\nI try to read this as follows: Country X is doing good in Indicators I and J, but is not yet doing good in Indicator K.\r\n\r\n\r\n\r\nIn the interactive version, users can also see the last years to compare how a certain country has changed compared to itself.\r\n\r\n\r\n\r\nNetwork graph\r\nA network graph can help to see connections between countries who have similar indicator values. We use the 2020 values for comparing two countries and draw lines between them if they are similar. The relative position of a country in the network does not have a meaning, it is self-arranging depending on the number of connections.\r\nIn the interactive version you can brush a rectangle with your mouse over countries of interest to see their historical details.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTo get a better view, we can look at some continents separately.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSimilar to the first country comparison plot, we can see how the indicators evolve over time for different countries. Long-term trends and jumps in the indicators are worth a google search. In the interactive version you can also check other countries of your interest.\r\n\r\n\r\n\r\nFinal notes\r\nIn case of any questions or interest for discussion, feel free to reach out to me via mail (richard.vogg@web.de) or Twitter.\r\nWe live in a complex world. The work done by the Fragile State Index helps to understand in which areas countries are progressing. It is important to understand that despite of all the bad events and news that occur every day, progress and improvements are being made in many areas. (Side note: I recommend Hans Rosling’s book Factfulness which is related to this topic.)\r\nData analysis is one powerful tool to find and communicate patterns and information. It is much more powerful to combine this information with knowledge about political and economic events in certain countries. I highly recommend reading some of the articles on the website which tell the story behind the data for selected countries.\r\nCreating the network was made super easy by David Robinson’s {widyr} package to calculate pairwise distances between the countries and Thomas Lin Pedersen’s {ggraph}.\r\nThe Fragile State Index data is freely available at their website. After downloading the documents, I used the following cleaning script in R to combine everything in one dataframe.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(readxl)\r\n\r\ndf <- lapply(2006:2020,function(x) {\r\n  read_xlsx(paste0(\"data/fsi-\",x,\".xlsx\")) %>%\r\n    select(1:16)\r\n}) %>% \r\n  do.call(rbind,.) %>%\r\n  mutate(Year=format(Year,\"%Y\"))\r\n\r\ndf_long <- df %>%\r\n  tidyr::pivot_longer(cols = `C1: Security Apparatus`:`X1: External Intervention`,\r\n                      names_to=\"category\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-03-the-fragile-state-index/the-fragile-state-index_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-08T17:55:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-12-select-points-on-maps/",
    "title": "Select points on maps",
    "description": "When you want to show spatial data with additional information for each data point you can use shiny for interactive exploration.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-02-12",
    "categories": [
      "shiny",
      "spatial"
    ],
    "contents": "\r\n\r\nContents\r\nThe result\r\nPackages\r\nThe data\r\nQuick exploration\r\nFunctions\r\nShiny App\r\nClosing comments\r\n\r\nThe result\r\nThis is what we want to achieve. The user can select one or more points on the map to see additional information for the position.\r\n\r\n\r\n\r\nPackages\r\nTo run the example successfully, I needed the following packages.\r\n\r\n\r\nlibrary(shiny)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nThe data\r\nWe will work with some example data, consisting in random points and for each of the points some temporal information.\r\n\r\nlon\r\nlat\r\nmonth_1\r\nmonth_2\r\nmonth_3\r\nmonth_4\r\nmonth_5\r\nmonth_6\r\nmonth_7\r\n9.0\r\n52.2\r\n30\r\n31\r\n41\r\n58\r\n76\r\n91\r\n76\r\n9.5\r\n53.6\r\n24\r\n34\r\n26\r\n43\r\n56\r\n65\r\n69\r\n12.3\r\n49.4\r\n78\r\n94\r\n108\r\n128\r\n134\r\n136\r\n134\r\n8.3\r\n52.2\r\n28\r\n42\r\n52\r\n65\r\n67\r\n70\r\n90\r\n8.2\r\n50.1\r\n58\r\n76\r\n78\r\n74\r\n73\r\n95\r\n108\r\n\r\nWe first convert the data to be in tidy format (using pivot_longer from the {tidyr} package), i.e. we will have one column called month and one column containing the values.\r\n\r\nlon\r\nlat\r\nmonth\r\nvalue\r\n9\r\n52.2\r\n1\r\n30\r\n9\r\n52.2\r\n2\r\n31\r\n9\r\n52.2\r\n3\r\n41\r\n9\r\n52.2\r\n4\r\n58\r\n9\r\n52.2\r\n5\r\n76\r\n9\r\n52.2\r\n6\r\n91\r\n9\r\n52.2\r\n7\r\n76\r\n9\r\n52.2\r\n8\r\n74\r\n\r\nQuick exploration\r\nLet’s see what we can do with this data. First, we can use borders to show a quick map.\r\n\r\n\r\nmap <- ggplot(df)+\r\n  geom_point(aes(x=lon,y=lat))+\r\n  borders(regions=\"Germany\")+\r\n  coord_quickmap()+\r\n  theme_void()\r\n\r\nmap\r\n\r\n\r\n\r\n\r\nAs we want to be able to select points interactively, we want to display which points were selected. This is what the map would look like.\r\n\r\n\r\nexmpl <- df %>% slice(1) %>% select(lon,lat)\r\n\r\nmap + \r\n  geom_point(data=exmpl,aes(x=lon,y=lat),size=3,col=\"red\")\r\n\r\n\r\n\r\n\r\nLast, we want to show the temporal development for the selected points. This can be an easy line chart, for the filtered point.\r\n\r\n\r\ndf %>%\r\n  filter(lon == exmpl$lon,lat == exmpl$lat) %>%\r\n  mutate(month=factor(month.abb[month],levels=month.abb)) %>%\r\n    ggplot(aes(x=month,group=1))+\r\n    geom_line(aes(y=value),size=2)\r\n\r\n\r\n\r\n\r\nFunctions\r\nTo make the Shiny app short and concise, I will put the functionalities which create the three plots into functions.\r\nplot_map(df): Creates the map with the points.\r\nplot_selected_point_in_map(df,lon,lat): Creates the same map but highlighting the points in lon and lat.\r\nshow_detail(df,lon,lat): Creates the line plot for selected points.\r\nShiny App\r\nFirst, we will define the user interface. In this case we have a sidebar panel which contains the map and a main panel where the line plot will appear when points are selected.\r\nThe important lines to notice are that in plotOutput for the map, we will add the parameters click and brush. Doing this, we can later define what happens when someone clicks on the points.\r\n\r\n\r\nui <- fluidPage(\r\n\r\n    sidebarLayout(\r\n        sidebarPanel(\r\n          p(\"Please select one or various points.\"),\r\n            plotOutput(\"example_map\", \r\n                       click = \"plot1_click\",\r\n                       brush = \"plot1_brush\"\r\n            )\r\n        ),\r\n\r\n        mainPanel(\r\n            plotOutput(\"time_plot\")\r\n        )\r\n    )\r\n)\r\n\r\n\r\n\r\nInside the server function, we have several steps, which I will explain one by one. First, we need some variables that will be updated when the user selects points. lat, lon and time are empty in the beginning. For map, we will plot the initial map which shows all points.\r\n\r\n\r\nvals <- reactiveValues(lat=NULL,\r\n                       lon=NULL,\r\n                       map=plot_map(df), \r\n                       time = NULL) \r\n\r\n\r\n\r\nFor the two plot outputs it is easy, we will just return the respective variable of the reactive value, i.e. the map and the line chart.\r\n\r\n\r\noutput$example_map <- renderPlot({\r\n  return(vals$map)\r\n})\r\n    \r\noutput$time_plot <- renderPlot({\r\n  return(vals$time)\r\n})\r\n\r\n\r\n\r\nThe key for the triggered action is the process of selecting a point. Remember that in the ui part we specified another parameter for the map: click = \"plot1_click\". Here we will specify what happens when someone clicks on the map.\r\nFirst, we will check if there is a close point to the click in the dataset df. If this is not the case, nothing happens. Otherwise, we will assign the coordinates of the selected point to vals$lat and vals$lon our reactive variables. Additional we will change the map from the standard map to the map with selected red points and will show the line plot for the selected points. As these are reactive values, our plots will be updated automatically.\r\n\r\n\r\nobserveEvent(input$plot1_click, {\r\n    point <- nearPoints(df %>% distinct(lon,lat), input$plot1_click, addDist = FALSE)\r\n    if(length(point[[1]])==0) {} #happens when no point is selected\r\n    else {\r\n        vals$lon <- point[[1]]\r\n        vals$lat <- point[[2]]\r\n        vals$map <- plot_selected_point_in_map(df,vals$lon,vals$lat)\r\n        vals$time <-  show_detail(df,vals$lon,vals$lat)\r\n    }\r\n})\r\n\r\n\r\n\r\nYou will see that the brush option is very similar. Note, that there are two other options, hover and dblclick which can trigger actions when you just hover over a points or double click. This would be the complete server function when putting the steps together.\r\n\r\n\r\nserver <- function(input, output) {\r\n    \r\n    vals <- reactiveValues(lat=NULL,\r\n                           lon=NULL,\r\n                           map=plot_map(df),\r\n                           time = NULL)\r\n    \r\n    output$example_map <- renderPlot({\r\n        return(vals$map)\r\n    })\r\n    \r\n    output$time_plot <- renderPlot({\r\n      return(vals$time)\r\n        \r\n    })\r\n    \r\n    observeEvent(input$plot1_click, {\r\n        point <- nearPoints(df %>% distinct(lon,lat), input$plot1_click, addDist = FALSE)\r\n        if(length(point[[1]])==0) {} #happens when no point is selected\r\n        else {\r\n            vals$lon <- point[[1]]\r\n            vals$lat <- point[[2]]\r\n            vals$map <- plot_selected_point_in_map(df,vals$lon,vals$lat)\r\n            vals$time <- show_detail(df,vals$lon,vals$lat)\r\n        }\r\n        \r\n    })\r\n    \r\n    observeEvent(input$plot1_brush, {\r\n        point <- brushedPoints(df %>% distinct(lon,lat), input$plot1_brush)\r\n        if(length(point[[1]])==0) {} #happens when no point is selected\r\n        else {\r\n            vals$lon <- point[[1]]\r\n            vals$lat <- point[[2]]\r\n            vals$map <- plot_selected_point_in_map(df,vals$lon,vals$lat)\r\n            vals$time <- show_detail(df,vals$lon,vals$lat)\r\n        }\r\n    })\r\n}\r\n\r\n\r\n\r\nClosing comments\r\nI used this functionality in a basic app which is exploring changes in temperature, precipitation and soil moisture in Chile in the last 10 years (compared to the 30 years average). The first draft can be found here.\r\nOf course, this interactivity is not limited to maps, you can use this to make any type of plot more interactive and create very insightful Shiny apps.\r\nIn case you are interested in the data creation process for this example. I used the wakefield package which creates series of columns.\r\n\r\n\r\nlibrary(wakefield)\r\n\r\ndf <- r_data_frame(\r\n  n=50,\r\n  lon = runif(), \r\n  lat = runif(), \r\n  r_series(age,j=12,integer=TRUE, relate = \"+5_10\", name=\"month\")\r\n) %>%\r\n  mutate(lon=round(7.5+(12.5-7.5)*lon,1),\r\n         lat = round(48+(54-48)*lat,1))\r\n\r\n\r\n\r\nLater I transformed the data to long format.\r\n\r\n\r\ndf <- df %>%\r\n  tidyr::pivot_longer(cols=starts_with(\"month\"), names_to = \"month\",values_to=\"value\") %>%\r\n  mutate(month=readr::parse_number(month))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-12-select-points-on-maps/img/gif.gif",
    "last_modified": "2021-02-15T11:13:48+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-31-tracking-covid-in-germany/",
    "title": "Tracking COVID in Germany",
    "description": "Animations can help to show events over time. I found data from the RKI about daily COVID cases in Germany and want to describe the process of creating the animation. It involves fuzzy matching, as the names of the counties (Landkreise) are not identical in the RKI data and the shapefile I used.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-01-31",
    "categories": [
      "covid",
      "germany",
      "fuzzy matching",
      "spatial"
    ],
    "contents": "\r\n\r\nContents\r\nThe result\r\nThe data\r\nA map with missing values\r\nFuzzy matching\r\nCreating a gif\r\nClosing comments\r\n\r\nThe result\r\n\r\n\r\n\r\nThe data\r\nI found a dataset about COVID cases in the different communes (LK) in Germany in an Excel file online (Link). It shows cases (Fälle) and the 7-Day-Incidence value (per 100,000 inhabitants).\r\n(Note: unfortunately, the RKI changed the format of the data shortly after I downloaded the file, now the same data is only available on a broader Bundesland level. You might want to try this Link).\r\n\r\nLK\r\nFälle\r\nInzidenz\r\nSK Augsburg\r\n999\r\n336.8377\r\nSK Herne\r\n464\r\n296.5823\r\nSK Berlin Neukölln\r\n917\r\n285.5604\r\nSK Duisburg\r\n1401\r\n280.9383\r\nLK Rottal-Inn\r\n335\r\n275.7156\r\nLK Bautzen\r\n780\r\n260.2099\r\n\r\nThe SK (Stadt) and LK (Landkreis) are important, as they indicate whether it is about the city or the county around the city. In some cases you have both.\r\n\r\nLK\r\nFälle\r\nInzidenz\r\nLK Würzburg\r\n237\r\n146.02408\r\nSK Würzburg\r\n163\r\n127.40945\r\nSK Aschaffenburg\r\n59\r\n83.09625\r\nLK Aschaffenburg\r\n130\r\n74.62687\r\n\r\nNow, if I want to show this information on a map, I need a shapefile with the communes. The good news is, there is one (Link). The bad news is, the names are in a different format than in the Excel file with the COVID cases.\r\n\r\nGEN\r\nBEZ\r\nFlensburg\r\nKreisfreie Stadt\r\nKiel\r\nKreisfreie Stadt\r\nLübeck\r\nKreisfreie Stadt\r\nNeumünster\r\nKreisfreie Stadt\r\nDithmarschen\r\nKreis\r\nHerzogtum Lauenburg\r\nKreis\r\n\r\nA first fix to bring the two formats closer together would be to convert BEZ into LK and SK and paste it together with GEN.\r\n\r\nname\r\nSK Flensburg\r\nSK Kiel\r\nSK Lübeck\r\nSK Neumünster\r\nLK Dithmarschen\r\nLK Herzogtum Lauenburg\r\n\r\nA map with missing values\r\nThis already helps a lot. Let’s see if we can join the COVID indicence values to the shapefile with the communes.\r\n\r\n\r\n\r\nThis already looks quite good, but there are a lot of missing communes.\r\nLet’s look into the Excel file with COVID cases which ones did not have a match.\r\n\r\nLK\r\nFälle\r\nInzidenz\r\nSK Berlin Neukölln\r\n917\r\n285.5604\r\nSK Offenbach\r\n330\r\n253.3006\r\nSK Mülheim a.d.Ruhr\r\n378\r\n221.5294\r\nSK Berlin Friedrichshain-Kreuzberg\r\n617\r\n218.7486\r\nSK Berlin Reinickendorf\r\n554\r\n213.3321\r\nLK Lindau\r\n162\r\n197.6068\r\n\r\nCan we find equivalent names in the shapefile? We search for Berlin, Offenbach, Mülheim and Lindau.\r\n\r\nname\r\nSK Mülheim an der Ruhr\r\nSK Offenbach am Main\r\nLK Offenbach\r\nLK Lindau (Bodensee)\r\nSK Berlin\r\n\r\nWe notice a few things: Berlin has only one entry in the shapefile and a more detailed breakdown in the COVID cases document. SK Offenbach is called Offenbach am Main, Lindau is called Lindau (Bodensee). And Mülheim a.d.Ruhr is written Mülheim an der Ruhr.\r\nOne option would be to find all the matches manually and replace them. There are around 40, so this would be feasible but a little annoying. Maybe we can save this time of comparing each element and let the computer find the best match for us.\r\nFuzzy matching is doing exactly this: Based on string distances, it finds the closest match in the other source.\r\nFuzzy matching\r\nI wrote a function to do the calculation of the string distances and find the best fit. Additionally, we have the option to clean the input before, i.e. transform all letters to lowercase, remove or replace certain words which disturb the process. Usually such words would become clear after running the matching process once and noticing some undesired results.\r\n\r\noriginal\r\nbest_fit\r\nsimilarity\r\nStadtRegion Aachen\r\nLK Städteregion Aachen\r\n0.2315310\r\nSK Berlin Friedrichshain-Kreuzberg\r\nSK Berlin\r\n0.1470588\r\nSK Berlin Steglitz-Zehlendorf\r\nSK Berlin\r\n0.1379310\r\nRegion Hannover\r\nLK Region Hannover\r\n0.1333333\r\nSK Berlin Reinickendorf\r\nSK Berlin\r\n0.1217391\r\nLK Lindau\r\nLK Lindau (Bodensee)\r\n0.1100000\r\nLK Sankt Wendel\r\nLK St. Wendel\r\n0.0970513\r\nSK Berlin Spandau\r\nSK Berlin\r\n0.0941176\r\nSK Neustadt a.d.Weinstraße\r\nSK Neustadt an der Weinstraße\r\n0.0915340\r\nSK Mülheim a.d.Ruhr\r\nSK Mülheim an der Ruhr\r\n0.0841542\r\nSK Ludwigshafen\r\nSK Ludwigshafen am Rhein\r\n0.0750000\r\nSK Frankenthal\r\nSK Frankenthal (Pfalz)\r\n0.0727273\r\nSK Landau i.d.Pfalz\r\nSK Landau in der Pfalz\r\n0.0665072\r\nSK Halle\r\nSK Halle (Saale)\r\n0.0545455\r\nLK Neustadt a.d.Aisch-Bad Windsheim\r\nLK Neustadt a.d. Aisch-Bad Windsheim\r\n0.0341270\r\nSK Freiburg i.Breisgau\r\nSK Freiburg im Breisgau\r\n0.0264822\r\nLK Mühldorf a.Inn\r\nLK Mühldorf a. Inn\r\n0.0111111\r\nLK Neumarkt i.d.OPf.\r\nLK Neumarkt i.d. OPf.\r\n0.0095238\r\nLK Pfaffenhofen a.d.Ilm\r\nLK Pfaffenhofen a.d. Ilm\r\n0.0083333\r\nLK Wunsiedel i.Fichtelgebirge\r\nLK Wunsiedel i. Fichtelgebirge\r\n0.0066667\r\n\r\nThis table serves as a control point. We can check that all the items were matched correctly. It also serves as a dictionary which we will use to replace the original names before joining.\r\n\r\n\r\n\r\nCreating a gif\r\nTo create the gif, we select dates of interest (in our case day by day from Feb 15 to Nov 25).\r\n\r\n\r\nsel_dates <- seq.Date(from=as.Date(\"2020-02-15\"),to=as.Date(\"2020-11-25\"),by=\"day\")\r\nimg_frames <- paste0(\"covid\", seq_along(sel_dates), \".png\")\r\n\r\n\r\n\r\nWe loop through the dates and create one image for each day. Note how easy it is to create the Germany map with ggplot(aes(fill=cases7_per_bin))+geom_sf() from the {sf} package. The rest is just changing the colors.\r\n\r\n\r\nfor (i in seq_along(sel_dates)) {\r\n  \r\n  message(paste(\" - image\", i, \"of\", length(sel_dates)))\r\n  \r\n  map <- final %>%\r\n    filter(date %in% sel_dates[i]) %>%\r\n    ggplot(aes(fill=cases7_per_bin))+geom_sf()+\r\n    labs(title = 'COVID in Germany',\r\n         subtitle = format(sel_dates[i],\"%B\"),\r\n         fill = \"7 day incidence values\",\r\n         caption = \"Data from: NPGEO Corona, Hub RKI\") +\r\n    scale_fill_manual(values=c(\"grey80\",\"grey60\",\r\n                               \"grey40\",\"gold2\",\"orangered1\",\"red3\",\"red4\"),drop=FALSE)\r\n\r\n\r\n  png(filename=paste0(\"covid\",i,\".png\"),width = 800,height = 800)\r\n  print(map)\r\n  dev.off()\r\n}\r\n\r\n\r\n\r\nAfter saving all the images, we can use {magick}’s image_write_gif function to put them together and create our gif.\r\n\r\n\r\nmagick::image_write_gif(magick::image_read(img_frames),\r\n                        path = \"covid.gif\",\r\n                        delay = 1/10)\r\n\r\n\r\n\r\nClosing comments\r\nI recently started putting the fuzzy matching functionality into a package. Check out the first version of {fuzzymatch} here. The function fuzzy_matches performs the task of matching names from one datasource to another.\r\nThe code behind this example was written with R and can be found here (under Day 25 - COVID)\r\nI previously used fuzzy matching in an example from the banking world (matching company names) and talked about this in Latin-R 2019, you can find the presentation and the deck here.\r\nIf you wonder about what happened to the Berlin case in our example above, I did not take extra care of it. In practice one would have to sum up the cases and find an average incidence value to replace the values from different parts of the city. As this was not directly related to the fuzzy matching exercise, I omitted it.\r\nI am very curious if there are other applications for fuzzy matching in the GIS world, or if there are other methods to overcome a problem like this. Feel free to reach out to me.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-31-tracking-covid-in-germany/img/covid.gif",
    "last_modified": "2021-02-14T17:11:53+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-30daymapchallenge/",
    "title": "What I learned: #30DayMapChallenge",
    "description": "During November 2020 I participated in the 30DayMapChallenge, initiated by Topi Tjukanov. I created 30 maps according to the daily topics and want to summarise what I learned in this blogpost.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "spatial",
      "rayshader",
      "geogrid",
      "sf",
      "ggrepel",
      "webglobe"
    ],
    "contents": "\r\n\r\nContents\r\nMy mapping experience\r\nInteresting datasetsEarthquakes\r\nElevation data\r\nChile Shapefiles\r\nElection results Plebiscito Chile\r\nForest fires in Chile\r\nGlobal precipitation and temperatures\r\nOpen Trade Statistics\r\nLand cover and more\r\nMovebank for animal tracking\r\nCOVID history Germany\r\nNASA Worldview\r\n\r\nHow to plot the world / countries?ggplot2 + sf\r\nOpen Street Map\r\ntmap\r\nleaflet\r\n\r\nInteresting techniquesinset_element\r\ngeogrid\r\nrayshader\r\ngganimate\r\nFuzzy matching\r\nEmojis on a map\r\nwebglobe\r\nggrepel\r\n\r\nClosing words\r\n\r\nThis was the map I created for the last day. It shows around which places I created my maps (indicating which place was shown on which day). I invite you to look at my codes on Github (Link) in case that any of the topics mentioned in this post are of a particular interest for you.\r\n\r\nMy mapping experience\r\nBefore 2020 I had no idea of how to create a map. At the beginning of the year there was a GIS workshop for the Green Initiative Group at our company and an expert explained us about raster and vector data, where to find data about Chile and many more interesting things around GIS.\r\nIn May, I participated in the awesome tutorial by Stephanie Orellana Bello about Spatial Data in R (Link to the tutorial in Spanish). This allowed me to do my first steps on my own. I am familiar with R, but had to learn a lot about the spatial part.\r\nI learned a lot during the Challenge and would like to summarize the most interesting things. Overall, my learnings can be split into three large topics:\r\nInteresting data\r\nHow to plot the world / countries?\r\nInteresting techniques (and packages)\r\nInteresting datasets\r\nFor datasets it is important that they can be downloaded easily and are in a format that can either be directly mapped or combined with a prebuild world or country map.\r\nEarthquakes\r\nFollowing this Link you can get information about earthquakes. It is highly customizable (you can select an area of interest on a map and select a timeframe of interest).\r\nI used it to create the map for Day 1 (Points).\r\n\r\n\r\n\r\nElevation data\r\nTo download worldwide elevation data, you have two options (probably many more, but these are the two ones I tried). For the first one, you would need to register at NASA Earthdata and can click on the square of interest on this website (I used this during day 11 - 3D). Alternatively, you can also use the get_elev_raster() function of the {elevatr} package (compare day 24 - Elevation).\r\nChile Shapefiles\r\nThe Biblioteca del Congreso Nacional de Chile (Link) provides some interesting shapefiles around regions and communes in Chile. Moreover, I used the data about water masses (lakes, lagoons etc.) on two submissions.\r\nElection results Plebiscito Chile\r\nThe results of the chilean elections for a new constitution on October 26th are a very interesting dataset to analyze. Gonzalo Diaz has made the results of each commune available following this link.\r\nForest fires in Chile\r\nThere are many forest fires in the central regions of Chile every year during summer. CONAF makes the numbers of the last years available here. This is an excellent example of an untidy dataset. I have a cleaning skript for that dataset here. After cleaning it is easy to look at trends over time and per region.\r\nGlobal precipitation and temperatures\r\nWorldwide historical data (until today) about temperature, precipitation, radiation and many more interesting measurements can be found at the Physical Sciences Laboratory of the NOAA (National Oceanic and Atmospheric Administration) here. I used this for two days: Day 13 - Raster where I showed the precipitation of every day in 2019 in Germany and Day 14 - Climate Change where I showed precipitation anomalies in South America.\r\n\r\nOpen Trade Statistics\r\nMauricio Vargas created the excellent {tradestatistics} package (info). It contains historical information of trade between countries. I used this on day 15 - Connections, to show how much trash has been exported from the US into different countries.\r\nLand cover and more\r\nThe OECD.Stat website contains a lot of interesting data on country level (Link). I used this on day 18 - Land use to see which percentage of which country is used for agricultural purposes.\r\nMovebank for animal tracking\r\nThis interesting project makes available some data about animal movement. Not all datasets are freely available, but a lot of them can be downloaded and analyzed here.\r\n\r\nCOVID history Germany\r\nNPGEO Corona registered all COVID cases per German county (Landkreis) since the beginning of the pandemy. You can download the data here. In order to bring this on a map, you might also be interested in the shapefiles of the counties which can be downloaded here.\r\n\r\nNASA Worldview\r\nNASA Worldview is an amazing project which contains worldwide satellite imagery for many many applications. Not all of it can be downloaded, but it is definitely worth taking a look (Link). NASA also has a short tutorial about how to use the website here.\r\nHow to plot the world / countries?\r\nggplot2 + sf\r\nggplot2 is one of the best packages R can offer (I use it a lot for data visualization). Now I learned that it is also great to work with maps.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nggplot() + \r\n  borders(regions = c(\"Germany\",\"France\")) +\r\n  coord_quickmap()\r\n\r\n\r\n\r\n\r\nInstead of plotting directly, you can also get the borders of a country as a dataframe with the map_data(map=\"world\",region=\"Chile\") (this can be useful in case you want to transform it). I used this function to plot Chile over Europe for Day 29 - Globe.\r\n\r\nOpen Street Map\r\nIf you have downloaded any shapefiles (for example from one of the previous examples) you can use the {sf} package to work with them and still plot them with ggplot. The Open Street Map project also provides shapefiles of all types of interesting points, lines and areas.\r\n\r\n\r\nlibrary(osmdata) #Open Street Map\r\nlibrary(sf)\r\n\r\n# Unfortunately the bounding box of Valparaiso is not super good, so we adapt it\r\nbbox <- getbb(\"Valparaíso,Valparaiso Chile\")\r\nbbox[1,] <- c(-71.6615, -71.5607)\r\nbbox[2,] <- c(-33.08, -33.017)\r\n\r\n\r\nstreets <- bbox %>%\r\n  opq() %>%\r\n  add_osm_feature(key = \"highway\") %>%\r\n  osmdata_sf()\r\n\r\nggplot()+\r\n  geom_sf(data = streets$osm_lines,col=\"grey50\",size=0.5)+\r\n  theme_void()\r\n\r\n\r\n\r\n\r\ntmap\r\n\r\n\r\nlibrary(tmap)\r\n\r\ndata(\"World\")\r\n\r\ntm_shape(World)+\r\n  tm_polygons(col=\"life_exp\")\r\n\r\n\r\n\r\n\r\nleaflet\r\nThere are a lot of different designs available. Just change providers$OpenStreetMap for providers$Stamen.Watercolor or others to have a different view of the world.\r\nYou can add rectangles, labels, markers and more and zoom into the map.\r\n\r\n\r\nlibrary(leaflet)\r\n\r\nleaflet() %>%\r\n  addProviderTiles(provider=providers$OpenStreetMap) %>%\r\n  addRectangles(5.99, 47.30, 15.02, 54.98)\r\n\r\n\r\n\r\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addProviderTiles\",\"args\":[\"OpenStreetMap\",null,null,{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addRectangles\",\"args\":[47.3,5.99,54.98,15.02,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2,\"smoothFactor\":1,\"noClip\":false},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[47.3,54.98],\"lng\":[5.99,15.02]}},\"evals\":[],\"jsHooks\":[]}\r\nInteresting techniques\r\ninset_element\r\nDuring the Challenge Thomas Lin Pedersen announced an exciting update of the {patchwork} package. One function called inset_element was added which allows to place a plot inside a plot.\r\nIt is incredibly easy to use. For the Day 9 - Monochrome submission, I created a small map of South America to show the location of Valparaíso and added it as an inset to the prior map called valpo.\r\n\r\n\r\nvalpo + inset_element(south_america,0.4,0.85,0.5,1,align_to = \"full\")\r\n\r\n\r\n\r\n\r\n\r\n\r\ngeogrid\r\nJoseph Bailey created this awesome package which converts a shapefile into a similar gridded shapefile. This can be very helpful if you have some very small and some very large areas and it is hard to see the smaller areas. One has to pay this equal representation with the loss of shapes, so you will lose the fact that people can recognize an area by its shape. I used this to show the results of the Chilean election results (there were only two options, so it can be shown as a percentage). Take a look to see the advantages and disadvantages of each version.\r\n\r\n\r\n\r\nrayshader\r\nThis package, created by Tyler Morgan-Wall, is awesome! It creates 3D plots or 3D elevation maps which look great and are highly customizable. I did my first steps with this package during the challenge.\r\n\r\n\r\n#Downloaded from the elevation data source from part 1\r\nperu <- raster(\"C:/Richard/R and Python/Datasets/Peru Elevation/S09W078.hgt\")\r\n\r\n#smaller area\r\nbbox <- extent(-77.680,-77.630,-8.910,-8.850)\r\n\r\nalpamayo <- crop(peru, bbox)\r\n\r\nelmat <- raster_to_matrix(alpamayo)\r\n\r\nelmat %>%\r\n  sphere_shade(texture = \"desert\",sunangle = 270) %>%\r\n  add_shadow(ray_shade(elmat, zscale = 30), 0.5) %>%\r\n  plot_3d(elmat, zscale = 30, fov = 0, theta = 45, zoom = 0.75, phi = 20, windowsize = c(1000, 800),\r\n          baseshape = \"circle\")\r\nSys.sleep(0.2)\r\nrender_snapshot(\"Day11_3D/plot.png\")\r\n\r\n\r\n\r\nThis creates one view of the Alpamayo mountain in Peru. It is also easy to create many of those and create a gif.\r\n\r\ngganimate\r\nNow that we are already talking about animated plots, I want to highlight the package {gganimate} (also by Thomas Lin Pedersen) which makes a gif out of a ggplot.\r\nFuzzy matching\r\nWhen you find an interesting dataset which is not linked to a shapefile, you can not map it directly, but have to join it to a suitable shapefile first. During this join it can happen that the names are not exactly the same (e.g. United States vs. United States of America, or Korea Republic vs. South Korea). Fuzzy matching can help you to find inexact matches. As this is a longer topic, I actually wrote a whole post about this during day 19 - NULL, which can be found here.\r\nEmojis on a map\r\nI guess there are also many ways to add emojis to a map. I used this to show Brewery locations of a private alcohol-free beer testing we did with my family.\r\nA minimal example would look like this, where I added the latitude, longitude and average rating (Durchschnitt) to each brewery.\r\n\r\n\r\nggplot(data=breweries,aes(x=lon,y=lat))+\r\n  borders(regions=\"Germany\",fill=\"grey30\")+\r\n  geom_text(label=\"\\U0001F37A\",family=\"EmojiOne\",size=10,aes(col=Durchschnitt))+\r\n  scale_color_gradient(low=\"red\",high=\"yellow\")+\r\n  theme_void()\r\n\r\n\r\n\r\nThe final code was slightly longer (mainly to change the layout, crop the map etc.), but the idea of using Emojis is to find the Emoji unicode value online and add it as text with geom_text, using “EmojiOne” as font family.\r\n\r\nwebglobe\r\nI was not aware of how easy it is to work with plots of the globe. The {webglobe} package makes it very easy to highlight countries or points. Check the code of Day 29 - Globe for the whole example, it is not very long, I promise.\r\n\r\n\r\nlibrary(webglobe)\r\n\r\nwebglobe(immediate=FALSE) + \r\n  wgpolygondf(chile_shift,fill=\"purple\",stroke_width = 2) +\r\n  wgcamcenter(50, 20, 8000)\r\n\r\n\r\n\r\nggrepel\r\nAmong the maps submitted to the #30DayMapChallenge I most enjoyed those that told a story. Annotations are a helpful tool when describing certain points in a map. The {ggrepel} function helps to avoid overlaps between labels. I used this in many maps, for example during Day 17 - Historical where I explained how the midpoint of Europe shifted in the past when countries joined or left.\r\n\r\nClosing words\r\nParticipating in the #30DayMapChallenge was a lot of fun, a big thanks to Topi Tjukanov for organizing it and congratulations to all participants. It was great to browse through the contributions on Twitter, learn from you and dream about what type of cool maps I might be able to produce at some point in the future.\r\nThe description and rules of the challenge can be found here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-28-30daymapchallenge/img/day30.png",
    "last_modified": "2021-04-30T22:46:44+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-03-useful-packages-for-data-composition/",
    "title": "Useful packages for data simulation",
    "description": "We will explore the packages wakefield, rcorpora, charlatan, fabricatr, and GenOrd which can be helpful for data simulation.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-12-03",
    "categories": [
      "simulation",
      "packages"
    ],
    "contents": "\r\n\r\nContents\r\nAdditional packages\r\nwakefieldSeries\r\n\r\nrcorpora\r\ncharlatan\r\nfabricatrOrdered data\r\nTime series\r\n\r\nGenOrd\r\nMore packages\r\n\r\nWhen we simulate data we can rely on the distribution functions like rnorm, rexp and sample from base R. However, we can also leverage the great work from authors of packages which were written to make the simulation process easier. In this blogpost I will explore some of them.\r\nAdditional packages\r\nBefore starting with the simulation packages, we can load these two packages which will help with data transformation and visualization.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nwakefield\r\nLooking for interesting packages around data simulation I stumbled across the {wakefield} package by Tyler Rinker.\r\n\r\n\r\nlibrary(wakefield)\r\n\r\n\r\n\r\nIntroduction can be found here. It is very easy to create data with all types of variables.\r\n\r\n\r\nr_data_frame(\r\n    n = 500,\r\n    id,\r\n    age,\r\n    iq,\r\n    height,\r\n    died,\r\n    animal,\r\n    internet_browser,\r\n    political\r\n)\r\n\r\n\r\n# A tibble: 500 x 8\r\n   ID      Age    IQ Height Died  Animal            Browser Political \r\n   <chr> <int> <dbl>  <dbl> <lgl> <fct>             <fct>   <fct>     \r\n 1 001      69    91     69 TRUE  Chinchilla        Chrome  Democrat  \r\n 2 002      49    87     69 FALSE Galapagos Penguin Chrome  Republican\r\n 3 003      60   112     64 FALSE Birds Of Paradise Firefox Republican\r\n 4 004      43    97     69 TRUE  Sea Squirt        IE      Democrat  \r\n 5 005      32    92     75 TRUE  Galapagos Penguin Chrome  Republican\r\n 6 006      42    99     70 FALSE Ocelot            Chrome  Republican\r\n 7 007      87    91     70 FALSE Poison Dart Frog  Chrome  Republican\r\n 8 008      23    91     67 TRUE  Drever            Firefox Democrat  \r\n 9 009      54    94     67 FALSE Pointer           Safari  Democrat  \r\n10 010      86    96     69 TRUE  Sea Squirt        Chrome  Democrat  \r\n# ... with 490 more rows\r\n\r\nThere are a lot of predefined variables that you can use. (Call variables(type=\"matrix\",ncols=5) to see them.)\r\n\r\n      [,1]         [,2]          [,3]               [,4]       \r\n [1,] \"age\"        \"dice\"        \"hair\"             \"military\" \r\n [2,] \"animal\"     \"dna\"         \"height\"           \"month\"    \r\n [3,] \"answer\"     \"dob\"         \"income\"           \"name\"     \r\n [4,] \"area\"       \"dummy\"       \"internet_browser\" \"normal\"   \r\n [5,] \"car\"        \"education\"   \"iq\"               \"political\"\r\n [6,] \"children\"   \"employment\"  \"language\"         \"race\"     \r\n [7,] \"coin\"       \"eye\"         \"level\"            \"religion\" \r\n [8,] \"color\"      \"grade\"       \"likert\"           \"sat\"      \r\n [9,] \"date_stamp\" \"grade_level\" \"lorem_ipsum\"      \"sentence\" \r\n[10,] \"death\"      \"group\"       \"marital\"          \"sex\"      \r\n      [,5]           \r\n [1,] \"sex_inclusive\"\r\n [2,] \"smokes\"       \r\n [3,] \"speed\"        \r\n [4,] \"state\"        \r\n [5,] \"string\"       \r\n [6,] \"upper\"        \r\n [7,] \"valid\"        \r\n [8,] \"year\"         \r\n [9,] \"zip_code\"     \r\n[10,]                \r\nattr(,\"class\")\r\n[1] \"matrix\" \"array\" \r\n\r\nAdditionally, you can access the distribution functions easily and tweak parameters of the predefined functions.\r\n\r\n\r\ntest <- r_data_frame(\r\n    n = 500,\r\n    id,\r\n    age(x=18:50),\r\n    `Reading(mins)` = rpois(lambda=20),\r\n    income(digits=0)\r\n)\r\n\r\n\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      39              22  49775\r\n 2 002      19              25  19148\r\n 3 003      19              20  32714\r\n 4 004      41              19  11000\r\n 5 005      35              21  64802\r\n 6 006      29              20  36548\r\n 7 007      43              21  62348\r\n 8 008      35              18  24371\r\n 9 009      50              18 135653\r\n10 010      48              23  51157\r\n# ... with 490 more rows\r\n\r\nLooks too perfect? Include random missing values in columns 2 and 4: (Note: If you create a larger dataframe, you can use the %>% operator to structure your code better).\r\n\r\n\r\ntest <- r_na(test, cols=c(2,4),prob=0.3)\r\n\r\n\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      39              22     NA\r\n 2 002      19              25     NA\r\n 3 003      19              20     NA\r\n 4 004      41              19  11000\r\n 5 005      NA              21  64802\r\n 6 006      29              20  36548\r\n 7 007      43              21     NA\r\n 8 008      35              18     NA\r\n 9 009      50              18     NA\r\n10 010      NA              23  51157\r\n# ... with 490 more rows\r\n\r\nSeries\r\n{wakefield} allows us to create several variables which can be seen as a sequence, for example survey results.\r\n\r\n\r\nr_series(likert,j = 5,n=10,name=\"Question\")\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Question_1    Question_2    Question_3    Question_4    Question_5 \r\n * <ord>         <ord>         <ord>         <ord>         <ord>      \r\n 1 Strongly Dis~ Agree         Disagree      Strongly Agr~ Disagree   \r\n 2 Disagree      Strongly Dis~ Strongly Agr~ Agree         Agree      \r\n 3 Strongly Agr~ Neutral       Strongly Agr~ Strongly Agr~ Disagree   \r\n 4 Disagree      Strongly Agr~ Neutral       Strongly Agr~ Strongly A~\r\n 5 Agree         Strongly Dis~ Agree         Strongly Dis~ Neutral    \r\n 6 Strongly Dis~ Disagree      Strongly Agr~ Strongly Agr~ Disagree   \r\n 7 Strongly Agr~ Strongly Agr~ Strongly Dis~ Strongly Agr~ Agree      \r\n 8 Strongly Agr~ Neutral       Neutral       Agree         Strongly A~\r\n 9 Strongly Dis~ Strongly Agr~ Strongly Dis~ Neutral       Strongly A~\r\n10 Strongly Dis~ Disagree      Disagree      Strongly Dis~ Disagree   \r\n\r\nThese can also be packaged inside a data frame, for example when simulating test results for students.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1 Test_2 Test_3\r\n   <chr>   <int>  <int>  <int>  <int>\r\n 1 01         16     89     85     91\r\n 2 02         17     91     87     85\r\n 3 03         13     84     88     85\r\n 4 04         13     91     89     83\r\n 5 05         10     86     87     78\r\n 6 06         15     87     87     84\r\n 7 07         12     83     85     90\r\n 8 08         16     89     94     84\r\n 9 09         14     93     82     91\r\n10 10         10     89     86     87\r\n\r\nThat is great but not very real, because the test results are completely independent from each other. The relate parameter inside the r_series function helps to connect the results, and the format is fM_sd.\r\nf is one of (+,-,*,/)\r\nM is the mean value\r\nsd is the standard deviation of the mean value\r\nExamples: * additive: +3_1: The test results get better on average 3 points with a standard deviation of 1. * multiplicative: *1.05_0.2: The results get better on average 5% with a standard deviation of 0.2.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\",relate=\"+3_1\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1     Test_2     Test_3    \r\n   <chr>   <int> <variable> <variable> <variable>\r\n 1 01         13 96.6       98.8       99.6      \r\n 2 02         18 90.7       93.4       98.3      \r\n 3 03         13 83.8       88.0       92.7      \r\n 4 04         10 91.6       93.3       96.1      \r\n 5 05         15 92.3       95.5       99.6      \r\n 6 06         16 88.9       92.0       95.8      \r\n 7 07         11 93.0       96.4       98.9      \r\n 8 08         10 91.5       95.0       96.5      \r\n 9 09         14 85.9       86.5       90.3      \r\n10 10         13 84.5       88.4       90.9      \r\n\r\nWith this in mind, you can create customer balances over time very easily.\r\n\r\n\r\nbalances <- r_data_frame(\r\n  n=10,\r\n  Client=name,\r\n  age,\r\n  r_series(income,j=12,name=\"Month\",relate=\"*1.03_0.1\")\r\n)\r\n\r\n\r\n\r\nThis result is worth to be visualized.\r\n\r\n\r\nbalances %>%\r\n  tidyr::pivot_longer(-c(1,2),names_to=\"Month\") %>%\r\n  mutate(Month=readr::parse_number(Month)) %>%\r\n  ggplot(aes(x=Month,y=value))+geom_line()+facet_wrap(~Client,scales=\"free_y\")\r\n\r\n\r\n\r\n\r\nWe can see that there are customers who had very positive balance development and others whose balances were fluctuating more or declining. However, when we simulate a sufficiently large number of customers, we will observe that on average the increase each month will be the desired 3% with a standard deviation of 0.1.\r\nrcorpora\r\nCheck the github repository here.\r\nThe rcorpora library has 293 collections of words that can be very helpful for data simulation.\r\n\r\n\r\nlibrary(rcorpora)\r\n\r\nlength(corpora())\r\n\r\n\r\n[1] 293\r\n\r\ncorpora()[sample(1:293,10)]\r\n\r\n\r\n [1] \"words/stopwords/de\"                            \r\n [2] \"humans/prefixes\"                               \r\n [3] \"humans/thirdPersonPronouns\"                    \r\n [4] \"animals/cats\"                                  \r\n [5] \"humans/celebrities\"                            \r\n [6] \"music/bands_that_have_opened_for_tool\"         \r\n [7] \"technology/knots\"                              \r\n [8] \"foods/iba_cocktails\"                           \r\n [9] \"words/adjs\"                                    \r\n[10] \"societies_and_groups/fraternities/professional\"\r\n\r\nTo view the words of one collection use the name in the corpora() function.\r\n\r\n\r\ncorpora(\"foods/pizzaToppings\")\r\n\r\n\r\n$description\r\n[1] \"A list of pizza toppings.\"\r\n\r\n$pizzaToppings\r\n [1] \"anchovies\"        \"artichoke\"        \"bacon\"           \r\n [4] \"breakfast bacon\"  \"Canadian bacon\"   \"cheese\"          \r\n [7] \"chicken\"          \"chili peppers\"    \"feta\"            \r\n[10] \"garlic\"           \"green peppers\"    \"grilled onions\"  \r\n[13] \"ground beef\"      \"ham\"              \"hot sauce\"       \r\n[16] \"meatballs\"        \"mushrooms\"        \"olives\"          \r\n[19] \"onions\"           \"pepperoni\"        \"pineapple\"       \r\n[22] \"sausage\"          \"spinach\"          \"sun-dried tomato\"\r\n[25] \"tomatoes\"        \r\n\r\nLet see how we can use this in a simulated dataframe.\r\n\r\n\r\ntibble(\r\n  first_name=corpora(\"humans/firstNames\")$firstNames %>% sample(100,replace=TRUE),\r\n  last_name=corpora(\"humans/lastNames\")$lastNames %>% sample(100,replace=TRUE),\r\n  self_description=corpora(\"humans/descriptions\")$descriptions %>% sample(100,replace=TRUE),\r\n  home_country=corpora(\"geography/countries\")$countries %>% sample(100,replace=TRUE),\r\n  favorite_pizza_topping=corpora(\"foods/pizzaToppings\")$pizzaToppings %>% sample(100,replace=TRUE)\r\n)\r\n\r\n\r\n# A tibble: 100 x 5\r\n   first_name last_name self_description home_country favorite_pizza_~\r\n   <chr>      <chr>     <chr>            <chr>        <chr>           \r\n 1 Jacob      Peterson  lean             Peru         sausage         \r\n 2 Ruby       Roberts   immature         Nicaragua    Canadian bacon  \r\n 3 Carson     Gonzales  quick            Haiti        bacon           \r\n 4 Shane      Stewart   one-sided        Nauru        ground beef     \r\n 5 Molly      Rice      civil            Iraq         breakfast bacon \r\n 6 Stephanie  Chavez    ambitious        Monaco       green peppers   \r\n 7 Kaden      Gutierrez picky            Egypt        Canadian bacon  \r\n 8 Valerie    Foster    irritating       Malta        spinach         \r\n 9 Genesis    Owens     talented         Tuvalu       green peppers   \r\n10 Amy        Wood      unfriendly       Vatican Cit~ hot sauce       \r\n# ... with 90 more rows\r\n\r\ncharlatan\r\nSimilar to wakefield, charlatan has some out-of-the-box variables that can be used in your simulated data.\r\n\r\n\r\nlibrary(charlatan)\r\n\r\nch_job(n=10)\r\n\r\n\r\n [1] \"Development worker, community\"    \r\n [2] \"Veterinary surgeon\"               \r\n [3] \"Advertising account executive\"    \r\n [4] \"Optician, dispensing\"             \r\n [5] \"Museum education officer\"         \r\n [6] \"Research officer, trade union\"    \r\n [7] \"Engineer, automotive\"             \r\n [8] \"Pharmacist, community\"            \r\n [9] \"Sales promotion account executive\"\r\n[10] \"Customer service manager\"         \r\n\r\nYou can even use get typical names or jobs for a given country. To see the available languages and countries type charlatan::PersonProvider$new()$allowed_locales().\r\n\r\n\r\nch_name(n=10,locale=\"de_DE\")\r\n\r\n\r\n [1] \"Ernestine Drubin\"             \"Burkhardt Koch B.Sc.\"        \r\n [3] \"Christof Barth B.Eng.\"        \"Univ.Prof. Burkhardt Beier\"  \r\n [5] \"Lambert Mans-Bolnbach\"        \"Prof. Darius Anders B.Sc.\"   \r\n [7] \"Univ.Prof. Felix Römer B.Sc.\" \"Kay-Uwe Atzler-Döhn\"         \r\n [9] \"Reingard Neuschäfer B.Eng.\"   \"Klaus-Dieter Hölzenbecher\"   \r\n\r\n\r\n\r\nch_phone_number(locale=\"de_DE\",n=10)\r\n\r\n\r\n [1] \"+49(0)7367 624040\"   \"03154789561\"         \"(08044) 075665\"     \r\n [4] \"(02605) 71714\"       \"+49(0)6273434607\"    \"06457 409315\"       \r\n [7] \"00114 07336\"         \"+49(0)6680 38728\"    \"04223885899\"        \r\n[10] \"+49 (0) 4533 749527\"\r\n\r\nA nice small application with fake locations and random R colors.\r\n\r\n\r\nlocations <- data.frame(lon=ch_lon(n=10),lat=ch_lat(n=10),col=ch_color_name(n=10))\r\n\r\nggplot(locations)+\r\n  borders(\"world\")+\r\n  geom_point(aes(x=lon,y=lat,col=col),size=3)+\r\n  coord_quickmap()\r\n\r\n\r\n\r\n\r\nfabricatr\r\nEasy creation of hierarchical data is possible with {fabricatr}. In this example there are five families, each one has between 1 and 12 members. Each family member has between 1 and 5 accounts. With add_level() we can automatically produce a table that shows all accounts of all members in all families.\r\n\r\n\r\nlibrary(fabricatr)\r\n\r\nfabricate(\r\n  family  = add_level(N = 5,\r\n  n_members = sample(1:12, N, replace = TRUE,prob=12:1)),\r\n  \r\n  members  = add_level(N = n_members,\r\n  n_accounts = sample(1:5,N,replace=TRUE,prob=(5:1)^2)),\r\n  \r\n  account = add_level(N = n_accounts)\r\n  ) %>%\r\nhead(10)\r\n\r\n\r\n   family n_members members n_accounts account\r\n1       1         2      01          2      01\r\n2       1         2      01          2      02\r\n3       1         2      02          1      03\r\n4       2         9      03          4      04\r\n5       2         9      03          4      05\r\n6       2         9      03          4      06\r\n7       2         9      03          4      07\r\n8       2         9      04          2      08\r\n9       2         9      04          2      09\r\n10      2         9      05          2      10\r\n\r\nLink levels. We can create 15 clients with their birth year and join year and some correlation between both variables.\r\n\r\n\r\ndf <- fabricate(\r\n  age = add_level(N=51, birth_year=1950:2000),\r\n  tenure = add_level(N = 20, join_year=1991:2010, nest = FALSE),\r\n  client = link_levels(N = 15, by = join(age, tenure, rho = 0.7))\r\n)\r\n\r\ndf %>% select(client,birth_year,join_year)\r\n\r\n\r\n   client birth_year join_year\r\n1      01       1971      1998\r\n2      02       1999      2009\r\n3      03       1989      2006\r\n4      04       1973      2008\r\n5      05       1978      1996\r\n6      06       1983      2001\r\n7      07       1960      1996\r\n8      08       1978      1997\r\n9      09       1975      1997\r\n10     10       1984      2002\r\n11     11       1997      2008\r\n12     12       1991      2002\r\n13     13       1963      1999\r\n14     14       1996      2009\r\n15     15       1984      2001\r\n\r\nOrdered data\r\nfabricatr has an amazing function to create ordered categorical data.\r\nThe function we need is draw_ordered. It internally simulates a numeric variable (x) and breaks them into predefined categories.\r\n\r\n\r\ndraw_ordered(\r\n  x = rnorm(10),\r\n  breaks = c(-2,-1,0.8,2),\r\n  break_labels = c(\"Very boring\",\"Boring\",\"OK\",\"Interesting\",\"Very Interesting\")\r\n)\r\n\r\n\r\n [1] OK          Boring      OK          Interesting OK         \r\n [6] Boring      OK          OK          Interesting Interesting\r\nLevels: Very boring Boring OK Interesting Very Interesting\r\n\r\nLet’s take a look at another example where we have two types of clients, gold clients that receive a yearly gift from the bank and standard clients that do not. How could we simulate their responses to a satisfaction survey?\r\n\r\n\r\ndf <- fabricate(\r\n  N = 100,\r\n  gold_client_flag = draw_binary(prob = 0.3, N),\r\n  satisfaction = draw_ordered(\r\n    x = rnorm(N, mean = -0.4 + 1.2 * gold_client_flag),\r\n    breaks = c(-1.5, -0.5, 0.5, 1.5),\r\n    break_labels = c(\"Very Unsatisfied\", \"Unsatisfied\", \"Neutral\",\r\n                     \"Satisfied\", \"Very Satisfied\")\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n   ID gold_client_flag     satisfaction\r\n1 001                0 Very Unsatisfied\r\n2 002                0          Neutral\r\n3 003                0          Neutral\r\n4 004                0        Satisfied\r\n5 005                0      Unsatisfied\r\n6 006                0          Neutral\r\n\r\nWe can summarize the results and see the differences between the two groups. Ideal data for teaching hypothesis testing.\r\n\r\n\r\ndf %>% count(gold_client_flag,satisfaction) %>%\r\n  tidyr::pivot_wider(id_cols=satisfaction,names_from=\"gold_client_flag\",values_from=\"n\")\r\n\r\n\r\n# A tibble: 5 x 3\r\n  satisfaction       `0`   `1`\r\n  <fct>            <int> <int>\r\n1 Very Unsatisfied    12    NA\r\n2 Unsatisfied         22     2\r\n3 Neutral             29     9\r\n4 Satisfied           11     9\r\n5 Very Satisfied       1     5\r\n\r\nTime series\r\nExample from this article.\r\nThis example contains the GDP of five countries over the course of five years.\r\n\r\n\r\npanel_units <- fabricate(\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.8),\r\n    growth_error = runif(N, 0.1, 0.5)\r\n  ),\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    gdp_measure = base_gdp + (ts_year * growth_units) + rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\nhead(panel_units,10)\r\n\r\n\r\n   countries base_gdp growth_units growth_error years ts_year\r\n1          1 15.81650    0.2936397    0.2843482    01       0\r\n2          1 15.81650    0.2936397    0.2843482    02       1\r\n3          1 15.81650    0.2936397    0.2843482    03       2\r\n4          1 15.81650    0.2936397    0.2843482    04       3\r\n5          1 15.81650    0.2936397    0.2843482    05       4\r\n6          2 15.25248    0.5143369    0.4467515    06       0\r\n7          2 15.25248    0.5143369    0.4467515    07       1\r\n8          2 15.25248    0.5143369    0.4467515    08       2\r\n9          2 15.25248    0.5143369    0.4467515    09       3\r\n10         2 15.25248    0.5143369    0.4467515    10       4\r\n   gdp_measure\r\n1     15.54446\r\n2     16.25074\r\n3     16.26999\r\n4     16.50294\r\n5     17.46510\r\n6     15.13692\r\n7     16.36070\r\n8     16.30388\r\n9     16.94710\r\n10    17.80540\r\n\r\n\r\n\r\n\r\nWe can take this to the next level and introduce some year specific information and then cross this with the country specific information. We just have to add one layer.\r\n\r\n\r\npanel_global_data <- fabricate(\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    year_shock = rnorm(N, 0, 0.5) #each year has a global trend\r\n  ),\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.5), \r\n    growth_error = runif(N, 0.1, 0.5),\r\n    nest = FALSE\r\n  ),\r\n  country_years = cross_levels(\r\n    by = join(years, countries),\r\n    gdp_measure = base_gdp + year_shock + (ts_year * growth_units) +\r\n      rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nGenOrd\r\nThis package helps to create discrete random variables with prescribed correlation matrix and marginal distributions.\r\n\r\n\r\nlibrary(GenOrd)\r\n\r\n\r\nk <- 4 #number of random variables\r\nmarginal <- list(0.6, c(1/3,2/3), c(1/4,2/4,3/4), c(1/5,2/5,3/5,4/5))\r\n\r\n\r\n\r\nRead the list as follows:\r\nWe will create 4 random variables.\r\nThe first variable will have two values: 60% of the data will be 1, 40% will be 2.\r\nThe second variable will have three values, 1,2 and 3 with a probability of 33% each.\r\netc…\r\nEach vector in this list refers to one variable, and we will see the cumulative probability for each value.\r\n\r\n\r\ncorrcheck(marginal)\r\n\r\n\r\n[[1]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n           [,1]       [,2]       [,3]       [,4]\r\n[1,]  1.0000000 -0.8333333 -0.8215838 -0.8660254\r\n[2,] -0.8333333  1.0000000 -0.9128709 -0.9237604\r\n[3,] -0.8215838 -0.9128709  1.0000000 -0.9486833\r\n[4,] -0.8660254 -0.9237604 -0.9486833  1.0000000\r\n\r\n[[2]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n          [,1]      [,2]      [,3]      [,4]\r\n[1,] 1.0000000 0.8333333 0.8215838 0.8660254\r\n[2,] 0.8333333 1.0000000 0.9128709 0.9237604\r\n[3,] 0.8215838 0.9128709 1.0000000 0.9486833\r\n[4,] 0.8660254 0.9237604 0.9486833 1.0000000\r\n\r\nThis function shows what are allowable ranges for the correlation matrix, given the input from the marginal distributions.\r\n\r\n\r\nSigma <- matrix(c(1,0.5,0.4,0.3,\r\n                  0.5,1,0.5,0.4,\r\n                  0.4,0.5,1,0.5,\r\n                  0.3,0.4,0.5,1),\r\n                k, k, byrow=TRUE)\r\n\r\n\r\n\r\nWe will create 1000 observations, with the given correlation matrix. Each variable will have the marginal distribution described above.\r\n\r\n\r\nn <- 1000 # sample size\r\nm <- ordsample(n, marginal, Sigma)\r\n\r\ndf <- data.frame(m)\r\nhead(df)\r\n\r\n\r\n  X1 X2 X3 X4\r\n1  1  1  4  1\r\n2  2  3  4  5\r\n3  1  2  2  3\r\n4  2  3  3  4\r\n5  2  3  3  5\r\n6  2  3  4  5\r\n\r\nLet’s verify that the data is actually what we expected. We check the correlation and the marginal distribution for two of the variables.\r\n\r\n\r\ncor(df)\r\n\r\n\r\n          X1        X2        X3        X4\r\nX1 1.0000000 0.4894198 0.3312862 0.2572479\r\nX2 0.4894198 1.0000000 0.4567171 0.3850669\r\nX3 0.3312862 0.4567171 1.0000000 0.4893278\r\nX4 0.2572479 0.3850669 0.4893278 1.0000000\r\n\r\ndf %>% count(X4)\r\n\r\n\r\n  X4   n\r\n1  1 182\r\n2  2 193\r\n3  3 218\r\n4  4 212\r\n5  5 195\r\n\r\ndf %>% count(X1)\r\n\r\n\r\n  X1   n\r\n1  1 584\r\n2  2 416\r\n\r\nLater we can rename the columns and values, but will have assured that they have the desired correlations.\r\nMore packages\r\nIn this blogpost by Joseph Rickert on R Views.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-03-useful-packages-for-data-composition/useful-packages-for-data-composition_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2021-01-19T16:13:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-13-data-composition-with-rmultinom/",
    "title": "Data simulation with rmultinom",
    "description": "When creating several datasets that depend on each other, the rmultinom function from the stats package can be a useful helper. In this example we will see how to create customer transactions from a customer table.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-11-13",
    "categories": [
      "simulation",
      "rmultinom"
    ],
    "contents": "\r\n\r\nContents\r\nrmultinom - like a game setup\r\nAn application of rmultinomAutomate this for all customers\r\n\r\nOther possible applications\r\n\r\nIn this article I want to show how the rmultinom() function can help to simulate data. We will simulate client data, and for each client we will create transactions.\r\nThe rmultinom() function simulates the multinomial distribution (Link).\r\nrmultinom - like a game setup\r\nIn my head I always picture the multinomial distribution as a game setup. You have N balls and K bins. Instead of the number of bins, we send a vector of probabilities (of length K), how likely it is for the balls to land in each bin (you can imagine that some bins are closer and others are further away, or that some are larger than others). This vector will be normalized automatically, so you do not have to worry about this.\r\nLet’s try an example, with N=1000 and K=5. We want one of the bins to be twice as large as the others.\r\n\r\n\r\ntest1 <- rmultinom(n=1,size=1000,c(2,1,1,1,1))\r\n\r\n\r\n\r\n\r\n\r\n\r\nAn application of rmultinom\r\nHow can we use this function to create transactions for a given number of customers? The key is to simulate all important values on client level and use rmultinom to decompose the values into smaller portions. First, let’s get some clients.\r\n\r\n\r\nset.seed(61)\r\n\r\nage <- rnorm(10,mean=50,sd=15) %>% pmax(18) %>% round()\r\ntenure <- (age - 18 - runif(10,1,30)) %>% pmax(0) %>% round()\r\nincome <- rexp(10,0.0001) %>% round(2)\r\n\r\nclient <- data.frame(id=1:10,age,tenure,income)\r\n\r\nclient\r\n\r\n\r\n   id age tenure   income\r\n1   1  44     22 32181.33\r\n2   2  44      7  5454.56\r\n3   3  24      3  4559.78\r\n4   4  55     34 26790.03\r\n5   5  29      0 18592.27\r\n6   6  47     19 40229.93\r\n7   7  61     14  1065.49\r\n8   8  58     17  2750.75\r\n9   9  71     49 12292.53\r\n10 10  45     15   282.05\r\n\r\nFor this exercise, we do not distinguish between different types of transactions. In practice, it would make sense to separate rent, supermarket, transport and other categories.\r\nWe create a second dataframe for clients, which contains “invisible” information needed for the transactions. Let’s begin with the total spending. This can depend on anything we know about the client. In this case, we will assume that each client has more or less the same behavior and spends around 70% of their income. The standard deviation of 0.1 assures that this value varies from client to client.\r\n\r\n\r\ncl_secret_info <- client\r\n\r\ncl_secret_info$total_spend <- (cl_secret_info$income * rnorm(10,0.7,sd=0.1)) %>% round(2)\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend\r\n1   1  44     22 32181.33    20755.16\r\n2   2  44      7  5454.56     4342.34\r\n3   3  24      3  4559.78     3280.65\r\n4   4  55     34 26790.03    21012.47\r\n5   5  29      0 18592.27    13990.48\r\n6   6  47     19 40229.93    25986.32\r\n7   7  61     14  1065.49      699.14\r\n8   8  58     17  2750.75     2158.39\r\n9   9  71     49 12292.53    10135.33\r\n10 10  45     15   282.05      160.08\r\n\r\nThe next ingredient is the number of transactions. For this example, we create a formula depending on age: Clients which are younger than 50 have (on average) a higher number of transactions per month.\r\n\r\n\r\ncl_secret_info$n_trans <- ifelse(cl_secret_info$age < 50, rbinom(10,60,0.5),rbinom(10,60,0.3))\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans\r\n1   1  44     22 32181.33    20755.16      27\r\n2   2  44      7  5454.56     4342.34      37\r\n3   3  24      3  4559.78     3280.65      28\r\n4   4  55     34 26790.03    21012.47      25\r\n5   5  29      0 18592.27    13990.48      32\r\n6   6  47     19 40229.93    25986.32      27\r\n7   7  61     14  1065.49      699.14      11\r\n8   8  58     17  2750.75     2158.39      14\r\n9   9  71     49 12292.53    10135.33      16\r\n10 10  45     15   282.05      160.08      27\r\n\r\nNow we already know that our transaction table will have 244 rows, the sum of all our 10 clients’ transactions.\r\nWe will create a last parameter which is an indicator of how similar the transactions are. You could split $100 into one large transaction of $80 and four small transactions of $5 each or you could have five transactions of around $20 each. The higher the value of diff_trans the higher the variability within the transactions of a client.\r\n\r\n\r\ncl_secret_info$diff_trans <- rexp(10,100/cl_secret_info$total_spend) %>% ceiling()\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans diff_trans\r\n1   1  44     22 32181.33    20755.16      27        328\r\n2   2  44      7  5454.56     4342.34      37         55\r\n3   3  24      3  4559.78     3280.65      28         38\r\n4   4  55     34 26790.03    21012.47      25         65\r\n5   5  29      0 18592.27    13990.48      32          1\r\n6   6  47     19 40229.93    25986.32      27       1408\r\n7   7  61     14  1065.49      699.14      11          1\r\n8   8  58     17  2750.75     2158.39      14         47\r\n9   9  71     49 12292.53    10135.33      16        183\r\n10 10  45     15   282.05      160.08      27          2\r\n\r\nNow we have all the necessary ingredients to split total_spend into n_trans transactions for each client. And this is the moment where the rmultinom function is extremely helpful. Let’s take a look at the first client, who spends $20755.16 in 27 transactions. The high diff_trans value indicates that there will likely be some very high transaction values and some very low.\r\nBefore doing the rmultinom magic, we will create the vector with the bins first. Remember that this vector determines how “large” each bin is or how likely it is to\r\n\r\n\r\nbins <- runif(cl_secret_info$n_trans[1],min=1,max=cl_secret_info$diff_trans[1])\r\n\r\ntransactions1 <- rmultinom(1,cl_secret_info$total_spend[1],bins)\r\n\r\ndf <- data.frame(client_id=1, trans_id=1:cl_secret_info$n_trans[1],value=transactions1)\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[880,1240,1364,1262,158,1291,1182,86,1455,1354,288,831,738,8,1209,164,594,822,280,1093,321,297,211,1276,9,1375,967]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>client_id<\\/th>\\n      <th>trans_id<\\/th>\\n      <th>value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nAutomate this for all customers\r\nIn order to efficiently do this for all customers we will put what we just did in a function.\r\n\r\n\r\ncreate_transactions <- function(i) {\r\n  bins <- runif(cl_secret_info$n_trans[i],min=1,max=cl_secret_info$diff_trans[i])\r\n\r\n  transactions <- rmultinom(1,cl_secret_info$total_spend[i],bins)\r\n\r\n  df <- data.frame(client_id=i, trans_id=1:cl_secret_info$n_trans[i],value=transactions)\r\n  \r\n  return(df)\r\n}\r\n\r\n\r\n\r\nWe call this function repeatedly with lapply.\r\n\r\n\r\ntrans_list <- lapply(seq_along(client$id),create_transactions)\r\n\r\n\r\n\r\nFinally, we bind all the transactions from all clients together in our final dataframe.\r\n\r\n\r\ntrans_df <- do.call(rbind,trans_list)\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,6,7,8,9,10,11,12,13,14,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[573,1367,688,250,228,521,873,1136,533,374,1118,399,482,473,1265,536,1237,701,585,1041,1311,1408,617,1036,1298,381,324,115,51,206,40,184,69,73,150,89,111,34,165,188,128,216,157,119,61,139,204,6,46,188,141,139,131,225,73,140,60,145,182,135,9,91,94,38,148,52,262,91,26,116,164,24,10,36,157,247,225,24,186,167,50,58,14,40,116,186,180,185,106,138,211,61,1513,360,286,160,467,1165,1553,1520,218,916,443,783,1549,153,224,455,327,1545,813,1282,647,1480,679,1144,1330,427,418,473,438,420,464,460,442,431,424,450,432,408,416,445,427,419,444,437,432,434,433,457,421,438,439,427,475,416,459,462,422,1027,1425,355,63,1355,1845,329,530,1729,998,1150,356,53,1370,1290,248,1692,854,1802,19,1562,1244,663,1533,770,797,927,63,60,56,76,65,65,73,59,69,55,58,178,262,110,49,154,233,29,258,251,32,228,231,109,34,366,835,517,464,1270,368,100,1232,210,701,617,211,583,1190,179,1292,7,7,2,2,4,5,7,6,9,1,5,3,6,5,8,6,8,4,12,5,8,8,11,4,8,5,4]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>client_id<\\/th>\\n      <th>trans_id<\\/th>\\n      <th>value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nOther possible applications\r\nStudents and grades (it is easier if the grades are points and you have a total number of points to reach). You might want to check the package {wakefield} to create sequences of grades / tests etc.\r\nProducts and sales numbers in supermarkets.\r\nAnimals and tracked kilometers.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-13-data-composition-with-rmultinom/data-composition-with-rmultinom_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-19T16:08:46+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-01-simulate-dependent-variables/",
    "title": "Simulate dependent variables",
    "description": "When you simulate a dataset it is often not enough to have independent variables, but you  want to have some dependency between the variables. In this post we explore ways of creating this dependency.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-11-01",
    "categories": [
      "simulation",
      "correlation"
    ],
    "contents": "\r\n\r\nContents\r\nPackages\r\nSimulating dependent variablesRule based\r\nCorrelation based\r\n\r\nClosing comments\r\n\r\nPackages\r\nMost of the functions that we are using here are actually part of base R.\r\nWe will need some functions from the {dplyr} and {ggplot2} packages for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nSimulating dependent variables\r\nStorytelling with data is an important skill for anyone who is analyzing data. You try to find interesting information in data and then think of how to convey these insights to others.\r\nIn the previous post we saw how to simulate independent variables. Instead of finding stories in the data we now want to “hide” stories for others to find or to show how a certain analytics / visualization / data wrangling technique works.\r\nAn example: when we look at bank clients, we would expect older clients to have (on average!) a higher balance than younger clients.\r\nIn this section we are going to have a look at techniques to create dependence between variables.\r\nRule based\r\nWe can use ifelse() and case_when() from the {dplyr} package to create new variables that depend on others. Let’s make a small example with two columns: married, which indicates if the person is married, and age.\r\nWe will simulate 1000 clients, around 50% of which are married.\r\n\r\n\r\nk <- 1000\r\nmarried <- sample(c(\"Y\",\"N\"),k,replace=T)\r\n\r\n\r\n\r\nNext, we want that our married clients are slightly older than our non-married clients. For this example we assume that the average age of the married clients is 40, and the average age of the non-married clients is 30.\r\n\r\n\r\ndata <- data.frame(id=1:k,married) %>% \r\n  mutate(\r\n    age=ifelse(married==\"Y\", rnorm(k, 40, sd = 10), rnorm(k, 30, sd= 12)) %>% \r\n      pmax(18) %>% #every client should be at least 18\r\n      round()\r\n    )\r\n\r\n\r\n\r\n\r\n  id married age\r\n1  1       Y  39\r\n2  2       N  33\r\n3  3       N  29\r\n4  4       Y  44\r\n5  5       N  28\r\n6  6       Y  36\r\n\r\nWe can take a quick look if the difference is visible in a boxplot.\r\n\r\n\r\n\r\nIf you have more than two options, case_when() can help. We want to see the balance of clients which are either managers, analysts or senior analysts.\r\n\r\n\r\nk <- 1000\r\n\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\n\r\ndata <- data.frame(id=1:k,ocupation)\r\n\r\ndata <- data %>% mutate(balance=case_when(\r\n  ocupation==\"analyst\" ~ 100+rexp(k,0.01),\r\n  ocupation==\"sr analyst\" ~ 200+rexp(k,0.005),\r\n  TRUE ~ 200+rexp(k,0.001) #this is the else case\r\n))\r\n\r\n#Check the average balance per group\r\ndata %>% \r\n  ggplot(aes(x=ocupation,y=balance))+geom_violin()\r\n\r\n\r\n\r\n\r\nCorrelation based\r\nIf we just deal with numeric variables and want to have a slightly more complex connection between the variables, we can try another approach, for which we specify a correlation matrix beforehand and reorder our variables afterwards so that they match the desired correlation.\r\nOf course, we need to find reasonable correlation values, for example between age and number of kids (probably slightly positively correlated) or between savings and number of kids (probably slightly negatively correlated). This requires some research.\r\nFirst, we simulate the data independently. Ideas about how to do this can be found in the previous blogpost.\r\n\r\n\r\nset.seed(64)\r\n\r\nk <- 2000\r\n\r\nage <- rnorm(k,mean=35,sd=10) %>% pmax(18) %>% round()\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\ntenure <- rnorm(k,mean=15,sd=5) %>% pmax(1) %>% round()\r\nkids_cnt <- sample(0:6,k,replace=T,prob=c(100,120,80,30,5,2,1))\r\n\r\n\r\ndata <- data.frame(age,balance,kids_cnt,tenure)\r\n\r\n\r\n\r\n\r\n  age balance kids_cnt tenure\r\n1  18 3665.34        2     10\r\n2  18  268.55        2      8\r\n3  22 1628.59        0     22\r\n4  50 1995.58        1     12\r\n5  35 1510.58        0     20\r\n6  32   58.58        0      5\r\n7  45  945.11        0     21\r\n\r\nWe directly see that there are things that don’t make too much sense, like the 22-years-old with a tenure of 22 years. Further, there is no dependence between the variables.\r\nTo improve this, we want to reshuffle the rows and get a correlation close to a desired one. First we simulate a helping dataset of same size, where every entry is random normally distributed.\r\n\r\n\r\n#same size\r\nnvars <- ncol(data)\r\nnumobs <- nrow(data)\r\n\r\nset.seed(3)\r\nrnorm_helper <- matrix(rnorm(nvars*numobs,0,1),nrow=nvars)\r\n\r\n\r\n\r\nThe correlation of this matrix should be close to the identity matrix.\r\n\r\n\r\ncor(t(rnorm_helper))\r\n\r\n\r\n             [,1]        [,2]        [,3]         [,4]\r\n[1,]  1.000000000 -0.00574905 0.009783835 -0.023569599\r\n[2,] -0.005749050  1.00000000 0.049500977  0.010347672\r\n[3,]  0.009783835  0.04950098 1.000000000  0.005859748\r\n[4,] -0.023569599  0.01034767 0.005859748  1.000000000\r\n\r\nNext, we specify our desired correlation matrix. Just to put this in words, we want to correlate the four variables age, balance, kids_cnt and tenure. Each variable with itself has a correlation of 1. We want age and balance to have a positive correlation of 0.3, age and kids_cnt of 0.4 and age and tenure of 0.2. Likewise, we specify all desired correlations between pairs of variables.\r\n\r\n\r\nQ <- matrix(c(1,0.3,0.4,0.2,  0.3,1,-0.3,0.3,  0.4,-0.3,1,-0.3,  0.2,0.3,-0.3,1),ncol=nvars)\r\n\r\nQ\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0 -0.3  0.3\r\n[3,]  0.4 -0.3  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nWe can now multiply the rnorm_helper matrix with the Cholesky decomposition of our desired correlation matrix Q. Why this works, is explained in the following comment. If you are not interested in mathematical details, you can skip this part.\r\n\r\n(Explanation found here)\r\n\r\n\r\nL <- t(chol(Q))\r\nZ <- L %*% rnorm_helper\r\n\r\n\r\n\r\nGood, now we convert this new data to a data frame and name it like our original data.\r\n\r\n\r\nraw <- as.data.frame(t(Z),row.names=NULL,optional=FALSE)\r\nnames(raw) <- names(data)\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n         age     balance    kids_cnt     tenure\r\n1 -0.9619334 -0.56763178 -0.04795672 -1.3731410\r\n2  0.1957828  0.08747126  0.13371210  1.0071453\r\n3 -1.2188574  0.84333548 -1.64422257 -0.6774232\r\n4 -0.7163585  0.02610745 -0.27556113 -0.4031339\r\n5 -0.9530173 -0.90428943  0.88834971 -0.5867759\r\n6 -0.5784837 -1.07244273  0.01971809 -1.7605861\r\n7 -0.4844551 -0.85227479  1.06544301  0.2243580\r\n\r\nThe correlation of this dataset is close to our desired outcome.\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance   kids_cnt     tenure\r\nage      1.0000000  0.2937627  0.4053011  0.1707997\r\nbalance  0.2937627  1.0000000 -0.2590168  0.2802119\r\nkids_cnt 0.4053011 -0.2590168  1.0000000 -0.3038721\r\ntenure   0.1707997  0.2802119 -0.3038721  1.0000000\r\n\r\nHowever, this dataset raw does not have anything to do with our original data. It is still only transformed random normal data. But as we know that this dataset has the correct correlation, we can use this to reorder the rows of our other dataset.\r\nAnd then we just replace the largest value of the random normal dataset with the largest value in our dataset, the second largest with the second largest etc. We go column by column and repeat this procedure.\r\n\r\n\r\nfor(name in names(raw)) {\r\n  raw <- raw[order(raw[,name]),]\r\n  data <- data[order(data[,name]),]\r\n  raw[,name] <- data[,name]\r\n}\r\n\r\n\r\n\r\nLet’s check the correlation of this new dataset. It is close to our desired correlation matrix Q. The main reason for the small difference is that our variables take less values than a random normal distributed variable (e.g. kids count just takes values between 0 and 6).\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance   kids_cnt     tenure\r\nage      1.0000000  0.2423117  0.3777257  0.1743281\r\nbalance  0.2423117  1.0000000 -0.2010457  0.2390589\r\nkids_cnt 0.3777257 -0.2010457  1.0000000 -0.2803064\r\ntenure   0.1743281  0.2390589 -0.2803064  1.0000000\r\n\r\n\r\nTo compare: This was Q:\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0 -0.3  0.3\r\n[3,]  0.4 -0.3  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nOur final reshuffled and correctly correlated dataset is now stored in raw.\r\n\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n     age balance kids_cnt tenure\r\n1934  34   36.36        4      1\r\n733   37   52.44        2      1\r\n123   22  290.91        2      1\r\n1032  26  130.01        2      1\r\n1463  32   88.87        2      1\r\n448   43   26.54        5      1\r\n1804  35  911.63        1      2\r\n\r\nClosing comments\r\nIf you like the correlation method please take a look at the GenOrd package which is a little more professional, when working with ordinal categorical variables.\r\nThe Cholesky decomposition is only possible for positive definite matrices. If this is not the case and you accept a slightly stronger deviation from your desired correlation matrix, the easiest way is to add 0.1, 0.2 etc. to the diagonals until you obtain a positive definite matrix. Note that this lowers the correlation between all variables.\r\n\r\n\r\ndiag(nvars) * 0.1 + Q\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.1  0.3  0.4  0.2\r\n[2,]  0.3  1.1 -0.3  0.3\r\n[3,]  0.4 -0.3  1.1 -0.3\r\n[4,]  0.2  0.3 -0.3  1.1\r\n\r\nAfter the correlation process it might be helpful to check some of your data manually to see if the observation make sense and - if needed - perform manual corrections.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-01-simulate-dependent-variables/simulate-dependent-variables_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-01-23T10:23:14+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-data-simulation/",
    "title": "Simulate variables and data",
    "description": "The purpose of this post is to enable readers to create data from scratch which they can use for their analyses or visualizations.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "simulation",
      "distributions"
    ],
    "contents": "\r\n\r\nContents\r\nPackages\r\nData simulation\r\nManual values\r\nCategorical variables with sample()\r\nNumerical variables\r\nDistributions\r\nCombining variables in a dataframe\r\n\r\nPackages\r\nMost of the functions that we are using here are part of base R.\r\nWe will need some functions from the {dplyr} and {ggplot2} packages for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nData simulation\r\nIn this post we will learn how to simulate data like this:\r\n\r\n  client_id    name age  ocupation balance married_flg\r\n1         1   Frank  26 sr analyst  245.96          No\r\n2         2  Dorian  26    analyst 2273.39          No\r\n3         3     Eva  18    manager 2270.47          No\r\n4         4   Elena  34    analyst  373.45          No\r\n5         5    Andy  18    analyst  961.21         Yes\r\n6         6 Barbara  28    analyst   69.32         Yes\r\n7         7  Yvonne  37    analyst 3218.13         Yes\r\n\r\nImportant to make your data creation reproducible (i.e. if you run it again, it yields the same result) is the set.seed() function. As we are creating instances of random variables we assure with this function that every time the same sequence of random variables is generated. You can use any number you like inside this function.\r\n\r\n\r\nset.seed(64)\r\n\r\n\r\n\r\nManual values\r\nLet’s start with the most simple but most time-consuming way. Type everything manually and save it in a vector:\r\n\r\n\r\nclient_gen <- c(\"Millenial\",\"Gen X\",\"Millenial\",\r\n                \"Baby Boomer\",\"Gen X\",\"Millenial\",\"Gen X\")\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1   Millenial\r\n2  2       Gen X\r\n3  3   Millenial\r\n4  4 Baby Boomer\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nCategorical variables with sample()\r\nFor categorical variables, we can save some time using the sample function. You specify first the possible values and then how many of these values you would like to pick. If you want to allow values to be picked more than once, make sure to set replace=TRUE.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"),7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1 Baby Boomer\r\n2  2   Millenial\r\n3  3   Millenial\r\n4  4   Millenial\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nThe sample function is quite flexible and we can tweak the prob parameter, for example to say that we want (approximately) half of the population to be Baby Boomers. The effect will be visible if we produce larger amounts of data.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"), 1000, replace=TRUE, prob=c(0.25,0.25,0.5))\r\n\r\nqplot(client_gen)\r\n\r\n\r\n\r\n\r\nNumerical variables\r\nThe same sample() function works with numbers.\r\n\r\n\r\nclient_age <- sample(1:100,size=7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1          4\r\n2  2         42\r\n3  3         33\r\n4  4         62\r\n5  5         76\r\n6  6         65\r\n7  7         81\r\n\r\nIn both cases above, each number had the same probability of being selected. If we would like some numbers to be more likely to be selected, we can specify this with prob.\r\nThe probability values will be automatically scaled to 1. If I would like to have 50% of the population to have the age of 27, I can specify the weight. (Note: rep(1,5) is equivalent to c(1,1,1,1,1), replicating the number 1 five times.)\r\n\r\n\r\nclient_age <- sample(1:100,size=1000,replace=TRUE,prob=c(rep(1,26),99,rep(1,73)))\r\n\r\nqplot(client_age==27)\r\n\r\n\r\n\r\n\r\nDistributions\r\nIf you would like to work with probability distributions to create numerical variable, this is also very easy with the base functions of type r+(starting letters of the distribution).\r\nLet’s try the uniform distribution:\r\n\r\n\r\nclient_age <- runif(7,min=1,max=100)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1   55.10342\r\n2  2   85.19588\r\n3  3   86.47791\r\n4  4   73.91516\r\n5  5   48.15197\r\n6  6   32.90848\r\n7  7   58.76874\r\n\r\nAs we are simulating ages, we are not interested in decimal values. We can use the round() function to round each number to the next integer.\r\n\r\n\r\nrunif(10000,1,100) %>% round() %>% head(10)\r\n\r\n\r\n [1] 93 26 28 29 90 81 95  3 21 62\r\n\r\nBut uniformly distributed variables are not always what we want. In the example above we simulated 10,000 clients and distributes their ages uniformly. For most applications it would be unrealistic that there are as many 99 year old clients as there are 50 year old clients.\r\nBut we can easily access a whole list of other distribution functions, like the famous Normal distribution (with mean and standard deviation as parameters).\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf we want to limit the values to not be smaller than 0 or larger than 100, we can use pmin and pmax.\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% pmax(0) %>% pmin(100) %>% qplot()\r\n\r\n\r\n\r\n\r\nFor many applications (like balance distribution or any data that contains outliers) I like to use the Exponential distribution (with parameter rate and expectation 1/rate).\r\n\r\n\r\nrexp(10000,rate=0.01) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf you want to explore further probability distributions check out this link. Playing around with the parameters of the distributions you will notice that you can simulate almost any variable you like (Take a short look at: The different faces of the Beta distribution).\r\n\r\n\r\n\r\nCombining variables in a dataframe\r\nTo create our first simulated dataframe, we can start by simulating the variables separately and then putting them together.\r\n\r\n\r\nset.seed(61)\r\n\r\nk <- 7\r\n\r\nid <- 1:k\r\nname <- c(\"Frank\",\"Dorian\",\"Eva\",\"Elena\",\"Andy\",\"Barbara\",\"Yvonne\")\r\nage <- rnorm(k,mean=30,sd=10) %>% pmax(18) %>% round()\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\nmarried <- sample(c(\"Yes\",\"No\"),k,replace=T,prob=c(0.6,0.4))\r\n\r\ndata <- data.frame(client_id=id,name,age,ocupation,balance,married_flg=married)\r\ndata\r\n\r\n\r\n  client_id    name age  ocupation balance married_flg\r\n1         1   Frank  26 sr analyst  245.96          No\r\n2         2  Dorian  26    analyst 2273.39          No\r\n3         3     Eva  18    manager 2270.47          No\r\n4         4   Elena  34    analyst  373.45          No\r\n5         5    Andy  18    analyst  961.21         Yes\r\n6         6 Barbara  28    analyst   69.32         Yes\r\n7         7  Yvonne  37    analyst 3218.13         Yes\r\n\r\nGreat! We just simulated a dataset which we can use now for visualization or modeling purposes.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-09-data-simulation/data-simulation_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-01-02T13:05:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-fuzzy-merging/",
    "title": "Fuzzy matching example with company names",
    "description": "Whenever you have text data that was input manually by a human, there is a chance that it contains errors: Typos, abbreviations or different ways of writing can be challenges for your analysis. Fuzzy matching is a way to find inexact matches that mean the same thing like mcdonalds, McDonalds and McDonald's Company.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "text data",
      "fuzzy matching",
      "stringdist"
    ],
    "contents": "\r\n\r\nContents\r\nPackages\r\nThe data\r\nMain process\r\nResults\r\nNext Steps and other resources\r\n\r\nPackages\r\nThe only packages you need are dplyr and stringdist.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(stringdist)\r\n\r\n\r\n\r\nThe data\r\nThis method requires as input two lists. To distinguish them, we will call the one that contains the handtyped input as the “dirty list”. The reference list will be called the “clean list”. In this blogpost I will create the dirty list by hand with a few made-up examples of alternative company names.\r\n\r\n\r\nnames <- c(\"Haliburton\", \"ExxonMobile\",\"ABBOTT LABORATORIES\",\"Marrriott\",\"Self\",\"Activision Blizzard\",\r\n           \"Quest dianotstics\",\"Unemployed\",\"other company\",\"burger king\",\r\n           \"MARRIOT\",\"wall mart\", \"Illumin\", \"3M\",\"NORTHROP TRUMMON\",\"MCCormicks\",\"MARSH MCLEANNON\",\r\n           \"FLO SERVE\", \"Kansas City Southern Fed.\",\"MCDONALD'S\",\"F5 Networks\",\r\n           \"McDonalds\",\"MacKindsey\",\"Oracle\",\"Self-employed\",\"None\",\"Retired\",\r\n           \"f5 networks\",\"Harley Davidson\",\"Harly Davidson\",\"HARLEY DAVIDSEN\",\"DRHorton\",\"D.R. Horten\",\r\n           \"cincinati fin\",\"cincinnatti financials\",\"cincinnati financial\",\"CINCINATTI FINANCE\",\r\n           \"Mohaws Industry\",\"Mowahk Industries\",\"Mohawk Ind\")\r\n\r\nset.seed(64)\r\ndirty_list <- sample(names,50000,replace=T)\r\n\r\n\r\n\r\n\r\ndirty_list\r\nHaliburton\r\nExxonMobile\r\nABBOTT LABORATORIES\r\nMarrriott\r\nSelf\r\nActivision Blizzard\r\nQuest dianotstics\r\nUnemployed\r\nother company\r\nburger king\r\n\r\nAs a clean list we will use the list of S&P500 companies. This can be downloaded or scraped from the internet.\r\n\r\nclean_list\r\n3M Company\r\nAbbott Laboratories\r\nAbbVie Inc.\r\nABIOMED Inc\r\nAccenture plc\r\nActivision Blizzard\r\nAdobe Systems Inc\r\nAdvanced Micro Devices Inc\r\nAdvance Auto Parts\r\nAES Corp\r\n\r\nBefore we start, we will pre-process both lists, remove some common words and transform everything to lower case. If you prefer, you can also use the {stringr} package for this. One comment from my experience: Usually, the construction of the common words to remove is an iterative approach: You would check your final result and see which words are still causing problems. Then you add them to the cleaner function and run the process again until you are satisfied with the results.\r\n\r\n\r\ncleaner <- function(vec) {\r\n  wordremove <- c(\" and \",\" comp \",\" company\",\"companies\",\" corp \",\"corporation\",\" inc \",\"[.]com\")\r\n  output <- vec %>% tolower() %>% \r\n    {gsub(paste(wordremove,collapse='|'),\"\",.)} %>%\r\n    {gsub(\"[[:punct:]]\",\"\",.)} %>%\r\n    {gsub(\"[[:blank:]]\",\"\",.)}\r\n  return(output)\r\n}\r\n\r\ncontrol <- data.frame(original=dirty_list)\r\n\r\nclean_list_cl <- cleaner(clean_list)\r\ndirty_list_cl <- cleaner(dirty_list)\r\n\r\n\r\n\r\nMain process\r\nWe calculate a matrix of string distances. The {stringdist} package has a lot of different methods implemented which can be checked here. After comparing some of the methods I decided to go with the Jaro-Winkler distance as it yields higher similarity for words which start with the same letters.\r\nExample\r\n\r\n\r\nstringdistmatrix(c(\"other\",\"words\",\"otherexample\",\"exapmle\"),\r\n                 c(\"example\",\"other example\",\"word\"),\r\n                 method='jw',p=0.1,useNames=\"strings\")\r\n\r\n\r\n                example other example      word\r\nother        1.00000000    0.12307692 0.5166667\r\nwords        1.00000000    0.48205128 0.0400000\r\notherexample 0.28174603    0.01538462 0.4444444\r\nexapmle      0.03333333    0.55799756 1.0000000\r\n\r\nEach row of the matrix of string distances is one string from the dirty list. We find the minimum in each row, which is equivalent to the best fit from the clean list.\r\n\r\n\r\ndistmatrix <- stringdist::stringdistmatrix(dirty_list_cl,clean_list_cl,method='jw',p=0.1)\r\nbest_fit <- apply(distmatrix,1,which.min) %>% as.integer()\r\nsimilarity <- apply(distmatrix,1,min)\r\n\r\ncontrol$best_fit <- clean_list[best_fit]\r\ncontrol$distance <- round(similarity,3)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nRetired\r\nResMed\r\n0.203\r\nSelf\r\nSealed Air\r\n0.244\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nSelf-employed\r\nTeleflex\r\n0.306\r\n\r\nResults\r\nWhen we order the control dataframe by similarity we can find a suitable cutoff value (in this example 0.12) to separate real matches from false positives. This cutoff value depends on the application.\r\n\r\n\r\ncontrol$result <- ifelse(control$distance<=0.12,control$best_fit,NA)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nresult\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nMarriott Int’l.\r\nRetired\r\nResMed\r\n0.203\r\nNA\r\nSelf\r\nSealed Air\r\n0.244\r\nNA\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nHalliburton Co.\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nIllumina Inc\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMohawk Industries\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nMarsh & McLennan\r\nSelf-employed\r\nTeleflex\r\n0.306\r\nNA\r\nMohawk Ind\r\nMohawk Industries\r\n0.088\r\nMohawk Industries\r\nMowahk Industries\r\nMohawk Industries\r\n0.017\r\nMohawk Industries\r\nHarly Davidson\r\nHarley-Davidson\r\n0.014\r\nHarley-Davidson\r\nf5 networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\n3M\r\n3M Company\r\n0.000\r\n3M Company\r\nOracle\r\nOracle Corp.\r\n0.080\r\nOracle Corp.\r\n\r\nNext Steps and other resources\r\nImprove performance for large datasets. On Github, I have an implementation of this method with the parallel package which improves performance slightly. But there is definitely more room for improvement.\r\nThere is an interesting video about performance improvement by not calculating the full matrix by Seth Verrinder and Kyle Putnam here.\r\nAndrés Cruz created an Add-in which helps to fine-tune the final result, his slide from LatinR 2019 can be found here.\r\nCheck out David Robinson’s fuzzyjoin package here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-09-fuzzy-merging/img/puzzle.png",
    "last_modified": "2021-03-20T11:56:25+01:00",
    "input_file": {}
  }
]
