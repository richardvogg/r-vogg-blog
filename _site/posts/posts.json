[
  {
    "path": "posts/2020-12-03-useful-packages-for-data-composition/",
    "title": "Useful packages for data simulation",
    "description": "We will explore the packages wakefield, rcorpora, charlatan, fabricatr, and GenOrd which can be helpful for data simulation.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-12-03",
    "categories": [
      "simulation",
      "packages"
    ],
    "contents": "\r\n\r\nContents\r\nAdditional packages\r\nwakefieldSeries\r\n\r\nrcorpora\r\ncharlatan\r\nfabricatrOrdered data\r\nTime series\r\n\r\nGenOrd\r\nMore packages\r\n\r\nWhen we simulate data we can rely on the distribution functions like rnorm, rexp and sample from base R. However, we can also leverage the great work from authors of packages which were written to make the simulation process easier. In this blogpost I will explore some of them.\r\nAdditional packages\r\nBefore starting with the simulation packages, we can load these two packages which will help with data transformation and visualization.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nwakefield\r\nLooking for interesting packages around data simulation I stumbled across the {wakefield} package by Tyler Rinker.\r\n\r\n\r\nlibrary(wakefield)\r\n\r\n\r\n\r\nIntroduction can be found here.\r\n\r\n\r\nr_data_frame(\r\n    n = 500,\r\n    id,\r\n    age,\r\n    iq,\r\n    height,\r\n    died,\r\n    animal,\r\n    internet_browser,\r\n    political\r\n)\r\n\r\n\r\n# A tibble: 500 x 8\r\n   ID      Age    IQ Height Died  Animal             Browser Political\r\n   <chr> <int> <dbl>  <dbl> <lgl> <fct>              <fct>   <fct>    \r\n 1 001      22    97     72 TRUE  Emperor Tamarin    Chrome  Republic~\r\n 2 002      85    87     71 FALSE Elephant Seal      Chrome  Democrat \r\n 3 003      75    99     66 TRUE  Chipmunk           Safari  Republic~\r\n 4 004      26   105     68 FALSE Cross River Goril~ Chrome  Democrat \r\n 5 005      88   102     74 FALSE Chipmunk           Chrome  Democrat \r\n 6 006      87   110     69 FALSE Cross River Goril~ Chrome  Democrat \r\n 7 007      26    94     65 TRUE  Reindeer           Chrome  Democrat \r\n 8 008      24    99     75 TRUE  Bedlington Terrier Chrome  Republic~\r\n 9 009      34    83     69 FALSE Whippet            Firefox Democrat \r\n10 010      39   101     71 TRUE  Emperor Tamarin    Firefox Democrat \r\n# ... with 490 more rows\r\n\r\nThere are a lot of predefined variables that you can use. (Call variables(type=\"matrix\",ncols=5) to see them.)\r\n\r\n      [,1]         [,2]          [,3]               [,4]       \r\n [1,] \"age\"        \"dice\"        \"hair\"             \"military\" \r\n [2,] \"animal\"     \"dna\"         \"height\"           \"month\"    \r\n [3,] \"answer\"     \"dob\"         \"income\"           \"name\"     \r\n [4,] \"area\"       \"dummy\"       \"internet_browser\" \"normal\"   \r\n [5,] \"car\"        \"education\"   \"iq\"               \"political\"\r\n [6,] \"children\"   \"employment\"  \"language\"         \"race\"     \r\n [7,] \"coin\"       \"eye\"         \"level\"            \"religion\" \r\n [8,] \"color\"      \"grade\"       \"likert\"           \"sat\"      \r\n [9,] \"date_stamp\" \"grade_level\" \"lorem_ipsum\"      \"sentence\" \r\n[10,] \"death\"      \"group\"       \"marital\"          \"sex\"      \r\n      [,5]           \r\n [1,] \"sex_inclusive\"\r\n [2,] \"smokes\"       \r\n [3,] \"speed\"        \r\n [4,] \"state\"        \r\n [5,] \"string\"       \r\n [6,] \"upper\"        \r\n [7,] \"valid\"        \r\n [8,] \"year\"         \r\n [9,] \"zip_code\"     \r\n[10,]                \r\nattr(,\"class\")\r\n[1] \"matrix\" \"array\" \r\n\r\nAdditionally, you can access the distribution functions easily and tweak parameters of the predefined functions.\r\n\r\n\r\ntest <- r_data_frame(\r\n    n = 500,\r\n    id,\r\n    age(x=18:50),\r\n    `Reading(mins)` = rpois(lambda=20),\r\n    income(digits=0)\r\n)\r\n\r\n\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      26              22  31899\r\n 2 002      29              26  10316\r\n 3 003      23              19  87808\r\n 4 004      39              17   8425\r\n 5 005      27              15   2519\r\n 6 006      42              24  15936\r\n 7 007      36              17  58968\r\n 8 008      30              21    977\r\n 9 009      21              16  26040\r\n10 010      35              19  19345\r\n# ... with 490 more rows\r\n\r\nLooks too perfect? Include random missing values in columns 2 and 4:\r\n\r\n\r\ntest <- test %>% r_na(cols=c(2,4),prob=0.3)\r\n\r\n\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      NA              22  31899\r\n 2 002      NA              26  10316\r\n 3 003      23              19  87808\r\n 4 004      39              17   8425\r\n 5 005      NA              15   2519\r\n 6 006      42              24  15936\r\n 7 007      36              17  58968\r\n 8 008      30              21    977\r\n 9 009      21              16  26040\r\n10 010      NA              19     NA\r\n# ... with 490 more rows\r\n\r\nSeries\r\n{wakefield} allows us to create several variables which can be seen as a sequence, for example survey results.\r\n\r\n\r\nr_series(likert,j = 5,n=10,name=\"Question\")\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Question_1    Question_2    Question_3    Question_4   Question_5  \r\n * <ord>         <ord>         <ord>         <ord>        <ord>       \r\n 1 Agree         Strongly Agr~ Strongly Agr~ Strongly Di~ Strongly Ag~\r\n 2 Disagree      Strongly Agr~ Neutral       Disagree     Strongly Ag~\r\n 3 Strongly Agr~ Strongly Dis~ Strongly Agr~ Disagree     Strongly Ag~\r\n 4 Disagree      Strongly Dis~ Neutral       Disagree     Disagree    \r\n 5 Agree         Neutral       Agree         Strongly Ag~ Strongly Di~\r\n 6 Strongly Dis~ Strongly Agr~ Neutral       Agree        Agree       \r\n 7 Disagree      Neutral       Strongly Dis~ Agree        Strongly Ag~\r\n 8 Neutral       Strongly Agr~ Agree         Strongly Di~ Disagree    \r\n 9 Neutral       Disagree      Agree         Neutral      Strongly Di~\r\n10 Neutral       Agree         Agree         Strongly Di~ Agree       \r\n\r\nThese can also be packaged inside a data frame, for example when simulating test results for students.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1 Test_2 Test_3\r\n   <chr>   <int>  <int>  <int>  <int>\r\n 1 01         13     88     89     91\r\n 2 02         22     88     88     88\r\n 3 03         16     94     84     86\r\n 4 04         12     86     87     88\r\n 5 05         15     89     80     84\r\n 6 06         14     85     88     90\r\n 7 07          9     81     89     83\r\n 8 08         17     85     93     90\r\n 9 09         17     83     88     86\r\n10 10         19     89     89     86\r\n\r\nThat is great but not very real, because the test results are completely independent from each other. The relate parameter inside the r_series function helps to connect the results, and the format is fM_sd.\r\nf is one of (+,-,*,/)\r\nM is the mean value\r\nsd is the standard deviation of the mean value\r\nExamples: * +3_1: The test results get better on average 3 points with a standard deviation of 1. * *1.05_0.2: The results get better on average 5% with a standard deviation of 0.2.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\",relate=\"+3_1\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1     Test_2     Test_3    \r\n   <chr>   <int> <variable> <variable> <variable>\r\n 1 01         14 93.5       94.5       95.5      \r\n 2 02         14 92.8       95.0       98.1      \r\n 3 03         13 92.1       95.8       99.0      \r\n 4 04         13 89.8       93.6       96.4      \r\n 5 05         11 84.4       87.8       90.9      \r\n 6 06         14 87.6       91.2       95.3      \r\n 7 07         14 85.0       88.3       94.1      \r\n 8 08         16 88.2       89.2       92.0      \r\n 9 09          9 86.4       88.9       91.5      \r\n10 10         19 90.1       91.7       95.0      \r\n\r\nWith this in mind, you can create customer balances over time very easily.\r\n\r\n\r\nbalances <- r_data_frame(\r\n  n=10,\r\n  Client=name,\r\n  age,\r\n  r_series(income,j=12,name=\"Month\",relate=\"*1.03_0.1\")\r\n)\r\n\r\n\r\n\r\nThis result is worth to be visualized.\r\n\r\n\r\nbalances %>%\r\n  tidyr::pivot_longer(-c(1,2),names_to=\"Month\") %>%\r\n  mutate(Month=readr::parse_number(Month)) %>%\r\n  ggplot(aes(x=Month,y=value))+geom_line()+facet_wrap(~Client,scales=\"free_y\")\r\n\r\n\r\n\r\n\r\nWe can see that there are customers who had very positive balance development and others whose balances were fluctuating more or declining. However, when we simulate a sufficiently large number of customers, we will observe that on average the increase each month will be the desired 3% with a standard deviation of 0.1.\r\nrcorpora\r\nCheck the github repository here.\r\nThe rcorpora library has 293 collections of words that can be very helpful for data simulation.\r\n\r\n\r\nlibrary(rcorpora)\r\n\r\nlength(corpora())\r\n\r\n\r\n[1] 293\r\n\r\ncorpora()[sample(1:293,10)]\r\n\r\n\r\n [1] \"words/stopwords/no\"               \r\n [2] \"foods/hot_peppers\"                \r\n [3] \"animals/birds_north_america\"      \r\n [4] \"humans/famousDuos\"                \r\n [5] \"foods/vegetable_cooking_times\"    \r\n [6] \"technology/video_hosting_websites\"\r\n [7] \"games/trivial_pursuit\"            \r\n [8] \"materials/layperson-metals\"       \r\n [9] \"animals/cats\"                     \r\n[10] \"objects/corpora_winners\"          \r\n\r\nTo view the words of one collection use the name in the corpora() function.\r\n\r\n\r\ncorpora(\"foods/pizzaToppings\")\r\n\r\n\r\n$description\r\n[1] \"A list of pizza toppings.\"\r\n\r\n$pizzaToppings\r\n [1] \"anchovies\"        \"artichoke\"        \"bacon\"           \r\n [4] \"breakfast bacon\"  \"Canadian bacon\"   \"cheese\"          \r\n [7] \"chicken\"          \"chili peppers\"    \"feta\"            \r\n[10] \"garlic\"           \"green peppers\"    \"grilled onions\"  \r\n[13] \"ground beef\"      \"ham\"              \"hot sauce\"       \r\n[16] \"meatballs\"        \"mushrooms\"        \"olives\"          \r\n[19] \"onions\"           \"pepperoni\"        \"pineapple\"       \r\n[22] \"sausage\"          \"spinach\"          \"sun-dried tomato\"\r\n[25] \"tomatoes\"        \r\n\r\nLet see how we can use this in a simulated dataframe.\r\n\r\n\r\ntibble(\r\n  first_name=corpora(\"humans/firstNames\")$firstNames %>% sample(100,replace=TRUE),\r\n  last_name=corpora(\"humans/lastNames\")$lastNames %>% sample(100,replace=TRUE),\r\n  self_description=corpora(\"humans/descriptions\")$descriptions %>% sample(100,replace=TRUE),\r\n  home_country=corpora(\"geography/countries\")$countries %>% sample(100,replace=TRUE),\r\n  favorite_pizza_topping=corpora(\"foods/pizzaToppings\")$pizzaToppings %>% sample(100,replace=TRUE)\r\n)\r\n\r\n\r\n# A tibble: 100 x 5\r\n   first_name last_name self_description home_country favorite_pizza_~\r\n   <chr>      <chr>     <chr>            <chr>        <chr>           \r\n 1 Fernando   King      lazy             Mongolia     grilled onions  \r\n 2 Kennedy    Rogers    prejudiced       Montenegro   ham             \r\n 3 Emmanuel   Jones     orderly          Mongolia     hot sauce       \r\n 4 Edward     Gonzalez  fussy            Guinea       ground beef     \r\n 5 Samantha   Griffin   level-headed     India        meatballs       \r\n 6 Micah      Gardner   agile            Fiji         spinach         \r\n 7 Diego      Daniels   fastidious       Denmark      cheese          \r\n 8 Natalie    Campbell  awesome          Czech Repub~ pineapple       \r\n 9 Kaylee     Myers     unsophisticated  Benin        meatballs       \r\n10 Aidan      Black     enterprising     Singapore    garlic          \r\n# ... with 90 more rows\r\n\r\ncharlatan\r\nSimilar to wakefield, charlatan has some out-of-the-box variables that can be used in your simulated data.\r\n\r\n\r\nlibrary(charlatan)\r\n\r\nch_job(n=10)\r\n\r\n\r\n [1] \"Industrial/product designer\" \"Youth worker\"               \r\n [3] \"Visual merchandiser\"         \"Cabin crew\"                 \r\n [5] \"Warehouse manager\"           \"Forensic psychologist\"      \r\n [7] \"Musician\"                    \"Warehouse manager\"          \r\n [9] \"Pharmacist, hospital\"        \"Merchant navy officer\"      \r\n\r\nYou can even use get typical names or jobs for a given country. To see the available languages and countries type charlatan::PersonProvider$new()$allowed_locales().\r\n\r\n\r\nch_name(n=10,locale=\"de_DE\")\r\n\r\n\r\n [1] \"Frau Silva Jacobi Jäckel\"          \r\n [2] \"Gerhardt Sölzer\"                   \r\n [3] \"Gesa Junken-Ring\"                  \r\n [4] \"Robert Salz\"                       \r\n [5] \"Christoph Geisler\"                 \r\n [6] \"Dipl.-Ing. Andrew Steinberg B.Eng.\"\r\n [7] \"Hiltraud Trubin-Hölzenbecher\"      \r\n [8] \"Carsten Lange\"                     \r\n [9] \"Hans Dieter Scholz\"                \r\n[10] \"Univ.Prof. Vinzenz Misicher\"       \r\n\r\n\r\n\r\nch_phone_number(locale=\"de_DE\",n=10)\r\n\r\n\r\n [1] \"+49(0)6053 39062\"    \"+49(0)3116570271\"    \"07281 572364\"       \r\n [4] \"08191 49288\"         \"(06819) 00442\"       \"02564 195619\"       \r\n [7] \"04519 705930\"        \"+49(0)1695 423941\"   \"+49(0)4772 34159\"   \r\n[10] \"+49 (0) 7975 358065\"\r\n\r\nA nice small application with fake locations and random R colors.\r\n\r\n\r\nlocations <- data.frame(lon=ch_lon(n=10),lat=ch_lat(n=10),col=ch_color_name(n=10))\r\n\r\nggplot(locations)+\r\n  borders(\"world\")+\r\n  geom_point(aes(x=lon,y=lat,col=col),size=3)+\r\n  coord_quickmap()\r\n\r\n\r\n\r\n\r\nfabricatr\r\nEasy creation of hierarchical data is possible with {fabricatr}. In this example there are five families, each one has between 1 and 12 members. Each family member has between 1 and 5 accounts. With add_level() we can automatically produce a table that shows all accounts of all members in all families.\r\n\r\n\r\nlibrary(fabricatr)\r\n\r\nfabricate(\r\n  family  = add_level(N = 5,\r\n  n_members = sample(1:12, N, replace = TRUE,prob=12:1)),\r\n  \r\n  members  = add_level(N = n_members,\r\n  n_accounts = sample(1:5,N,replace=TRUE,prob=(5:1)^2)),\r\n  \r\n  account = add_level(N = n_accounts)\r\n  ) %>%\r\nhead(10)\r\n\r\n\r\n   family n_members members n_accounts account\r\n1       1         1      01          1      01\r\n2       2         7      02          2      02\r\n3       2         7      02          2      03\r\n4       2         7      03          2      04\r\n5       2         7      03          2      05\r\n6       2         7      04          1      06\r\n7       2         7      05          1      07\r\n8       2         7      06          1      08\r\n9       2         7      07          1      09\r\n10      2         7      08          2      10\r\n\r\nLink levels. We can create 15 clients with their birth year and join year and some correlation between both variables.\r\n\r\n\r\ndf <- fabricate(\r\n  age = add_level(N=51, birth_year=1950:2000),\r\n  tenure = add_level(N = 20, join_year=1991:2010, nest = FALSE),\r\n  client = link_levels(N = 15, by = join(age, tenure, rho = 0.7))\r\n)\r\n\r\ndf %>% select(client,birth_year,join_year)\r\n\r\n\r\n   client birth_year join_year\r\n1      01       1967      2000\r\n2      02       1967      2004\r\n3      03       1957      2002\r\n4      04       1997      2009\r\n5      05       1954      1991\r\n6      06       1995      2004\r\n7      07       1998      2005\r\n8      08       1960      2002\r\n9      09       1961      2005\r\n10     10       1970      2007\r\n11     11       1999      2008\r\n12     12       1983      1992\r\n13     13       1981      2001\r\n14     14       1984      2003\r\n15     15       1956      2001\r\n\r\nOrdered data\r\nfabricatr has an amazing function to create ordered categorical data.\r\nThe function we need is draw_ordered. It internally simulates a numeric variable (x) and breaks them into predefined categories.\r\n\r\n\r\ndraw_ordered(\r\n  x = rnorm(10),\r\n  breaks = c(-2,-1,0.8,2),\r\n  break_labels = c(\"Very boring\",\"Boring\",\"OK\",\"Interesting\",\"Very Interesting\")\r\n)\r\n\r\n\r\n [1] OK          OK          OK          Boring      OK         \r\n [6] OK          OK          Interesting OK          OK         \r\nLevels: Very boring Boring OK Interesting Very Interesting\r\n\r\nLet’s take a look at another example where we have two types of clients, gold clients that receive a yearly gift from the bank and standard clients that do not. How could we simulate their responses to a satisfaction survey?\r\n\r\n\r\ndf <- fabricate(\r\n  N = 100,\r\n  gold_client_flag = draw_binary(prob = 0.3, N),\r\n  satisfaction = draw_ordered(\r\n    x = rnorm(N, mean = -0.4 + 1.2 * gold_client_flag),\r\n    breaks = c(-1.5, -0.5, 0.5, 1.5),\r\n    break_labels = c(\"Very Unsatisfied\", \"Unsatisfied\", \"Neutral\",\r\n                     \"Satisfied\", \"Very Satisfied\")\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n   ID gold_client_flag satisfaction\r\n1 001                0      Neutral\r\n2 002                0      Neutral\r\n3 003                0    Satisfied\r\n4 004                1      Neutral\r\n5 005                0      Neutral\r\n6 006                1      Neutral\r\n\r\nWe can summarize the results and see the differences between the two groups. Ideal data for teaching hypothesis testing.\r\n\r\n\r\ndf %>% count(gold_client_flag,satisfaction) %>%\r\n  tidyr::pivot_wider(id_cols=satisfaction,names_from=\"gold_client_flag\",values_from=\"n\")\r\n\r\n\r\n# A tibble: 5 x 3\r\n  satisfaction       `0`   `1`\r\n  <fct>            <int> <int>\r\n1 Very Unsatisfied    10    NA\r\n2 Unsatisfied         22     1\r\n3 Neutral             30     5\r\n4 Satisfied            9    14\r\n5 Very Satisfied      NA     9\r\n\r\nTime series\r\nExample from this article.\r\nThis example contains the GDP of five countries over the course of five years.\r\n\r\n\r\npanel_units <- fabricate(\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.8),\r\n    growth_error = runif(N, 0.1, 0.5)\r\n  ),\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    gdp_measure = base_gdp + (ts_year * growth_units) + rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\npanel_units\r\n\r\n\r\n   countries base_gdp growth_units growth_error years ts_year\r\n1          1 18.24860    0.3598048    0.1882976    01       0\r\n2          1 18.24860    0.3598048    0.1882976    02       1\r\n3          1 18.24860    0.3598048    0.1882976    03       2\r\n4          1 18.24860    0.3598048    0.1882976    04       3\r\n5          1 18.24860    0.3598048    0.1882976    05       4\r\n6          2 20.22503    0.2527846    0.2673691    06       0\r\n7          2 20.22503    0.2527846    0.2673691    07       1\r\n8          2 20.22503    0.2527846    0.2673691    08       2\r\n9          2 20.22503    0.2527846    0.2673691    09       3\r\n10         2 20.22503    0.2527846    0.2673691    10       4\r\n11         3 19.51671    0.4242682    0.4830631    11       0\r\n12         3 19.51671    0.4242682    0.4830631    12       1\r\n13         3 19.51671    0.4242682    0.4830631    13       2\r\n14         3 19.51671    0.4242682    0.4830631    14       3\r\n15         3 19.51671    0.4242682    0.4830631    15       4\r\n16         4 18.96536    0.7043073    0.4737888    16       0\r\n17         4 18.96536    0.7043073    0.4737888    17       1\r\n18         4 18.96536    0.7043073    0.4737888    18       2\r\n19         4 18.96536    0.7043073    0.4737888    19       3\r\n20         4 18.96536    0.7043073    0.4737888    20       4\r\n21         5 19.98155    0.2285968    0.1649447    21       0\r\n22         5 19.98155    0.2285968    0.1649447    22       1\r\n23         5 19.98155    0.2285968    0.1649447    23       2\r\n24         5 19.98155    0.2285968    0.1649447    24       3\r\n25         5 19.98155    0.2285968    0.1649447    25       4\r\n   gdp_measure\r\n1     18.27962\r\n2     18.76802\r\n3     18.70152\r\n4     19.35480\r\n5     19.68878\r\n6     20.31903\r\n7     20.80900\r\n8     21.01035\r\n9     21.07865\r\n10    21.35453\r\n11    19.59217\r\n12    20.41650\r\n13    20.28511\r\n14    20.68378\r\n15    21.75954\r\n16    18.67586\r\n17    20.06506\r\n18    21.01921\r\n19    21.69612\r\n20    21.40323\r\n21    20.13365\r\n22    20.21459\r\n23    20.25501\r\n24    20.83598\r\n25    21.05301\r\n\r\n\r\n\r\n\r\nWe can take this to the next level and introduce some year specific information and then cross this with the country specific information. We just have to add one layer.\r\n\r\n\r\npanel_global_data <- fabricate(\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    year_shock = rnorm(N, 0, 0.5) #each year has a global trend\r\n  ),\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.5), \r\n    growth_error = runif(N, 0.1, 0.5),\r\n    nest = FALSE\r\n  ),\r\n  country_years = cross_levels(\r\n    by = join(years, countries),\r\n    gdp_measure = base_gdp + year_shock + (ts_year * growth_units) +\r\n      rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nGenOrd\r\nThis package helps to create discrete random variables with prescribed correlation matrix and marginal distributions.\r\n\r\n\r\nlibrary(GenOrd)\r\n\r\n\r\nk <- 4 #number of random variables\r\nmarginal <- list(0.6, c(1/3,2/3), c(1/4,2/4,3/4), c(1/5,2/5,3/5,4/5))\r\n\r\n\r\n\r\nRead the list as follows: * We will create 4 random variables. * The first variable will have two values: 60% of the data will be 1, 40% will be 2. * The second variable will have three values, 1,2 and 3 with a probability of 33% each. * etc… * Each vector in this list refers to one variable, and we will see the cumulative probability for each value.\r\n\r\n\r\ncorrcheck(marginal)\r\n\r\n\r\n[[1]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n           [,1]       [,2]       [,3]       [,4]\r\n[1,]  1.0000000 -0.8333333 -0.8215838 -0.8660254\r\n[2,] -0.8333333  1.0000000 -0.9128709 -0.9237604\r\n[3,] -0.8215838 -0.9128709  1.0000000 -0.9486833\r\n[4,] -0.8660254 -0.9237604 -0.9486833  1.0000000\r\n\r\n[[2]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n          [,1]      [,2]      [,3]      [,4]\r\n[1,] 1.0000000 0.8333333 0.8215838 0.8660254\r\n[2,] 0.8333333 1.0000000 0.9128709 0.9237604\r\n[3,] 0.8215838 0.9128709 1.0000000 0.9486833\r\n[4,] 0.8660254 0.9237604 0.9486833 1.0000000\r\n\r\nThis function shows what are allowable ranges for the correlation matrix, given the input from the marginal distributions.\r\n\r\n\r\nSigma <- matrix(c(1,0.5,0.4,0.3,\r\n                  0.5,1,0.5,0.4,\r\n                  0.4,0.5,1,0.5,\r\n                  0.3,0.4,0.5,1),\r\n                k, k, byrow=TRUE)\r\n\r\n\r\n\r\nWe will create 1000 observations, with the given correlation matrix. Each variable will have the marginal distribution described above.\r\n\r\n\r\nn <- 1000 # sample size\r\nm <- ordsample(n, marginal, Sigma)\r\n\r\ndf <- data.frame(m)\r\nhead(df)\r\n\r\n\r\n  X1 X2 X3 X4\r\n1  1  1  2  1\r\n2  1  1  3  1\r\n3  1  2  3  5\r\n4  1  1  1  1\r\n5  1  1  1  1\r\n6  1  2  2  1\r\n\r\nLet’s verify that the data is actually what we expected. We check the correlation and the marginal distribution for two of the variables.\r\n\r\n\r\ncor(df)\r\n\r\n\r\n          X1        X2        X3        X4\r\nX1 1.0000000 0.5251874 0.3949602 0.3015469\r\nX2 0.5251874 1.0000000 0.5193721 0.4134063\r\nX3 0.3949602 0.5193721 1.0000000 0.5169985\r\nX4 0.3015469 0.4134063 0.5169985 1.0000000\r\n\r\ndf %>% count(X4)\r\n\r\n\r\n  X4   n\r\n1  1 196\r\n2  2 231\r\n3  3 183\r\n4  4 194\r\n5  5 196\r\n\r\ndf %>% count(X1)\r\n\r\n\r\n  X1   n\r\n1  1 627\r\n2  2 373\r\n\r\nLater we can rename the columns and values, but will have assured that they have the desired correlations.\r\nMore packages\r\nIn this blogpost by Joseph Rickert on R Views.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-03-useful-packages-for-data-composition/useful-packages-for-data-composition_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2021-01-02T13:08:13+01:00",
    "input_file": "useful-packages-for-data-composition.utf8.md"
  },
  {
    "path": "posts/2020-11-13-data-composition-with-rmultinom/",
    "title": "Data simulation with rmultinom",
    "description": "When creating several datasets that depend on each other, the rmultinom function from the stats package can be a useful helper. In this example we will see how to create customer transactions from a customer table.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-11-13",
    "categories": [
      "simulation",
      "rmultinom"
    ],
    "contents": "\r\n\r\nContents\r\nAutomate this for all customers\r\nOther possible applications\r\n\r\nIn this article I want to show how the rmultinom() function can help to simulate data. We will simulate client data, and for each client we will create transactions.\r\nThe rmultinom() function simulates the multinomial distribution (Link).\r\nIn my head I always picture the multinomial distribution as a game setup. You have N balls and K bins. Instead of the number of bins, we send a vector of probabilities (of length K), how likely it is for the balls to land in each bin (you can imagine that some bins are closer and others are further away, or that some are larger than others). This vector will be normalized automatically, so you do not have to worry about this.\r\nLet’s try an example, with N=1000 and K=5. We want one of the bins to be twice as large as the others.\r\n\r\n\r\ntest1 <- rmultinom(n=1,size=1000,c(2,1,1,1,1))\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote: The rmultinom() function with n=1 is similar to the sample() function. You would obtain an equivalent result with table(sample(1:5,1000,replace=TRUE,prob=c(2,1,1,1,1))) but I believe that rmultinom() is more elegant.\r\nHow can we use this function to create transactions for a given number of customers? The key is to simulate all important values on client level and use rmultinom to decompose the values into smaller portions. First, let’s get some clients.\r\n\r\n\r\nset.seed(61)\r\n\r\nage <- rnorm(10,mean=50,sd=15) %>% pmax(18) %>% round()\r\ntenure <- (age - 18 - runif(10,1,30)) %>% pmax(0) %>% round()\r\nincome <- rexp(10,0.0001) %>% round(2)\r\n\r\nclient <- data.frame(id=1:10,age,tenure,income)\r\n\r\nclient\r\n\r\n\r\n   id age tenure   income\r\n1   1  44     22 32181.33\r\n2   2  44      7  5454.56\r\n3   3  24      3  4559.78\r\n4   4  55     34 26790.03\r\n5   5  29      0 18592.27\r\n6   6  47     19 40229.93\r\n7   7  61     14  1065.49\r\n8   8  58     17  2750.75\r\n9   9  71     49 12292.53\r\n10 10  45     15   282.05\r\n\r\nFor this exercise, we do not distinguish between different types of transactions. In practice, it would make sense to separate rent, supermarket, transport and other categories.\r\nWe create a second dataframe for clients, which contains “invisible” information needed for the transactions. Let’s begin with the total spending. This can depend on anything we know about the client. In this case, we will assume that each client has more or less the same behavior and spends around 70% of their income. The standard deviation of 0.1 assures that this value varies from client to client.\r\n\r\n\r\ncl_secret_info <- client\r\n\r\ncl_secret_info$total_spend <- (cl_secret_info$income * rnorm(10,0.7,sd=0.1)) %>% round(2)\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend\r\n1   1  44     22 32181.33    20755.16\r\n2   2  44      7  5454.56     4342.34\r\n3   3  24      3  4559.78     3280.65\r\n4   4  55     34 26790.03    21012.47\r\n5   5  29      0 18592.27    13990.48\r\n6   6  47     19 40229.93    25986.32\r\n7   7  61     14  1065.49      699.14\r\n8   8  58     17  2750.75     2158.39\r\n9   9  71     49 12292.53    10135.33\r\n10 10  45     15   282.05      160.08\r\n\r\nThe next ingredient is the number of transactions. For this example, we create a formula depending on age: Clients which are younger than 50 have (on average) a higher number of transactions per month.\r\n\r\n\r\ncl_secret_info$n_trans <- ifelse(cl_secret_info$age < 50, rbinom(10,60,0.5),rbinom(10,60,0.3))\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans\r\n1   1  44     22 32181.33    20755.16      27\r\n2   2  44      7  5454.56     4342.34      37\r\n3   3  24      3  4559.78     3280.65      28\r\n4   4  55     34 26790.03    21012.47      25\r\n5   5  29      0 18592.27    13990.48      32\r\n6   6  47     19 40229.93    25986.32      27\r\n7   7  61     14  1065.49      699.14      11\r\n8   8  58     17  2750.75     2158.39      14\r\n9   9  71     49 12292.53    10135.33      16\r\n10 10  45     15   282.05      160.08      27\r\n\r\nNow we already know that our transaction table will have 244 rows, the sum of all our 10 clients’ transactions.\r\nWe will create a last parameter which is an indicator of how similar the transactions are. You could split $100 into one large transaction of $80 and four small transactions of $5 each or you could have five transactions of around $20 each. The higher the value of diff_trans the higher the variability within the transactions of a client.\r\n\r\n\r\ncl_secret_info$diff_trans <- rexp(10,100/cl_secret_info$total_spend) %>% ceiling()\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans diff_trans\r\n1   1  44     22 32181.33    20755.16      27        328\r\n2   2  44      7  5454.56     4342.34      37         55\r\n3   3  24      3  4559.78     3280.65      28         38\r\n4   4  55     34 26790.03    21012.47      25         65\r\n5   5  29      0 18592.27    13990.48      32          1\r\n6   6  47     19 40229.93    25986.32      27       1408\r\n7   7  61     14  1065.49      699.14      11          1\r\n8   8  58     17  2750.75     2158.39      14         47\r\n9   9  71     49 12292.53    10135.33      16        183\r\n10 10  45     15   282.05      160.08      27          2\r\n\r\nNow we have all the necessary ingredients to split total_spend into n_trans transactions for each client. And this is the moment where the rmultinom function is extremely helpful. Let’s take a look at the first client, who spends $20755.16 in 27 transactions. The high diff_trans value indicates that there will likely be some very high transaction values and some very low.\r\nBefore doing the rmultinom magic, we will create the vector with the bins first. Remember that this vector determines how “large” each bin is or how likely it is to\r\n\r\n\r\nbins <- runif(cl_secret_info$n_trans[1],min=1,max=cl_secret_info$diff_trans[1])\r\n\r\ntransactions1 <- rmultinom(1,cl_secret_info$total_spend[1],bins)\r\n\r\ndf <- data.frame(client_id=1, trans_id=1:cl_secret_info$n_trans[1],value=transactions1)\r\n\r\n\r\n\r\n\r\npreserveaa4ce7a2b5044801\r\n\r\nAutomate this for all customers\r\nIn order to efficiently do this for all customers we will put what we just did in a function.\r\n\r\n\r\ncreate_transactions <- function(i) {\r\n  bins <- runif(cl_secret_info$n_trans[i],min=1,max=cl_secret_info$diff_trans[i])\r\n\r\n  transactions <- rmultinom(1,cl_secret_info$total_spend[i],bins)\r\n\r\n  df <- data.frame(client_id=i, trans_id=1:cl_secret_info$n_trans[i],value=transactions)\r\n  \r\n  return(df)\r\n}\r\n\r\n\r\n\r\nWe call this function repeatedly with lapply.\r\n\r\n\r\ntrans_list <- lapply(seq_along(client$id),create_transactions)\r\n\r\n\r\n\r\nFinally, we bind all the transactions from all clients together in our final dataframe.\r\n\r\n\r\ntrans_df <- do.call(rbind,trans_list)\r\n\r\n\r\n\r\n\r\npreserved974ee25f34a534e\r\n\r\nOther possible applications\r\nStudents and grades (it is easier if the grades are points and you have a total number of points to reach). You might want to check the package {wakefield} to create sequences of grades / tests etc.\r\nProducts and sales numbers in supermarkets.\r\nAnimals and tracked kilometers.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-13-data-composition-with-rmultinom/data-composition-with-rmultinom_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-02T13:07:09+01:00",
    "input_file": "data-composition-with-rmultinom.utf8.md"
  },
  {
    "path": "posts/2020-11-01-simulate-dependent-variables/",
    "title": "Simulate dependent variables",
    "description": "When you simulate a dataset it is often not enough to have independent variables, but you  want to have some dependency between the variables. In this post we explore ways of creating this dependency.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-11-01",
    "categories": [
      "simulation",
      "correlation"
    ],
    "contents": "\r\n\r\nContents\r\nPackages\r\nSimulating dependent variablesRule based\r\nCorrelation based\r\n\r\nClosing comments\r\n\r\nPackages\r\nMost of the functions that we are using here are actually part of base R.\r\nWe will need some functions from the {dplyr} and {ggplot2} packages for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nSimulating dependent variables\r\nStorytelling with data is an important skill for anyone who is analyzing data. You try to find interesting information in data and then think of how to convey these insights to others.\r\nIn the previous post we saw how to simulate independent variables. Instead of finding stories in the data we now want to “hide” stories for others to find or to show how a certain analytics / visualization / data wrangling technique works.\r\nAn example: when we look at bank clients, we would expect older clients to have (on average!) a higher balance than younger clients.\r\nIn this section we are going to have a look at techniques to create dependence between variables.\r\nRule based\r\nWe can use ifelse() and case_when() from the {dplyr} package to create new variables that depend on others. Let’s make a small example with two columns: married, which indicates if the person is married, and age.\r\nWe will simulate 1000 clients, around 50% of which are married.\r\n\r\n\r\nk <- 1000\r\nmarried <- sample(c(\"Y\",\"N\"),k,replace=T)\r\n\r\n\r\n\r\nNext, we want that our married clients are slightly older than our non-married clients. For this example we assume that the average age of the married clients is 40, and the average age of the non-married clients is 30.\r\n\r\n\r\ndata <- data.frame(id=1:k,married) %>% \r\n  mutate(\r\n    age=ifelse(married==\"Y\", rnorm(k, 40, sd = 10), rnorm(k, 30, sd= 12)) %>% \r\n      pmax(18) %>% #every client should be at least 18\r\n      round()\r\n    )\r\n\r\n\r\n\r\n\r\n  id married age\r\n1  1       Y  46\r\n2  2       Y  53\r\n3  3       Y  46\r\n4  4       Y  30\r\n5  5       N  28\r\n6  6       N  38\r\n\r\nWe can take a quick look if the difference is visible in a boxplot.\r\n\r\n\r\n\r\nIf you have more than two options, case_when() can help. We want to see the balance of clients which are either managers, analysts or senior analysts.\r\n\r\n\r\nk <- 1000\r\n\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\n\r\ndata <- data.frame(id=1:k,ocupation)\r\n\r\ndata <- data %>% mutate(balance=case_when(\r\n  ocupation==\"analyst\" ~ 100+rexp(k,0.01),\r\n  ocupation==\"sr analyst\" ~ 200+rexp(k,0.005),\r\n  TRUE ~ 200+rexp(k,0.001) #this is the else case\r\n))\r\n\r\n#Check the average balance per group\r\ndata %>% \r\n  ggplot(aes(x=ocupation,y=balance))+geom_violin()\r\n\r\n\r\n\r\n\r\nCorrelation based\r\nIf we just deal with numeric variables and want to have a slightly more complex connection between the variables, we can try another approach, for which we specify a correlation matrix beforehand and reorder our variables afterwards so that they match the desired correlation.\r\nOf course, we need to find reasonable correlation values, for example between age and number of kids (probably slightly positively correlated) or between savings and number of kids (probably slightly negatively correlated). This requires some research.\r\nFirst, we simulate the data independently. Ideas about how to do this can be found in the previous blogpost.\r\n\r\n\r\nset.seed(64)\r\n\r\nk <- 2000\r\n\r\nage <- rnorm(k,mean=35,sd=10) %>% pmax(18) %>% round()\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\ntenure <- rnorm(k,mean=15,sd=5) %>% pmax(1) %>% round()\r\nkids_cnt <- sample(0:5,k,replace=T,prob=c(100,120,80,30,5,1))\r\n\r\n\r\ndata <- data.frame(age,balance,kids_cnt,tenure)\r\n\r\n\r\n\r\n\r\n  age balance kids_cnt tenure\r\n1  18 3665.34        2     10\r\n2  18  268.55        2      8\r\n3  22 1628.59        0     22\r\n4  50 1995.58        1     12\r\n5  35 1510.58        0     20\r\n6  32   58.58        0      5\r\n7  45  945.11        0     21\r\n\r\nWe directly see that there are things that don’t make too much sense, like the 22-years-old with a tenure of 22 years. Further, there is no dependence between the variables.\r\nTo improve this, we want to reshuffle the rows and get a correlation close to a desired one. First we simulate a helping dataset of same size, where every entry is random normally distributed.\r\n\r\n\r\n#same size\r\nnvars <- ncol(data)\r\nnumobs <- nrow(data)\r\n\r\nset.seed(3)\r\nrnorm_helper <- matrix(rnorm(nvars*numobs),nrow=numobs)\r\n\r\n\r\n\r\nThe correlation of this matrix should be close to the identity matrix.\r\n\r\n\r\ncor(rnorm_helper)\r\n\r\n\r\n             [,1]        [,2]        [,3]         [,4]\r\n[1,]  1.000000000  0.01401896 -0.04354997  0.003074106\r\n[2,]  0.014018960  1.00000000 -0.02230676 -0.023084811\r\n[3,] -0.043549974 -0.02230676  1.00000000  0.034222804\r\n[4,]  0.003074106 -0.02308481  0.03422280  1.000000000\r\n\r\nNext, we specify our desired correlation matrix. Just to put this in words, we want to correlate the four variables age, balance, kids_cnt and tenure. Each variable with itself has a correlation of 1. We want age and balance to have a positive correlation of 0.3, age and kids_cnt of 0.4 and age and tenure of 0.2. Likewise, we specify all desired correlations between pairs of variables.\r\n\r\n\r\nQ <- matrix(c(1,0.3,0.4,0.2,  0.3,1,0,0.3,  0.4,0,1,-0.3,  0.2,0.3,-0.3,1),ncol=nvars)\r\n\r\nQ\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0  0.0  0.3\r\n[3,]  0.4  0.0  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nWe can now multiply the rnorm_helper matrix with the Cholesky decomposition of our desired correlation matrix Q. Why this works, is explained in the following comment. If you are not interested in mathematical details, you can skip this part.\r\n\r\n(Explanation found here)\r\n\r\n\r\nL <- chol(Q)\r\nZ <- L %*% t(rnorm_helper)\r\n\r\n\r\n\r\nGood, now we convert this new data to a data frame and name it like our original data.\r\n\r\n\r\nraw <- as.data.frame(t(Z),row.names=NULL,optional=FALSE)\r\nnames(raw) <- names(data)\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n         age     balance   kids_cnt      tenure\r\n1 -0.9319024 -1.03937912  0.7529340 -0.02161421\r\n2 -0.5187915 -0.01357012  0.1255967 -0.79836995\r\n3 -0.3057782  0.77408122 -1.6277816 -0.05606764\r\n4 -1.1081923 -1.04008135  1.0143045 -0.33897914\r\n5  0.8742047 -0.22794845  2.1760844 -0.85681356\r\n6 -0.5173514 -0.69228249  0.2082067 -1.21550256\r\n7 -0.1811401 -1.31629872  1.6929196 -1.89784740\r\n\r\nThe correlation of this dataset is close to our desired outcome.\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance   kids_cnt     tenure\r\nage      1.0000000  0.2680816  0.2221714  0.1887492\r\nbalance  0.2680816  1.0000000 -0.2196451  0.2275166\r\nkids_cnt 0.2221714 -0.2196451  1.0000000 -0.3648036\r\ntenure   0.1887492  0.2275166 -0.3648036  1.0000000\r\n\r\nHowever, this dataset raw does not have anything to do with our original data. It is still only transformed random normal data. But as we know that this dataset has the correct correlation, we can use this to reorder the rows of our other dataset.\r\nAnd then we just replace the largest value of the random normal dataset with the largest value in our dataset, the second largest with the second largest etc. We go column by column and repeat this procedure.\r\n\r\n\r\nfor(name in names(raw)) {\r\n  raw <- raw[order(raw[,name]),]\r\n  data <- data[order(data[,name]),]\r\n  raw[,name] <- data[,name]\r\n}\r\n\r\n\r\n\r\nLet’s check the correlation of this new dataset. It is close to our desired correlation matrix Q. The main reason for the small difference is that our variables take less values than a random normal distributed variable (e.g. kids count just takes values between 0 and 5).\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance   kids_cnt     tenure\r\nage      1.0000000  0.2347427  0.1956973  0.1806137\r\nbalance  0.2347427  1.0000000 -0.1706980  0.2131863\r\nkids_cnt 0.1956973 -0.1706980  1.0000000 -0.3497388\r\ntenure   0.1806137  0.2131863 -0.3497388  1.0000000\r\n\r\n\r\nTo compare: This was Q:\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0  0.0  0.3\r\n[3,]  0.4  0.0  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nOur final reshuffled and correctly correlated dataset is now stored in raw.\r\n\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n     age balance kids_cnt tenure\r\n1054  18  244.80        2      1\r\n1216  27  251.62        1      1\r\n1898  18  120.84        2      1\r\n1736  30  722.46        0      1\r\n1316  27  587.91        0      1\r\n307   22  157.54        3      1\r\n1970  31 1156.21        2      2\r\n\r\nClosing comments\r\nIf you like the correlation method please take a look at the GenOrd package which is a little more professional, when working with ordinal categorical variables.\r\nThe Cholesky decomposition is only possible for positive definite matrices. If this is not the case and you accept a slightly stronger deviation from your desired correlation matrix, the easiest way is to add 0.1, 0.2 etc. to the diagonals until you obtain a positive definite matrix. Note that this lowers the correlation between all variables.\r\n\r\n\r\ndiag(nvars) * 0.1 + Q\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.1  0.3  0.4  0.2\r\n[2,]  0.3  1.1  0.0  0.3\r\n[3,]  0.4  0.0  1.1 -0.3\r\n[4,]  0.2  0.3 -0.3  1.1\r\n\r\nAfter the correlation process it might be helpful to check some of your data manually to see if the observation make sense and - if needed - perform manual corrections.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-01-simulate-dependent-variables/simulate-dependent-variables_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-01-02T12:49:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-data-simulation/",
    "title": "Simulate variables and data",
    "description": "The purpose of this post is to enable readers to create data from scratch which they can use for their analyses or visualizations.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "simulation",
      "distributions"
    ],
    "contents": "\r\n\r\nContents\r\nPackages\r\nData simulation\r\nManual values\r\nCategorical variables with sample()\r\nNumerical variables\r\nDistributions\r\nCombining variables in a dataframe\r\n\r\nPackages\r\nMost of the functions that we are using here are part of base R.\r\nWe will need some functions from the {dplyr} and {ggplot2} packages for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nData simulation\r\nIn this post we will learn how to simulate data like this:\r\n\r\n  client_id    name age  ocupation balance married_flg\r\n1         1   Frank  26 sr analyst  245.96          No\r\n2         2  Dorian  26    analyst 2273.39          No\r\n3         3     Eva  18    manager 2270.47          No\r\n4         4   Elena  34    analyst  373.45          No\r\n5         5    Andy  18    analyst  961.21         Yes\r\n6         6 Barbara  28    analyst   69.32         Yes\r\n7         7  Yvonne  37    analyst 3218.13         Yes\r\n\r\nImportant to make your data creation reproducible (i.e. if you run it again, it yields the same result) is the set.seed() function. As we are creating instances of random variables we assure with this function that every time the same sequence of random variables is generated. You can use any number you like inside this function.\r\n\r\n\r\nset.seed(64)\r\n\r\n\r\n\r\nManual values\r\nLet’s start with the most simple but most time-consuming way. Type everything manually and save it in a vector:\r\n\r\n\r\nclient_gen <- c(\"Millenial\",\"Gen X\",\"Millenial\",\r\n                \"Baby Boomer\",\"Gen X\",\"Millenial\",\"Gen X\")\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1   Millenial\r\n2  2       Gen X\r\n3  3   Millenial\r\n4  4 Baby Boomer\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nCategorical variables with sample()\r\nFor categorical variables, we can save some time using the sample function. You specify first the possible values and then how many of these values you would like to pick. If you want to allow values to be picked more than once, make sure to set replace=TRUE.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"),7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1 Baby Boomer\r\n2  2   Millenial\r\n3  3   Millenial\r\n4  4   Millenial\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nThe sample function is quite flexible and we can tweak the prob parameter, for example to say that we want (approximately) half of the population to be Baby Boomers. The effect will be visible if we produce larger amounts of data.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"), 1000, replace=TRUE, prob=c(0.25,0.25,0.5))\r\n\r\nqplot(client_gen)\r\n\r\n\r\n\r\n\r\nNumerical variables\r\nThe same sample() function works with numbers.\r\n\r\n\r\nclient_age <- sample(1:100,size=7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1          4\r\n2  2         42\r\n3  3         33\r\n4  4         62\r\n5  5         76\r\n6  6         65\r\n7  7         81\r\n\r\nIn both cases above, each number had the same probability of being selected. If we would like some numbers to be more likely to be selected, we can specify this with prob.\r\nThe probability values will be automatically scaled to 1. If I would like to have 50% of the population to have the age of 27, I can specify the weight. (Note: rep(1,5) is equivalent to c(1,1,1,1,1), replicating the number 1 five times.)\r\n\r\n\r\nclient_age <- sample(1:100,size=1000,replace=TRUE,prob=c(rep(1,26),99,rep(1,73)))\r\n\r\nqplot(client_age==27)\r\n\r\n\r\n\r\n\r\nDistributions\r\nIf you would like to work with probability distributions to create numerical variable, this is also very easy with the base functions of type r+(starting letters of the distribution).\r\nLet’s try the uniform distribution:\r\n\r\n\r\nclient_age <- runif(7,min=1,max=100)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1   55.10342\r\n2  2   85.19588\r\n3  3   86.47791\r\n4  4   73.91516\r\n5  5   48.15197\r\n6  6   32.90848\r\n7  7   58.76874\r\n\r\nAs we are simulating ages, we are not interested in decimal values. We can use the round() function to round each number to the next integer.\r\n\r\n\r\nrunif(10000,1,100) %>% round() %>% head(10)\r\n\r\n\r\n [1] 93 26 28 29 90 81 95  3 21 62\r\n\r\nBut uniformly distributed variables are not always what we want. In the example above we simulated 10,000 clients and distributes their ages uniformly. For most applications it would be unrealistic that there are as many 99 year old clients as there are 50 year old clients.\r\nBut we can easily access a whole list of other distribution functions, like the famous Normal distribution (with mean and standard deviation as parameters).\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf we want to limit the values to not be smaller than 0 or larger than 100, we can use pmin and pmax.\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% pmax(0) %>% pmin(100) %>% qplot()\r\n\r\n\r\n\r\n\r\nFor many applications (like balance distribution or any data that contains outliers) I like to use the Exponential distribution (with parameter rate and expectation 1/rate).\r\n\r\n\r\nrexp(10000,rate=0.01) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf you want to explore further probability distributions check out this link. Playing around with the parameters of the distributions you will notice that you can simulate almost any variable you like (Take a short look at: The different faces of the Beta distribution).\r\n\r\n\r\n\r\nCombining variables in a dataframe\r\nTo create our first simulated dataframe, we can start by simulating the variables separately and then putting them together.\r\n\r\n\r\nset.seed(61)\r\n\r\nk <- 7\r\n\r\nid <- 1:k\r\nname <- c(\"Frank\",\"Dorian\",\"Eva\",\"Elena\",\"Andy\",\"Barbara\",\"Yvonne\")\r\nage <- rnorm(k,mean=30,sd=10) %>% pmax(18) %>% round()\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\nmarried <- sample(c(\"Yes\",\"No\"),k,replace=T,prob=c(0.6,0.4))\r\n\r\ndata <- data.frame(client_id=id,name,age,ocupation,balance,married_flg=married)\r\ndata\r\n\r\n\r\n  client_id    name age  ocupation balance married_flg\r\n1         1   Frank  26 sr analyst  245.96          No\r\n2         2  Dorian  26    analyst 2273.39          No\r\n3         3     Eva  18    manager 2270.47          No\r\n4         4   Elena  34    analyst  373.45          No\r\n5         5    Andy  18    analyst  961.21         Yes\r\n6         6 Barbara  28    analyst   69.32         Yes\r\n7         7  Yvonne  37    analyst 3218.13         Yes\r\n\r\nGreat! We just simulated a dataset which we can use now for visualization or modeling purposes.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-09-data-simulation/data-simulation_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-01-02T13:05:43+01:00",
    "input_file": "data-simulation.utf8.md"
  },
  {
    "path": "posts/2020-10-09-fuzzy-merging/",
    "title": "Fuzzy matching",
    "description": "Whenever you have text data that was input manually by a human, there is a chance that it contains errors: Typos, abbreviations or different ways of writing can be challenges for your analysis. Fuzzy matching is a way to find inexact matches that mean the same thing like mcdonalds, McDonalds and McDonald's Company.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "text data",
      "fuzzy matching",
      "stringdist"
    ],
    "contents": "\r\n\r\nContents\r\nPackages\r\nThe data\r\nMain process\r\nResults\r\nNext Steps and other resources\r\n\r\nPackages\r\nThe only packages you need are dplyr and stringdist.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(stringdist)\r\n\r\n\r\n\r\nThe data\r\nThis method requires as input two lists. To distinguish them, we will call the one that contains the handtyped input as the “dirty list”. The reference list will be called the “clean list”. In this blogpost I will create the dirty list by hand with a few made-up examples of alternative company names.\r\n\r\n\r\nnames <- c(\"Haliburton\", \"ExxonMobile\",\"ABBOTT LABORATORIES\",\"Marrriott\",\"Self\",\"Activision Blizzard\",\r\n           \"Quest dianotstics\",\"Unemployed\",\"other company\",\"burger king\",\r\n           \"MARRIOT\",\"wall mart\", \"Illumin\", \"3M\",\"NORTHROP TRUMMON\",\"MCCormicks\",\"MARSH MCLEANNON\",\r\n           \"FLO SERVE\", \"Kansas City Southern Fed.\",\"MCDONALD'S\",\"F5 Networks\",\r\n           \"McDonalds\",\"MacKindsey\",\"Oracle\",\"Self-employed\",\"None\",\"Retired\",\r\n           \"f5 networks\",\"Harley Davidson\",\"Harly Davidson\",\"HARLEY DAVIDSEN\",\"DRHorton\",\"D.R. Horten\",\r\n           \"cincinati fin\",\"cincinnatti financials\",\"cincinnati financial\",\"CINCINATTI FINANCE\",\r\n           \"Mohaws Industry\",\"Mowahk Industries\",\"Mohawk Ind\")\r\n\r\nset.seed(64)\r\ndirty_list <- sample(names,50000,replace=T)\r\n\r\n\r\n\r\n\r\ndirty_list\r\nHaliburton\r\nExxonMobile\r\nABBOTT LABORATORIES\r\nMarrriott\r\nSelf\r\nActivision Blizzard\r\nQuest dianotstics\r\nUnemployed\r\nother company\r\nburger king\r\n\r\nAs a clean list we will use the list of S&P500 companies. This can be downloaded or scraped from the internet.\r\n\r\nclean_list\r\n3M Company\r\nAbbott Laboratories\r\nAbbVie Inc.\r\nABIOMED Inc\r\nAccenture plc\r\nActivision Blizzard\r\nAdobe Systems Inc\r\nAdvanced Micro Devices Inc\r\nAdvance Auto Parts\r\nAES Corp\r\n\r\nBefore we start, we will pre-process both lists, remove some common words and transform everything to lower case. If you prefer, you can also use the {stringr} package for this. One comment from my experience: Usually, the construction of the common words to remove is an iterative approach: You would check your final result and see which words are still causing problems. Then you add them to the cleaner function and run the process again until you are satisfied with the results.\r\n\r\n\r\ncleaner <- function(vec) {\r\n  wordremove <- c(\" and \",\" comp \",\" company\",\"companies\",\" corp \",\"corporation\",\" inc \",\"[.]com\")\r\n  output <- vec %>% tolower() %>% \r\n    {gsub(paste(wordremove,collapse='|'),\"\",.)} %>%\r\n    {gsub(\"[[:punct:]]\",\"\",.)} %>%\r\n    {gsub(\"[[:blank:]]\",\"\",.)}\r\n  return(output)\r\n}\r\n\r\ncontrol <- data.frame(original=dirty_list)\r\n\r\nclean_list_cl <- cleaner(clean_list)\r\ndirty_list_cl <- cleaner(dirty_list)\r\n\r\n\r\n\r\nMain process\r\nWe calculate a matrix of string distances. The {stringdist} package has a lot of different methods implemented which can be checked here. After comparing some of the methods I decided to go with the Jaro-Winkler distance as it yields higher similarity for words which start with the same letters.\r\nExample\r\n\r\n\r\nstringdistmatrix(c(\"other\",\"words\",\"otherexample\",\"exapmle\"),\r\n                 c(\"example\",\"other example\",\"word\"),\r\n                 method='jw',p=0.1,useNames=\"strings\")\r\n\r\n\r\n                example other example      word\r\nother        1.00000000    0.12307692 0.5166667\r\nwords        1.00000000    0.48205128 0.0400000\r\notherexample 0.28174603    0.01538462 0.4444444\r\nexapmle      0.03333333    0.55799756 1.0000000\r\n\r\nEach row of the matrix of string distances is one string from the dirty list. We find the minimum in each row, which is equivalent to the best fit from the clean list.\r\n\r\n\r\ndistmatrix <- stringdist::stringdistmatrix(dirty_list_cl,clean_list_cl,method='jw',p=0.1)\r\nbest_fit <- apply(distmatrix,1,which.min) %>% as.integer()\r\nsimilarity <- apply(distmatrix,1,min)\r\n\r\ncontrol$best_fit <- clean_list[best_fit]\r\ncontrol$distance <- round(similarity,3)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nRetired\r\nResMed\r\n0.203\r\nSelf\r\nSealed Air\r\n0.244\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nSelf-employed\r\nTeleflex\r\n0.306\r\n\r\nResults\r\nWhen we order the control dataframe by similarity we can find a suitable cutoff value (in this example 0.12) to separate real matches from false positives. This cutoff value depends on the application.\r\n\r\n\r\ncontrol$result <- ifelse(control$distance<=0.12,control$best_fit,NA)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nresult\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nMarriott Int’l.\r\nRetired\r\nResMed\r\n0.203\r\nNA\r\nSelf\r\nSealed Air\r\n0.244\r\nNA\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nHalliburton Co.\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nIllumina Inc\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMohawk Industries\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nMarsh & McLennan\r\nSelf-employed\r\nTeleflex\r\n0.306\r\nNA\r\nMohawk Ind\r\nMohawk Industries\r\n0.088\r\nMohawk Industries\r\nMowahk Industries\r\nMohawk Industries\r\n0.017\r\nMohawk Industries\r\nHarly Davidson\r\nHarley-Davidson\r\n0.014\r\nHarley-Davidson\r\nf5 networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\n3M\r\n3M Company\r\n0.000\r\n3M Company\r\nOracle\r\nOracle Corp.\r\n0.080\r\nOracle Corp.\r\n\r\nNext Steps and other resources\r\nImprove performance for large datasets. On Github, I have an implementation of this method with the parallel package which improves performance slightly. But there is definitely more room for improvement.\r\nThere is an interesting video about performance improvement by not calculating the full matrix by Seth Verrinder and Kyle Putnam here.\r\nAndrés Cruz created an Add-in which helps to fine-tune the final result, his slide from LatinR 2019 can be found here.\r\nCheck out David Robinson’s fuzzyjoin package here.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-09-fuzzy-merging/img/puzzle.png",
    "last_modified": "2021-01-02T13:10:17+01:00",
    "input_file": "fuzzy-merging.utf8.md"
  }
]
