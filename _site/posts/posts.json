[
  {
    "path": "posts/2020-12-03-simulate-dependent-variables/",
    "title": "Simulate dependent variables",
    "description": "When you simulate a dataset it is often not enough to have correctly distributed independent variables, but you also want to have some dependency between the variables. In this post we explore ways of doing this.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-12-03",
    "categories": [],
    "contents": "\r\nPackages\r\nMost of the functions that we are using here are actually part of base R.\r\nWe will need some functions from the dplyr package and the ggplot2 package for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nSimulating dependent variables\r\nIf we want to have a dataset that “makes sense” from a real world perspective, it would be great if managers in general had higher balances than analysts? Or if 18 years old clients are less likely to be married than 30 years olds? In this section we are going to have a look at techniques to create dependence between variables.\r\nRule based\r\nWe can use ifelse() and case_when() to create new variables that depend on others.\r\n\r\n\r\nk <- 7\r\nmarried <- sample(c(\"Y\",\"N\"),k,replace=T)\r\n\r\ndata <- data.frame(id=1:k,married)\r\n\r\ndata %>% mutate(\r\n  age=ifelse(married==\"Y\",rnorm(k,45,10),rnorm(k,30,10)) %>% pmax(18) %>% round()\r\n)\r\n\r\n\r\n  id married age\r\n1  1       Y  57\r\n2  2       N  33\r\n3  3       N  32\r\n4  4       N  28\r\n5  5       N  31\r\n6  6       N  38\r\n7  7       Y  39\r\n\r\nIn this small example we will not see the effect, but when we simulate 1000 clients and take a look at their average age, we can see that there is a difference between the two groups.\r\n\r\n\r\nk <- 1000\r\nmarried <- sample(c(\"Y\",\"N\"),k,replace=T)\r\n\r\ndata <- data.frame(id=1:k,married)\r\n\r\ndata %>% mutate(\r\n  age=ifelse(married==\"Y\",rnorm(k,45,5),rnorm(k,30,10)) %>% \r\n    pmax(18) %>% \r\n    round()\r\n    ) %>% \r\n  ggplot(aes(x=married,y=age))+geom_boxplot()\r\n\r\n\r\n\r\n\r\nIf you have more than two options, case_when() can help.\r\n\r\n\r\nk <- 1000\r\n\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\n\r\ndata <- data.frame(id=1:k,ocupation)\r\n\r\ndata <- data %>% mutate(balance=case_when(\r\n  ocupation==\"analyst\" ~ 100+rexp(k,0.01),\r\n  ocupation==\"sr analyst\" ~ 200+rexp(k,0.005),\r\n  TRUE ~ 500+rexp(k,0.001) #this is the else case\r\n))\r\n\r\n#Check the average balance per group\r\ndata %>% \r\n  ggplot(aes(x=ocupation,y=balance))+geom_violin()\r\n\r\n\r\n\r\n\r\nCorrelation based\r\nIf we just deal with numeric variables and want to have a slightly more complex connection between the different variables, we can also try this approach, for which we specify a correlation matrix beforehand and reorder our variables afterwards so that they match the desired correlation.\r\nOf course, we need to find reasonable correlation values, for example between age and number of kids (slightly positively correlated) or between savings and number of kids (slightly negatively correlated).\r\n\r\n\r\nset.seed(64)\r\n\r\nk <- 2000\r\n\r\nage <- rnorm(k,mean=35,sd=10) %>% pmax(18) %>% round()\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\ntenure <- rnorm(k,mean=15,sd=5) %>% pmax(1) %>% round()\r\nkids_cnt <- sample(0:5,k,replace=T,prob=c(100,120,80,30,5,1))\r\n\r\n\r\ndata <- data.frame(age,balance,kids_cnt,tenure)\r\ndata %>% head(7)\r\n\r\n\r\n  age balance kids_cnt tenure\r\n1  18 3665.34        2     10\r\n2  18  268.55        2      8\r\n3  22 1628.59        0     22\r\n4  50 1995.58        1     12\r\n5  35 1510.58        0     20\r\n6  32   58.58        0      5\r\n7  45  945.11        0     21\r\n\r\nWe directly see that there are things that don’t make sense, like the 22-years-old with a tenure of 22 years.\r\nTo improve this, we want to reshuffle the rows and get a distribution close to a desired one. First we simulate a helping dataset of same size, where every entry is random normal distributed.\r\n\r\n\r\nnvars <- ncol(data)\r\nnumobs <- nrow(data)\r\n\r\nset.seed(3)\r\nrnorm_helper <- matrix(rnorm(nvars*numobs,0,1),nrow=nvars)\r\n\r\n\r\n\r\nThe correlation of this matrix should be close to the identity matrix.\r\n\r\n\r\ncor(t(rnorm_helper))\r\n\r\n\r\n             [,1]        [,2]        [,3]         [,4]\r\n[1,]  1.000000000 -0.00574905 0.009783835 -0.023569599\r\n[2,] -0.005749050  1.00000000 0.049500977  0.010347672\r\n[3,]  0.009783835  0.04950098 1.000000000  0.005859748\r\n[4,] -0.023569599  0.01034767 0.005859748  1.000000000\r\n\r\nNext, we specify our desired correlation matrix:\r\n\r\n\r\nQ <- matrix(c(1,0.3,0.4,0.2,  0.3,1,0,0.3,  0.4,0,1,-0.3,  0.2,0.3,-0.3,1),ncol=nvars)\r\n\r\nQ\r\n\r\n\r\n     [,1] [,2] [,3] [,4]\r\n[1,]  1.0  0.3  0.4  0.2\r\n[2,]  0.3  1.0  0.0  0.3\r\n[3,]  0.4  0.0  1.0 -0.3\r\n[4,]  0.2  0.3 -0.3  1.0\r\n\r\nWe can now multiply the rnorm_helper matrix with the Cholesky decomposition of our desired correlation matrix Q. Why this works, is explained in the following comment. If you are not interested in mathematical details, you can skip this part.\r\n\r\n(Explanation found here)\r\n\r\n\r\nL <- t(chol(Q))\r\nZ <- L %*% rnorm_helper\r\n\r\n\r\n\r\nGood, now we convert this new data to a data frame and give it the name of our original data. The correlation of this dataset is close to our desired outcome.\r\n\r\n\r\nraw <- as.data.frame(t(Z),row.names=NULL,optional=FALSE)\r\nnames(raw) <- names(data)\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n         age     balance   kids_cnt     tenure\r\n1 -0.9619334 -0.56763178 -0.1130367 -1.3627041\r\n2  0.1957828  0.08747126  0.1520695  0.9806283\r\n3 -1.2188574  0.84333548 -1.3231141 -0.6184486\r\n4 -0.7163585  0.02610745 -0.1802922 -0.4043929\r\n5 -0.9530173 -0.90428943  0.8118207 -0.6505000\r\n6 -0.5784837 -1.07244273 -0.2978104 -1.7172915\r\n7 -0.4844551 -0.85227479  0.9530955  0.1474831\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance    kids_cnt     tenure\r\nage      1.0000000 0.29376273  0.39741190  0.1708577\r\nbalance  0.2937627 1.00000000  0.04302845  0.2777607\r\nkids_cnt 0.3974119 0.04302845  1.00000000 -0.3046079\r\ntenure   0.1708577 0.27776070 -0.30460794  1.0000000\r\n\r\nHowever, this dataset raw does not have anything to do with our original data. It is still our transformed random normal data. But as we know that this dataset has the correct correlation, we can use this to reorder the rows of our other dataset.\r\nAnd then we just replace the largest value of the random normal dataset with the largest value in our dataset, the second largest with the second largest etc. We go column by column and repeat this procedure.\r\n\r\n\r\nfor(name in names(raw)) {\r\n  raw <- raw[order(raw[,name]),]\r\n  data <- data[order(data[,name]),]\r\n  raw[,name] <- data[,name]\r\n}\r\n\r\n\r\n\r\nLet’s check the correlation of this new dataset. It is not exactly what we wished, but close enough. The reason for this is that our variables take less values than a random normal distributed variable (e.g. kids count just takes values between 0 and 5).\r\n\r\n\r\ncor(raw)\r\n\r\n\r\n               age    balance    kids_cnt     tenure\r\nage      1.0000000 0.24231169  0.36996781  0.1734406\r\nbalance  0.2423117 1.00000000  0.01880674  0.2358795\r\nkids_cnt 0.3699678 0.01880674  1.00000000 -0.3038222\r\ntenure   0.1734406 0.23587953 -0.30382217  1.0000000\r\n\r\nWe can also take a look at the reshuffled dataset.\r\n\r\n\r\nhead(raw,7,addrownums=FALSE)\r\n\r\n\r\n     age balance kids_cnt tenure\r\n1934  34   36.36        3      1\r\n733   37   52.44        1      1\r\n123   22  290.91        2      1\r\n1032  26  130.01        2      1\r\n1463  32   88.87        2      1\r\n448   43   26.54        4      1\r\n1804  35  911.63        2      2\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-03-simulate-dependent-variables/simulate-dependent-variables_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-12-04T10:59:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-03-useful-packages-for-data-composition/",
    "title": "Useful packages for data composition",
    "description": "We will explore the packages wakefield, rcorpora, charlatan, fabricatr, GenOrd.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-12-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAdditional packages\r\nwakefieldSeries\r\nVisualization\r\n\r\nrcorpora\r\ncharlatan\r\nfabricatrOrdered data\r\nTime series\r\n\r\nGenOrd\r\nMore packages\r\n\r\nAdditional packages\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nwakefield\r\nLooking for interesting packages around data composition I stumbled across the {wakefield} package by Tyler Rinker.\r\n\r\n\r\nlibrary(wakefield)\r\n\r\n\r\n\r\nIntroduction can be found here.\r\n\r\n\r\nr_data_frame(\r\n    n = 500,\r\n    id,\r\n    age,\r\n    hour,\r\n    iq,\r\n    height,\r\n    died,\r\n    animal,\r\n    dice,\r\n    internet_browser,\r\n    political\r\n)\r\n\r\n\r\n# A tibble: 500 x 10\r\n   ID      Age Hour     IQ Height Died  Animal  Dice Browser Political\r\n   <chr> <int> <tim> <dbl>  <dbl> <lgl> <fct>  <int> <fct>   <fct>    \r\n 1 001      25 00:0~   101     68 TRUE  India~     2 Firefox Democrat \r\n 2 002      66 00:0~    86     67 TRUE  Frenc~     3 Chrome  Republic~\r\n 3 003      65 00:0~   113     68 TRUE  Bande~     2 Chrome  Republic~\r\n 4 004      26 00:0~   106     68 FALSE Frenc~     3 Chrome  Democrat \r\n 5 005      46 00:0~   101     64 TRUE  Emu        1 Chrome  Democrat \r\n 6 006      54 00:0~    87     64 TRUE  Albat~     2 Chrome  Democrat \r\n 7 007      21 00:0~   112     68 FALSE Emu        3 Firefox Republic~\r\n 8 008      66 00:0~   103     74 TRUE  Sea S~     4 Safari  Republic~\r\n 9 009      19 00:0~    86     67 TRUE  Albat~     4 Chrome  Democrat \r\n10 010      85 00:0~    93     64 FALSE India~     1 Chrome  Democrat \r\n# ... with 490 more rows\r\n\r\nThere are a lot of predefined variables that you can use.\r\n\r\n      [,1]         [,2]          [,3]               [,4]       \r\n [1,] \"age\"        \"dice\"        \"hair\"             \"military\" \r\n [2,] \"animal\"     \"dna\"         \"height\"           \"month\"    \r\n [3,] \"answer\"     \"dob\"         \"income\"           \"name\"     \r\n [4,] \"area\"       \"dummy\"       \"internet_browser\" \"normal\"   \r\n [5,] \"car\"        \"education\"   \"iq\"               \"political\"\r\n [6,] \"children\"   \"employment\"  \"language\"         \"race\"     \r\n [7,] \"coin\"       \"eye\"         \"level\"            \"religion\" \r\n [8,] \"color\"      \"grade\"       \"likert\"           \"sat\"      \r\n [9,] \"date_stamp\" \"grade_level\" \"lorem_ipsum\"      \"sentence\" \r\n[10,] \"death\"      \"group\"       \"marital\"          \"sex\"      \r\n      [,5]           \r\n [1,] \"sex_inclusive\"\r\n [2,] \"smokes\"       \r\n [3,] \"speed\"        \r\n [4,] \"state\"        \r\n [5,] \"string\"       \r\n [6,] \"upper\"        \r\n [7,] \"valid\"        \r\n [8,] \"year\"         \r\n [9,] \"zip_code\"     \r\n[10,]                \r\nattr(,\"class\")\r\n[1] \"matrix\" \"array\" \r\n\r\nAdditionally, you can also access the distribution functions easily and tweak parameters of the predefined functions.\r\n\r\n\r\ntest <- r_data_frame(\r\n    n = 500,\r\n    id,\r\n    age(x=18:50),\r\n    `Reading(mins)` = rpois(lambda=20),\r\n    income(digits=0)\r\n)\r\n\r\ntest\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      27              13  40099\r\n 2 002      48              30  18588\r\n 3 003      35              19  81664\r\n 4 004      34              21  85229\r\n 5 005      28              22  16206\r\n 6 006      47              18  35152\r\n 7 007      25              22  13919\r\n 8 008      19              22  28440\r\n 9 009      18              24  81271\r\n10 010      22              24   7473\r\n# ... with 490 more rows\r\n\r\nLooks too perfect? Include random missing values:\r\n\r\n\r\ntest %>% r_na(cols=c(2,4),prob=0.3)\r\n\r\n\r\n# A tibble: 500 x 4\r\n   ID      Age `Reading(mins)` Income\r\n   <chr> <int>           <int>  <dbl>\r\n 1 001      27              13  40099\r\n 2 002      48              30  18588\r\n 3 003      NA              19  81664\r\n 4 004      34              21  85229\r\n 5 005      28              22  16206\r\n 6 006      47              18     NA\r\n 7 007      25              22     NA\r\n 8 008      NA              22  28440\r\n 9 009      18              24  81271\r\n10 010      22              24   7473\r\n# ... with 490 more rows\r\n\r\nSeries\r\nSequences of answers:\r\n\r\n\r\nr_series(likert,j = 5,n=10,name=\"Question\")\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Question_1    Question_2    Question_3    Question_4   Question_5  \r\n * <ord>         <ord>         <ord>         <ord>        <ord>       \r\n 1 Strongly Dis~ Strongly Dis~ Strongly Dis~ Agree        Strongly Di~\r\n 2 Disagree      Agree         Agree         Agree        Disagree    \r\n 3 Strongly Dis~ Strongly Dis~ Neutral       Strongly Di~ Agree       \r\n 4 Disagree      Strongly Dis~ Strongly Dis~ Strongly Di~ Agree       \r\n 5 Agree         Disagree      Strongly Agr~ Strongly Ag~ Disagree    \r\n 6 Strongly Agr~ Strongly Dis~ Agree         Neutral      Strongly Ag~\r\n 7 Agree         Strongly Dis~ Agree         Agree        Disagree    \r\n 8 Agree         Disagree      Agree         Strongly Di~ Disagree    \r\n 9 Neutral       Neutral       Strongly Dis~ Strongly Ag~ Strongly Ag~\r\n10 Agree         Disagree      Agree         Strongly Di~ Neutral     \r\n\r\nThese can also be packaged inside a data frame:\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1 Test_2 Test_3\r\n   <chr>   <int>  <int>  <int>  <int>\r\n 1 01         11     85     90     99\r\n 2 02         17     89     95     91\r\n 3 03         17     83     82     88\r\n 4 04         10     85     82     84\r\n 5 05         10     87     82     83\r\n 6 06         18     88     87     88\r\n 7 07         17     91     92     89\r\n 8 08         15     89     85     85\r\n 9 09         13     86     90     81\r\n10 10         15     86     83     86\r\n\r\nThat is great but not very real, because the test results are completely independent from each other. The relate parameter helps to connect the results, and the format is fM_sd.\r\nf is one of (+,-,*,/)\r\nM is the mean value\r\nsd is the standard deviation of the mean value\r\nExample +3_1: The test results get better on average 3 points with a standard deviation of 1.\r\n\r\n\r\nr_data_frame(\r\n  n=10,\r\n  Student=id,\r\n  age=rpois(14),\r\n  r_series(grade,j=3,integer=TRUE,name=\"Test\",relate=\"+3_1\")\r\n)\r\n\r\n\r\n# A tibble: 10 x 5\r\n   Student   age Test_1     Test_2     Test_3    \r\n   <chr>   <int> <variable> <variable> <variable>\r\n 1 01         18 90.2       94.0       96.0      \r\n 2 02          8 84.0       87.6       90.9      \r\n 3 03         18 88.7       92.0       93.8      \r\n 4 04         12 91.5       93.5       97.8      \r\n 5 05         12 94.3       97.2       99.6      \r\n 6 06         13 92.9       96.9       99.7      \r\n 7 07          8 91.9       95.2       97.7      \r\n 8 08         15 87.1       92.6       95.2      \r\n 9 09         12 91.1       94.4       96.3      \r\n10 10          7 84.4       89.4       93.1      \r\n\r\nWith this in mind, you can create customer balances over time very easily.\r\n\r\n\r\nbalances <- r_data_frame(\r\n  n=10,\r\n  Client=name,\r\n  age,\r\n  r_series(income,j=12,name=\"Month\",relate=\"*1.03_0.1\")\r\n)\r\n\r\n\r\n\r\nWe can see that there are customers who had very positive balance development and others whose balances were fluctuating more or declining. However, when we simulate a sufficiently large number of customers, we\r\n\r\n\r\nbalances %>%\r\n  tidyr::pivot_longer(-c(1,2),names_to=\"Month\") %>%\r\n  mutate(Month=readr::parse_number(Month)) %>%\r\n  ggplot(aes(x=Month,y=value))+geom_line()+facet_wrap(~Client,scales=\"free_y\")\r\n\r\n\r\n\r\n\r\nVisualization\r\nThis is a great feature to quickly get a glimpse of data types and missing values (+ missing value distribution).\r\n\r\n\r\nr_data_frame(n=500,\r\n    id,\r\n    dob,\r\n    animal,\r\n    grade, grade,\r\n    death,\r\n    dummy,\r\n    grade_letter,\r\n    gender,\r\n    sentence\r\n) %>%\r\n   r_na(cols=c(\"DOB\",\"Animal\",\"Gender\")) %>% \r\n   table_heat(palette = \"Set1\")\r\n\r\n\r\n\r\nrcorpora\r\nCheck the github repository here.\r\nThe rcorpora library has 293 collections of words that can be very helpful for data simulation.\r\n\r\n\r\nlibrary(rcorpora)\r\n\r\nlength(corpora())\r\n\r\n\r\n[1] 293\r\n\r\ncorpora()[sample(1:293,10)]\r\n\r\n\r\n [1] \"humans/celebrities\"          \"materials/gemstones\"        \r\n [3] \"foods/sandwiches\"            \"words/stopwords/pt\"         \r\n [5] \"technology/new_technologies\" \"geography/norwegian_cities\" \r\n [7] \"words/stopwords/jp\"          \"religion/christian_saints\"  \r\n [9] \"words/stopwords/lv\"          \"sports/football/serieA\"     \r\n\r\nTo view the words of one collection use the name in the corpora() function.\r\n\r\n\r\ncorpora(\"foods/pizzaToppings\")\r\n\r\n\r\n$description\r\n[1] \"A list of pizza toppings.\"\r\n\r\n$pizzaToppings\r\n [1] \"anchovies\"        \"artichoke\"        \"bacon\"           \r\n [4] \"breakfast bacon\"  \"Canadian bacon\"   \"cheese\"          \r\n [7] \"chicken\"          \"chili peppers\"    \"feta\"            \r\n[10] \"garlic\"           \"green peppers\"    \"grilled onions\"  \r\n[13] \"ground beef\"      \"ham\"              \"hot sauce\"       \r\n[16] \"meatballs\"        \"mushrooms\"        \"olives\"          \r\n[19] \"onions\"           \"pepperoni\"        \"pineapple\"       \r\n[22] \"sausage\"          \"spinach\"          \"sun-dried tomato\"\r\n[25] \"tomatoes\"        \r\n\r\nLet see how we can use this in a simulated dataframe.\r\n\r\n\r\ntibble(\r\n  first_name=corpora(\"humans/firstNames\")$firstNames %>% sample(100,replace=TRUE),\r\n  last_name=corpora(\"humans/lastNames\")$lastNames %>% sample(100,replace=TRUE),\r\n  self_description=corpora(\"humans/descriptions\")$descriptions %>% sample(100,replace=TRUE),\r\n  home_country=corpora(\"geography/countries\")$countries %>% sample(100,replace=TRUE),\r\n  favorite_pizza_topping=corpora(\"foods/pizzaToppings\")$pizzaToppings %>% sample(100,replace=TRUE)\r\n)\r\n\r\n\r\n# A tibble: 100 x 5\r\n   first_name last_name self_description home_country favorite_pizza_~\r\n   <chr>      <chr>     <chr>            <chr>        <chr>           \r\n 1 Skylar     Edwards   groggy           Latvia       olives          \r\n 2 Timothy    Gordon    lean             Nicaragua    garlic          \r\n 3 Kelsey     Rose      willing          Cameroon     chicken         \r\n 4 Luke       Stewart   delightful       Sri Lanka    ground beef     \r\n 5 Nicole     Wallace   cruel            Qatar        pineapple       \r\n 6 Jonah      Rice      wary             Tajikistan   pepperoni       \r\n 7 Megan      Stevens   plain-speaking   Swaziland    cheese          \r\n 8 Brody      Barnes    reluctant        Guyana       chicken         \r\n 9 Jason      Foster    one-sided        Uruguay      grilled onions  \r\n10 Thomas     Chavez    unaffected       Estonia      ground beef     \r\n# ... with 90 more rows\r\n\r\ncharlatan\r\n\r\n\r\nlibrary(charlatan)\r\n\r\nch_job(n=10)\r\n\r\n\r\n [1] \"Counsellor\"                     \"Systems analyst\"               \r\n [3] \"Clinical research associate\"    \"Aeronautical engineer\"         \r\n [5] \"Accommodation manager\"          \"Dance movement psychotherapist\"\r\n [7] \"Fitness centre manager\"         \"Programmer, applications\"      \r\n [9] \"Event organiser\"                \"Actor\"                         \r\n\r\nch_name(n=10,locale=\"de_DE\")\r\n\r\n\r\n [1] \"Frida Höfig\"                    \"Univ.Prof. Ignatz Adler B.Eng.\"\r\n [3] \"Herr Hans-Adolf Schaaf B.Eng.\"  \"Hans-Christian Lübs MBA.\"      \r\n [5] \"Ing. Marc Fröhlich\"             \"Luisa Mitschke\"                \r\n [7] \"Frau Marie-Louise Bien B.Eng.\"  \"Katja Schomber-Schmiedt\"       \r\n [9] \"Frau Slavica Zänker MBA.\"       \"Dr. Piotr Stey\"                \r\n\r\nch_color_name(n=10)\r\n\r\n\r\n [1] \"MintCream\"     \"Navy\"          \"Black\"         \"Teal\"         \r\n [5] \"MediumBlue\"    \"MistyRose\"     \"Tomato\"        \"PeachPuff\"    \r\n [9] \"LightSalmon\"   \"PaleVioletRed\"\r\n\r\nch_phone_number(locale=\"de_DE\",n=10)\r\n\r\n\r\n [1] \"+49 (0) 1720 802187\" \"03620 99631\"         \"(01226) 649261\"     \r\n [4] \"+49(0) 824237073\"    \"+49 (0) 8400 206866\" \"+49(0)8440 75302\"   \r\n [7] \"06401 91167\"         \"+49(0)2991 35592\"    \"05552 060876\"       \r\n[10] \"00126 77808\"        \r\n\r\nA nice small application with fake locations.\r\n\r\n\r\nlocations <- data.frame(lon=ch_lon(n=10),lat=ch_lat(n=10))\r\n\r\nggplot(locations)+\r\n  borders(\"world\")+\r\n  geom_point(aes(x=lon,y=lat))+\r\n  coord_quickmap()\r\n\r\n\r\n\r\n\r\nfabricatr\r\nEasy creation of hierarchical data. In this example there are five families, each one has between 1 and 12 members. Each family member has between 1 and 5 accounts. With add_level() we can automatically produce a table that shows all accounts of all members in all families.\r\n\r\n\r\nlibrary(fabricatr)\r\n\r\nfabricate(\r\n  family  = add_level(N = 5,\r\n  n_members = sample(1:12, N, replace = TRUE,prob=12:1)),\r\n  \r\n  members  = add_level(N = n_members,\r\n  n_accounts = sample(1:5,N,replace=TRUE,prob=(5:1)^2)),\r\n  \r\n  account = add_level(N = n_accounts)\r\n  ) %>%\r\nhead(10)\r\n\r\n\r\n   family n_members members n_accounts account\r\n1       1         6      01          1      01\r\n2       1         6      02          1      02\r\n3       1         6      03          1      03\r\n4       1         6      04          1      04\r\n5       1         6      05          2      05\r\n6       1         6      05          2      06\r\n7       1         6      06          3      07\r\n8       1         6      06          3      08\r\n9       1         6      06          3      09\r\n10      2         1      07          1      10\r\n\r\nLink levels. We can create 15 clients with their birth year and join year and some correlation between both variables.\r\n\r\n\r\ndf <- fabricate(\r\n  age = add_level(N=51, birth_year=1950:2000),\r\n  tenure = add_level(N = 20, join_year=1991:2010, nest = FALSE),\r\n  client = link_levels(N = 15, by = join(age, tenure, rho = 0.7))\r\n)\r\n\r\ndf %>% select(client,birth_year,join_year)\r\n\r\n\r\n   client birth_year join_year\r\n1      01       1957      1996\r\n2      02       1958      2003\r\n3      03       1966      2001\r\n4      04       1952      1996\r\n5      05       1977      2001\r\n6      06       1985      2003\r\n7      07       1975      1992\r\n8      08       1984      2005\r\n9      09       1999      2010\r\n10     10       1977      2003\r\n11     11       1964      1991\r\n12     12       1953      1992\r\n13     13       1987      2002\r\n14     14       1960      1994\r\n15     15       1992      2007\r\n\r\nOrdered data\r\nfabricatr has an amazing function to create ordered data.\r\nLet’s take a look at an example where we have two types of clients, gold clients that receive a yearly gift from the bank and standard clients that do not. How could we simulate their responses to a satisfaction survey?\r\n\r\n\r\ndf <- fabricate(\r\n  N = 100,\r\n  gold_client_flag = draw_binary(prob = 0.3, N),\r\n  satisfaction = draw_ordered(\r\n    x = rnorm(N, mean = -0.4 + 1.2 * gold_client_flag),\r\n    breaks = c(-1.5, -0.5, 0.5, 1.5),\r\n    break_labels = c(\"Very Unsatisfied\", \"Unsatisfied\", \"Neutral\",\r\n                     \"Satisfied\", \"Very Satisfied\")\r\n  )\r\n)\r\n\r\ndf %>% count(gold_client_flag,satisfaction) %>%\r\n  tidyr::pivot_wider(id_cols=satisfaction,names_from=\"gold_client_flag\",values_from=\"n\")\r\n\r\n\r\n# A tibble: 5 x 3\r\n  satisfaction       `0`   `1`\r\n  <fct>            <int> <int>\r\n1 Very Unsatisfied    10    NA\r\n2 Unsatisfied         23     4\r\n3 Neutral             17    15\r\n4 Satisfied            8     9\r\n5 Very Satisfied       5     9\r\n\r\nTo draw counts from a distribution we can use draw_count().\r\n\r\n\r\nrtlme_model <- fabricate(\r\n  N = 1000,\r\n  radio_coverage = rlnorm(N, meanlog=0, sdlog=1),\r\n  violent_incident_count = draw_count(mean = 1.5 * radio_coverage, N = N)\r\n)\r\n\r\n\r\n\r\nTime series\r\nExample from this article.\r\nThis example contains the gdp of five countries over the course of five years.\r\n\r\n\r\npanel_units <- fabricate(\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.8),\r\n    growth_error = runif(N, 0.1, 0.5)\r\n  ),\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    gdp_measure = base_gdp + (ts_year * growth_units) + rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\npanel_units\r\n\r\n\r\n   countries base_gdp growth_units growth_error years ts_year\r\n1          1 16.90156    0.7887063    0.4250585    01       0\r\n2          1 16.90156    0.7887063    0.4250585    02       1\r\n3          1 16.90156    0.7887063    0.4250585    03       2\r\n4          1 16.90156    0.7887063    0.4250585    04       3\r\n5          1 16.90156    0.7887063    0.4250585    05       4\r\n6          2 19.00123    0.5316010    0.2704959    06       0\r\n7          2 19.00123    0.5316010    0.2704959    07       1\r\n8          2 19.00123    0.5316010    0.2704959    08       2\r\n9          2 19.00123    0.5316010    0.2704959    09       3\r\n10         2 19.00123    0.5316010    0.2704959    10       4\r\n11         3 21.45789    0.5811573    0.4674703    11       0\r\n12         3 21.45789    0.5811573    0.4674703    12       1\r\n13         3 21.45789    0.5811573    0.4674703    13       2\r\n14         3 21.45789    0.5811573    0.4674703    14       3\r\n15         3 21.45789    0.5811573    0.4674703    15       4\r\n16         4 17.27903    0.6806222    0.1447995    16       0\r\n17         4 17.27903    0.6806222    0.1447995    17       1\r\n18         4 17.27903    0.6806222    0.1447995    18       2\r\n19         4 17.27903    0.6806222    0.1447995    19       3\r\n20         4 17.27903    0.6806222    0.1447995    20       4\r\n21         5 19.50876    0.4422331    0.2363325    21       0\r\n22         5 19.50876    0.4422331    0.2363325    22       1\r\n23         5 19.50876    0.4422331    0.2363325    23       2\r\n24         5 19.50876    0.4422331    0.2363325    24       3\r\n25         5 19.50876    0.4422331    0.2363325    25       4\r\n   gdp_measure\r\n1     16.92928\r\n2     17.56074\r\n3     18.53657\r\n4     19.88491\r\n5     20.04814\r\n6     19.06130\r\n7     19.59367\r\n8     20.38241\r\n9     20.30867\r\n10    21.12840\r\n11    21.03028\r\n12    22.38920\r\n13    22.94488\r\n14    23.09727\r\n15    23.71597\r\n16    17.12976\r\n17    17.68792\r\n18    18.46198\r\n19    19.32061\r\n20    19.61428\r\n21    19.40967\r\n22    19.75703\r\n23    20.31434\r\n24    20.58925\r\n25    21.08585\r\n\r\n\r\n\r\n\r\nWe can take this to the next level and introduce some year specific information and then cross this with the country specific information. We just have to add one layer.\r\n\r\n\r\npanel_global_data <- fabricate(\r\n  years = add_level(\r\n    N = 5,\r\n    ts_year = 0:4,\r\n    year_shock = rnorm(N, 0, 0.3) #each year has a global trend\r\n  ),\r\n  countries = add_level(\r\n    N = 5,\r\n    base_gdp = runif(N, 15, 22),\r\n    growth_units = runif(N, 0.2, 0.5), \r\n    growth_error = runif(N, 0.1, 0.5),\r\n    nest = FALSE\r\n  ),\r\n  country_years = cross_levels(\r\n    by = join(years, countries),\r\n    gdp_measure = base_gdp + year_shock + (ts_year * growth_units) +\r\n      rnorm(N, sd=growth_error)\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nGenOrd\r\nThis package helps to create discrete random variables with prescribed correlation matrix and marginal distributions.\r\n\r\n\r\nlibrary(GenOrd)\r\n\r\n\r\nk <- 4 #number of random variables\r\nmarginal <- list(0.6, c(1/3,2/3), c(1/4,2/4,3/4), c(1/5,2/5,3/5,4/5))\r\n\r\n\r\n\r\nRead this as follows: * We will create 4 random variables. * The first variable will have two values 60% of the data will be 1, 40% will be 2. * The second variable will have three values, 1,2 and 3 with a probability of 33% each. * etc… * Each vector in this list refers to one variable, and we will see the cumulative probability for each value.\r\n\r\n\r\ncorrcheck(marginal)\r\n\r\n\r\n[[1]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n           [,1]       [,2]       [,3]       [,4]\r\n[1,]  1.0000000 -0.8333333 -0.8215838 -0.8660254\r\n[2,] -0.8333333  1.0000000 -0.9128709 -0.9237604\r\n[3,] -0.8215838 -0.9128709  1.0000000 -0.9486833\r\n[4,] -0.8660254 -0.9237604 -0.9486833  1.0000000\r\n\r\n[[2]]\r\n4 x 4 Matrix of class \"dsyMatrix\"\r\n          [,1]      [,2]      [,3]      [,4]\r\n[1,] 1.0000000 0.8333333 0.8215838 0.8660254\r\n[2,] 0.8333333 1.0000000 0.9128709 0.9237604\r\n[3,] 0.8215838 0.9128709 1.0000000 0.9486833\r\n[4,] 0.8660254 0.9237604 0.9486833 1.0000000\r\n\r\nThis function shows what are allowable ranges for the correlation matrix, given the input from the marginal distributions.\r\n\r\n\r\nSigma <- matrix(c(1,0.5,0.4,0.3,\r\n                  0.5,1,0.5,0.4,\r\n                  0.4,0.5,1,0.5,\r\n                  0.3,0.4,0.5,1),\r\n                k, k, byrow=TRUE)\r\n\r\n\r\nn <- 1000 # sample size\r\nm <- ordsample(n, marginal, Sigma)\r\n\r\ndf <- data.frame(m)\r\nhead(df)\r\n\r\n\r\n  X1 X2 X3 X4\r\n1  2  2  2  2\r\n2  2  3  4  5\r\n3  1  2  3  3\r\n4  1  1  1  2\r\n5  1  1  1  2\r\n6  1  1  2  2\r\n\r\nLet’s verify that the data is actually what we expected.\r\n\r\n\r\ncor(df)\r\n\r\n\r\n          X1        X2        X3        X4\r\nX1 1.0000000 0.5230816 0.4086599 0.3521497\r\nX2 0.5230816 1.0000000 0.5188758 0.4397761\r\nX3 0.4086599 0.5188758 1.0000000 0.5348078\r\nX4 0.3521497 0.4397761 0.5348078 1.0000000\r\n\r\ndf %>% count(X4)\r\n\r\n\r\n  X4   n\r\n1  1 199\r\n2  2 205\r\n3  3 207\r\n4  4 201\r\n5  5 188\r\n\r\ndf %>% count(X1)\r\n\r\n\r\n  X1   n\r\n1  1 609\r\n2  2 391\r\n\r\nLater, we can rename the columns and values, but will have assured that they have the desired correlations.\r\nMore packages\r\nIn this blogpost by Joseph Rickert on R Views.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-03-useful-packages-for-data-composition/useful-packages-for-data-composition_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2020-12-04T11:51:25+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-13-data-composition-with-rmultinom/",
    "title": "Data composition with rmultinom",
    "description": "When creating several datasets that depend on each other, the rmultinom function from the stats package can be a useful helper. In this example we will see how to create customer transactions from a customer table.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-11-13",
    "categories": [],
    "contents": "\r\nIn this short article I want to show how the rmultinom() function can help to simulate data. We will simulate client data, and for each client we will create transactions.\r\nThe rmultinom() function simulates the multinomial distribution (Link).\r\nIn my head I always picture the multinomial distribution as a game setup. You have N balls and K bins. Instead of the number of bins, we send a vector of probabilities (of length K), how likely it is for the balls to land in each bin (you can imagine that some bins are closer and others are further away, or that some are larger than others). This vector will be normalized automatically, so you do not have to worry about this.\r\nLet’s try an example, with N=1000 and K=5. We want one of the bins to be twice as large as the others.\r\n\r\n\r\ntest1 <- rmultinom(n=1,size=1000,c(2,1,1,1,1))\r\n\r\n\r\n\r\n\r\n\r\n\r\nHow can we use this function to create transactions for a given number of customers? The key is to simulate all important values on client level and use rmultinom to decompose the values into smaller portions. First, let’s get some clients.\r\n\r\n\r\nset.seed(61)\r\n\r\nage <- rnorm(10,mean=50,sd=15) %>% pmax(18) %>% round()\r\ntenure <- (age - 18 - runif(10,1,30)) %>% pmax(0) %>% round()\r\nincome <- rexp(10,0.0001) %>% round(2)\r\n\r\nclient <- data.frame(id=1:10,age,tenure,income)\r\n\r\nclient\r\n\r\n\r\n   id age tenure   income\r\n1   1  44     22 32181.33\r\n2   2  44      7  5454.56\r\n3   3  24      3  4559.78\r\n4   4  55     34 26790.03\r\n5   5  29      0 18592.27\r\n6   6  47     19 40229.93\r\n7   7  61     14  1065.49\r\n8   8  58     17  2750.75\r\n9   9  71     49 12292.53\r\n10 10  45     15   282.05\r\n\r\nFor this exercise, we do not distinguish between different types of transactions. In practice, it would make sense to separate rent, supermarket, transport and other categories.\r\nWe create a second dataframe for clients, which contains “invisible” information needed for the transactions. Let’s begin with the total spending. This can depend on anything we know about the client. In this case, we will assume that each client has more or less the same behavior and spends around 70% of their income. The standard deviation of 0.1 assures that this value varies from client to client.\r\n\r\n\r\ncl_secret_info <- client\r\n\r\ncl_secret_info$total_spend <- (cl_secret_info$income * rnorm(10,0.7,sd=0.1)) %>% round(2)\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend\r\n1   1  44     22 32181.33    20755.16\r\n2   2  44      7  5454.56     4342.34\r\n3   3  24      3  4559.78     3280.65\r\n4   4  55     34 26790.03    21012.47\r\n5   5  29      0 18592.27    13990.48\r\n6   6  47     19 40229.93    25986.32\r\n7   7  61     14  1065.49      699.14\r\n8   8  58     17  2750.75     2158.39\r\n9   9  71     49 12292.53    10135.33\r\n10 10  45     15   282.05      160.08\r\n\r\nThe next ingredient is the number of transactions. We will make a very simple formula depending on age.\r\n\r\n\r\ncl_secret_info$n_trans <- ifelse(cl_secret_info$age < 50, rbinom(10,60,0.5),rbinom(10,60,0.3))\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans\r\n1   1  44     22 32181.33    20755.16      27\r\n2   2  44      7  5454.56     4342.34      37\r\n3   3  24      3  4559.78     3280.65      28\r\n4   4  55     34 26790.03    21012.47      25\r\n5   5  29      0 18592.27    13990.48      32\r\n6   6  47     19 40229.93    25986.32      27\r\n7   7  61     14  1065.49      699.14      11\r\n8   8  58     17  2750.75     2158.39      14\r\n9   9  71     49 12292.53    10135.33      16\r\n10 10  45     15   282.05      160.08      27\r\n\r\nNow we already know that our transaction table will have 244 rows.\r\nWith this we can already make the split, but we will create a last parameter which is an indicator of how similar the transactions are. You could split $100 into one large transaction of $80 and a four small transactions of $5 each or you could have five transactions of around $20 each.\r\n\r\n\r\ncl_secret_info$diff_trans <- rexp(10,100/cl_secret_info$total_spend) %>% ceiling()\r\n\r\ncl_secret_info\r\n\r\n\r\n   id age tenure   income total_spend n_trans diff_trans\r\n1   1  44     22 32181.33    20755.16      27        328\r\n2   2  44      7  5454.56     4342.34      37         55\r\n3   3  24      3  4559.78     3280.65      28         38\r\n4   4  55     34 26790.03    21012.47      25         65\r\n5   5  29      0 18592.27    13990.48      32          1\r\n6   6  47     19 40229.93    25986.32      27       1408\r\n7   7  61     14  1065.49      699.14      11          1\r\n8   8  58     17  2750.75     2158.39      14         47\r\n9   9  71     49 12292.53    10135.33      16        183\r\n10 10  45     15   282.05      160.08      27          2\r\n\r\nNow we have all the necessary ingredients to split the total into n_trans transactions for each customer. And this is the moment where the rmultinom function is extremely helpful. Let’s take a look at the first client, who spends $20755.16 in 27 transactions. The high diff_trans value indicates that there will likely be some very high transaction values and some very low.\r\nBefore doing the rmultinom magic, we will create the vector with the bins first.\r\n\r\n\r\nbins <- runif(cl_secret_info$n_trans[1],min=1,max=cl_secret_info$diff_trans[1])\r\n\r\ntransactions1 <- rmultinom(1,cl_secret_info$total_spend[1],bins)\r\n\r\ndf <- data.frame(client_id=1, trans_id=1:cl_secret_info$n_trans[1],value=transactions1)\r\n\r\nDT::datatable(df)\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[880,1240,1364,1262,158,1291,1182,86,1455,1354,288,831,738,8,1209,164,594,822,280,1093,321,297,211,1276,9,1375,967]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>client_id<\\/th>\\n      <th>trans_id<\\/th>\\n      <th>value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nAutomate this for all customers\r\nIn order to efficiently do this for all customers we will put what we just did in a function and call this via lapply.\r\nThen we can bind all the transactions from all customers together in our final dataframe.\r\n\r\n\r\ncreate_transactions <- function(i) {\r\n  bins <- runif(cl_secret_info$n_trans[i],min=1,max=cl_secret_info$diff_trans[i])\r\n\r\n  transactions <- rmultinom(1,cl_secret_info$total_spend[i],bins)\r\n\r\n  df <- data.frame(client_id=i, trans_id=1:cl_secret_info$n_trans[i],value=transactions)\r\n  \r\n  return(df)\r\n}\r\n\r\ntrans_list <- lapply(seq_along(client$id),create_transactions)\r\n\r\ntrans_df <- do.call(rbind,trans_list)\r\n\r\nDT::datatable(trans_df)\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,6,7,8,9,10,11,12,13,14,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[573,1367,688,250,228,521,873,1136,533,374,1118,399,482,473,1265,536,1237,701,585,1041,1311,1408,617,1036,1298,381,324,115,51,206,40,184,69,73,150,89,111,34,165,188,128,216,157,119,61,139,204,6,46,188,141,139,131,225,73,140,60,145,182,135,9,91,94,38,148,52,262,91,26,116,164,24,10,36,157,247,225,24,186,167,50,58,14,40,116,186,180,185,106,138,211,61,1513,360,286,160,467,1165,1553,1520,218,916,443,783,1549,153,224,455,327,1545,813,1282,647,1480,679,1144,1330,427,418,473,438,420,464,460,442,431,424,450,432,408,416,445,427,419,444,437,432,434,433,457,421,438,439,427,475,416,459,462,422,1027,1425,355,63,1355,1845,329,530,1729,998,1150,356,53,1370,1290,248,1692,854,1802,19,1562,1244,663,1533,770,797,927,63,60,56,76,65,65,73,59,69,55,58,178,262,110,49,154,233,29,258,251,32,228,231,109,34,366,835,517,464,1270,368,100,1232,210,701,617,211,583,1190,179,1292,7,7,2,2,4,5,7,6,9,1,5,3,6,5,8,6,8,4,12,5,8,8,11,4,8,5,4]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>client_id<\\/th>\\n      <th>trans_id<\\/th>\\n      <th>value<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-13-data-composition-with-rmultinom/data-composition-with-rmultinom_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-12-04T10:56:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-data-simulation/",
    "title": "Data Simulation",
    "description": "The purpose of this post is to enable readers to create data from scratch which they can use for their analyses or visualizations.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [],
    "contents": "\r\nPackages\r\nMost of the functions that we are using here are actually part of base R.\r\nWe will need some functions from the dplyr package and the ggplot2 package for quick visualizations, but these are optional.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nImportant to make your data creation reproducible (i.e. if you run it again, it gives the same result) is the set.seed() function. As we are creating instances of random variables we assure with this function that every time the same sequence of random variables is generated. You can use any number you like inside this function.\r\n\r\n\r\nset.seed(64)\r\n\r\n\r\n\r\nManual values\r\nLet’s start with the most simple but most time-consuming way. Type everything manually and save it in a vector:\r\n\r\n\r\nclient_gen <- c(\"Millenial\",\"Gen X\",\"Millenial\",\r\n                \"Baby Boomer\",\"Gen X\",\"Millenial\",\"Gen X\")\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1   Millenial\r\n2  2       Gen X\r\n3  3   Millenial\r\n4  4 Baby Boomer\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nCategorical variables with sample()\r\nFor categorical variables, we can save some time using the sample function. You specify first the possible values and then how many of these values you would like to pick. If you want to allow values to be picked more than once, make sure to set replace=TRUE.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"),7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_gen)\r\n\r\n\r\n  id  client_gen\r\n1  1 Baby Boomer\r\n2  2   Millenial\r\n3  3   Millenial\r\n4  4   Millenial\r\n5  5       Gen X\r\n6  6   Millenial\r\n7  7       Gen X\r\n\r\nThe sample function is quite flexible and we can tweak the prob parameter, for example to say that we want (approximately) half of the population to be Baby Boomers. The effect will be visible if we produce larger amounts of data.\r\n\r\n\r\nclient_gen <- sample(c(\"Millenial\",\"Gen X\",\"Baby Boomer\"),1000,replace=TRUE,prob=c(0.25,0.25,0.5))\r\n\r\nqplot(client_gen)\r\n\r\n\r\n\r\n\r\nNumerical variables\r\nThe same sample() function works with numbers.\r\n\r\n\r\nclient_age <- sample(1:100,size=7,replace=TRUE)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1          4\r\n2  2         42\r\n3  3         33\r\n4  4         62\r\n5  5         76\r\n6  6         65\r\n7  7         81\r\n\r\nIn both cases above, each number had the same probability of being selected. If we would like some numbers to be more likely to be selected, we can specify this with prob.\r\nThe probability values will be automatically scaled to 1. If I would like to have 50% of the population to have the age of 27, I can specify the weight. (Note: rep(1,5) is equivalent to c(1,1,1,1,1), replicating the number 1 five times.)\r\n\r\n\r\nclient_age <- sample(1:100,size=1000,replace=TRUE,prob=c(rep(1,26),99,rep(1,73)))\r\n\r\nqplot(client_age==27)\r\n\r\n\r\n\r\n\r\nDistributions\r\nIf you would like to work with probability distributions to create numerical variable, this is also very easy with the base functions of type r+(starting letters of the distribution).\r\nLet’s try the uniform distribution:\r\n\r\n\r\nclient_age <- runif(7,min=1,max=100)\r\n\r\ndata.frame(id=1:7,client_age)\r\n\r\n\r\n  id client_age\r\n1  1   55.10342\r\n2  2   85.19588\r\n3  3   86.47791\r\n4  4   73.91516\r\n5  5   48.15197\r\n6  6   32.90848\r\n7  7   58.76874\r\n\r\nWe can use the round() function to round each value to their next integer.\r\n\r\n\r\nrunif(10000,1,100) %>% round() %>% qplot(binwidth=10)\r\n\r\n\r\n\r\n\r\nBut uniformly distributed variables are not always what we want. In the example above we simulated 10,000 clients and distributes their ages uniformly. Then there are as many 99 year old clients as there are 50 year old clients.\r\nBut we can easily access a whole list of other distribution functions, like the famous Normal distribution (with mean and standard deviation as parameters).\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf we want to limit the values to not be smaller than 0 or larger than 100, we can use pmin and pmax.\r\n\r\n\r\nrnorm(10000,mean=50,sd=20) %>% pmax(0) %>% pmin(100) %>% qplot()\r\n\r\n\r\n\r\n\r\nFor many applications (like balance distribution or any data that contains outliers) I like to use the Exponential distribution (with parameter rate and expectation 1/rate).\r\n\r\n\r\nrexp(10000,rate=0.01) %>% qplot()\r\n\r\n\r\n\r\n\r\nIf you want to explore further probability distributions check out this link.\r\nCombining variables in a dataframe\r\nTo create our first simulated dataframe, we can start by simulating the variables separately and then putting them together.\r\n\r\n\r\nset.seed(61)\r\n\r\nk <- 7\r\n\r\nid <- 1:k\r\nname <- c(\"Frank\",\"Dorian\",\"Eva\",\"Elena\",\"Andy\",\"Barbara\",\"Yvonne\")\r\nage <- rnorm(k,mean=30,sd=10) %>% pmax(18) %>% round()\r\nocupation <- sample(c(\"analyst\",\"manager\",\"sr analyst\"),k,replace=T,prob=c(10,2,3))\r\nbalance <- rexp(k,rate=0.001) %>% round(2)\r\nmarried <- sample(c(\"Yes\",\"No\"),k,replace=T,prob=c(0.6,0.4))\r\n\r\ndata <- data.frame(client_id=id,name,age,ocupation,balance,married_flg=married)\r\ndata\r\n\r\n\r\n  client_id    name age  ocupation balance married_flg\r\n1         1   Frank  26 sr analyst  245.96          No\r\n2         2  Dorian  26    analyst 2273.39          No\r\n3         3     Eva  18    manager 2270.47          No\r\n4         4   Elena  34    analyst  373.45          No\r\n5         5    Andy  18    analyst  961.21         Yes\r\n6         6 Barbara  28    analyst   69.32         Yes\r\n7         7  Yvonne  37    analyst 3218.13         Yes\r\n\r\nGreat! We just simulated a dataset which we can use now for visualization or modeling purposes.\r\nExporting\r\nWhen we are happy with our created dataset and want to use it somewhere else we can export it using the base R function.\r\n\r\n\r\nwrite.csv(raw,\"data.csv\")\r\n\r\n\r\n\r\nAlternatively, if this is not fast enough, we can also use the fwrite function from the data.table package which is much faster.\r\n\r\n\r\nlibrary(data.table)\r\nfwrite(raw,\"data.csv\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-09-data-simulation/data-simulation_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-12-04T10:43:26+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-fuzzy-merging/",
    "title": "Fuzzy merging",
    "description": "Imagine that you have a list of hand-typed names/words and a list of reference. Fuzzy merging helps   to map Fantasy company & co. / FANTASY COMP / Fnatasy Company to the same entity \"Fantasy Company\"   from the reference list.",
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [],
    "contents": "\r\nPackages\r\nThe only packages you need are dplyr and stringdist. I used tictoc to measure the runtime.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(stringdist)\r\nlibrary(tictoc)\r\n\r\n\r\n\r\nThe data\r\nThis method requires as input two lists. To distinguish them, we will call the one that contains different spellings as the “dirty list”. The reference list will be called the “clean list”. In this blogpost I will create the dirty list by hand.\r\n\r\n\r\nnames <- c(\"Haliburton\", \"ExxonMobile\",\"ABBOTT LABORATORIES\",\"Marrriott\",\"Self\",\"Activision Blizzard\",\r\n           \"Quest dianotstics\",\"Unemployed\",\"other company\",\"burger king\",\r\n           \"MARRIOT\",\"wall mart\", \"Illumin\", \"3M\",\"NORTHROP TRUMMON\",\"MCCormicks\",\"MARSH MCLEANNON\",\r\n           \"FLO SERVE\", \"Kansas City Southern Fed.\",\"MCDONALD'S\",\"F5 Networks\",\r\n           \"McDonalds\",\"MacKindsey\",\"Oracle\",\"Self-employed\",\"None\",\"Retired\",\r\n           \"f5 networks\",\"Harley Davidson\",\"Harly Davidson\",\"HARLEY DAVIDSEN\",\"DRHorton\",\"D.R. Horten\",\r\n           \"cincinati fin\",\"cincinnatti financials\",\"cincinnati financial\",\"CINCINATTI FINANCE\",\r\n           \"Mohaws Industry\",\"Mowahk Industries\",\"Mohawk Ind\")\r\n\r\nset.seed(64)\r\ndirty_list <- sample(names,50000,replace=T)\r\n\r\n\r\n\r\n\r\ndirty_list\r\nHaliburton\r\nExxonMobile\r\nABBOTT LABORATORIES\r\nMarrriott\r\nSelf\r\nActivision Blizzard\r\nQuest dianotstics\r\nUnemployed\r\nother company\r\nburger king\r\n\r\nAs a clean list we will use the list of S&P500 companies. This can be downloaded or scraped from the internet.\r\n\r\nclean_list\r\n3M Company\r\nAbbott Laboratories\r\nAbbVie Inc.\r\nABIOMED Inc\r\nAccenture plc\r\nActivision Blizzard\r\nAdobe Systems Inc\r\nAdvanced Micro Devices Inc\r\nAdvance Auto Parts\r\nAES Corp\r\n\r\nBefore we start, we will pre-process both lists, remove some common words and transform everything to lower case.\r\n\r\n\r\ncleaner <- function(vec) {\r\n  wordremove <- c(\" and \",\" comp \",\" company\",\"companies\",\" corp \",\"corporation\",\" inc \",\"[.]com\")\r\n  output <- vec %>% tolower() %>% \r\n    {gsub(paste(wordremove,collapse='|'),\"\",.)} %>%\r\n    {gsub(\"[[:punct:]]\",\"\",.)} %>%\r\n    {gsub(\"[[:blank:]]\",\"\",.)}\r\n  return(output)\r\n}\r\n\r\ncontrol <- data.frame(original=dirty_list)\r\n\r\nclean_list_cl <- cleaner(clean_list)\r\ndirty_list_cl <- cleaner(dirty_list)\r\n\r\n\r\n\r\nMain process\r\nWe calculate a matrix of string distances. The stringdist package has a lot of different methods implemented which can be checked here. After comparing some of the methods I decided to go with the Jaro-Winkler distance as it yields higher similarity for words which start with the same letters.\r\nExample\r\n\r\n\r\nstringdistmatrix(c(\"other\",\"words\",\"otherexample\",\"exapmle\"),\r\n                 c(\"example\",\"other example\",\"word\"),\r\n                 method='jw',p=0.1,useNames=\"strings\")\r\n\r\n\r\n                example other example      word\r\nother        1.00000000    0.12307692 0.5166667\r\nwords        1.00000000    0.48205128 0.0400000\r\notherexample 0.28174603    0.01538462 0.4444444\r\nexapmle      0.03333333    0.55799756 1.0000000\r\n\r\nEach row of the matrix of string distances is one string from the dirty list. We find the minimum in each row, which is equivalent to the best fit from the clean list.\r\n\r\n\r\ntic(\"a\")\r\ndistmatrix <- stringdist::stringdistmatrix(dirty_list_cl,clean_list_cl,method='jw',p=0.1)\r\nbest_fit <- apply(distmatrix,1,which.min) %>% as.integer()\r\nsimilarity <- apply(distmatrix,1,min)\r\n\r\ncontrol$best_fit <- clean_list[best_fit]\r\ncontrol$distance <- round(similarity,3)\r\ntoc()\r\n\r\n\r\na: 8.03 sec elapsed\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nRetired\r\nResMed\r\n0.203\r\nSelf\r\nSealed Air\r\n0.244\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nSelf-employed\r\nTeleflex\r\n0.306\r\n\r\nResults\r\nWhen we order the control dataframe by similarity we can find a suitable cutoff value (in this example 0.12) to separate real matches from false positives. This cutoff value depends on the application.\r\n\r\n\r\ncontrol$result <- ifelse(control$distance<=0.12,control$best_fit,NA)\r\n\r\n\r\n\r\n\r\noriginal\r\nbest_fit\r\ndistance\r\nresult\r\nMarrriott\r\nMarriott Int’l.\r\n0.089\r\nMarriott Int’l.\r\nRetired\r\nResMed\r\n0.203\r\nNA\r\nSelf\r\nSealed Air\r\n0.244\r\nNA\r\nHaliburton\r\nHalliburton Co.\r\n0.054\r\nHalliburton Co.\r\nF5 Networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\nIllumin\r\nIllumina Inc\r\n0.073\r\nIllumina Inc\r\nMohaws Industry\r\nMohawk Industries\r\n0.113\r\nMohawk Industries\r\nMARSH MCLEANNON\r\nMarsh & McLennan\r\n0.030\r\nMarsh & McLennan\r\nSelf-employed\r\nTeleflex\r\n0.306\r\nNA\r\nMohawk Ind\r\nMohawk Industries\r\n0.088\r\nMohawk Industries\r\nMowahk Industries\r\nMohawk Industries\r\n0.017\r\nMohawk Industries\r\nHarly Davidson\r\nHarley-Davidson\r\n0.014\r\nHarley-Davidson\r\nf5 networks\r\nF5 Networks\r\n0.000\r\nF5 Networks\r\n3M\r\n3M Company\r\n0.000\r\n3M Company\r\nOracle\r\nOracle Corp.\r\n0.080\r\nOracle Corp.\r\n\r\nNext Steps and other resources\r\nImprove performance for large datasets. On Github, I have an implementation of this method with the parallel package which improves performance slightly. But there is definitely more room for improvement.\r\nThere is an interesting video about performance improvement by not calculating the full matrix by Seth Verrinder and Kyle Putnam here.\r\nAndrés Cruz created an Add-in which helps to fine-tune the final result, his slide from LatinR 2019 can be found here.\r\nCheck out David Robinson’s fuzzyjoin package here.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-10-21T19:26:06+02:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to R-Vogg-Blog",
    "description": {},
    "author": [
      {
        "name": "Richard Vogg",
        "url": "https://github.com/richardvogg"
      }
    ],
    "date": "2020-10-09",
    "categories": [],
    "contents": "\r\nI am using R almost every day for the analysis of data.\r\nAfter reading this great blogpost by Rachel Thomas, called “Why you (yes, you) should blog” I started playing around with the idea of writing down some of the things I learn.\r\nEspecially this point resonated a lot in me:\r\n\r\nYou are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it. The context of your particular background, your particular style, and your knowledge level will give a different twist to what you’re writing about.\r\n\r\n— Rachel Thomas\r\n\r\nThe next event that brought me closer to this blog was a tutorial by Maëlle Salmon during Latin-R 2020. The material can be found here. She showed how easy it is to get started with {distill} and how to setup a blog.\r\nThe last ingredient I needed were topics to write about. As I will be speaking about Data Composition during rstudio::global(2021), I thought this was a great opportunity to summarize some methods and ideas around this topic.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-07T10:47:40+01:00",
    "input_file": "welcome.utf8.md"
  }
]
